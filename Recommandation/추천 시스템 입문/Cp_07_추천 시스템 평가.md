<h1>1. 3가지 평가 방법</h1>
<h4>오프라인 평가</h4>
<ul>
  <li>
    오프라인 평가란 실제 서비스상에서 열람, 구매 등의 <strong>사용자 행동 이력에서 얻은 과거의 로그</strong>를 사용해 모델의 예측 정확도를 평가하는 것을 의미한다.
  </li>
  <li>
    서비스 로그를 사용하기에 <strong>평가 비용이 낮고</strong> 데이터양이 풍부하여 <strong>평가 결과의 분산이 적다</strong>는 장점이 있다.
  </li>
  <li>
    오프라인 평가에서는 평가가 좋았으나 실제 결과는 좋지 못한 경우가 있을 수 있기에 오프라인 평가에서는 비즈니스 목적의 <strong>대체 지표</strong>가 있으면 좋다.
  </li>
</ul>

<h4>온라인 평가</h4>
<ul>
  <li>
    온라인 평가는 <strong>새로운 테스트 대상</strong>의 추천 모델이나 <strong>새로운 사용자 인터페이스</strong>를 일부 사용자에게 실제로 표시함으로써 <strong>평가를 수행</strong>한다.
  </li>
  <li>
    온라인으로 직접적으로 확인할 수 있기에 보다 정확한 평가를 수행할 수 있으나 구현 비용이 높고 모델 성능이 좋지 못하다면 만족도가 떨어질 수 있다.
  </li>
</ul>

<h4>사용자 스터디</h4>
<ul>
  <li>
    사용자 스터디에 의한 평가는 사용자에게 <strong>인터뷰나 설문</strong>을 하는 것으로 추천 모델이나 사용자 인터페이스의 <strong>정성적인 성질을 조사</strong>한다.
  </li>
  <li>
    <strong>사용자 경험에 관한 피드백</strong>을 직접 얻을 수 있어 서비스 로그만으로 알 수 없는 개선점을 발견할 수 있다.
  </li>
  <li>
    단, 사용자의 주관에 기반한 경우가 많기에 개인 취향에 따른 대답의 분산이 크고 데이터의 양을 충분히 얻기 어려워 재현성이 떨어질 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 오프라인 평가</h1>
<h2>2-1. 모델 정밀도 평가</h2>
<ul>
  <li>
    추천 시스템에서 모델의 주요 목적은 <strong>과거 사용자 행동을 학습하고 미지의 사용자 행동을 예측</strong>하는 것이다. 예측 능력은 일반화 성능이라고도 불린다.
  </li>
</ul>

<br>

<h2>2-2. 모델 밸리데이션</h2>
<ul>
  <li>
    데이터의 시계열성 등 데이터의 특징에 따라 데이터를 <strong>train, valid 그리고 test로 분할</strong>한다.
  </li>
    <ul>
      <li>
        valid는 학습 데이터에 대한 <strong>정밀도 검증 데이터</strong>에 해당한다.
      </li>
    </ul>
  <li>
    미지의 데이터에 대한 일반화 성능을 검증하는 것을 밸리데이션(validation)이라 한다.
  </li>
</ul>

<br>

<h2>2-3. 모델 튜닝</h2>
<ul>
  <li>
    모델 튜닝이란 예측 성능이 높아지도록 모델이 갖는 <strong>파라미터를 조정</strong>하는 것을 의미하며 <strong>valid 데이터</strong>에 대한 벨리데이션을 통해 수행한다.
  </li>
  <li>
    파라미터 최적화는 그리드 서치나 베이지안 최적화 등이 있으며 수동으로 수행할 수도 있다.
  </li>
</ul>

<br>

<h2>2-4. 평가 지표</h2>
<h3>2-4-1. 예측 오차 지표</h3>
<p>

- $ r_i $: 실제 평갓값
- $ \hat r_i $: 예측한 평갓값
- $ n $: 아이템 수

</p>

```python
r = [0, 1, 2, 3, 4]
r_hat = [0.1, 1.1, 2.1, 3.1, 4.1]
```


<h4>MAE</h4>
<p>

$$ \text{MAE}=\frac{1}{n} \sum_{i=1}^n(r_i-\hat r_i)^2 $$

MAE(Mean Absolute Error, 평균 절댓값 오차)는 예측값과 실측값 차이의 <strong>절댓값 평균</strong>을 나타내는 지표이다.

</p>

```python
from sklearn.metrics import mean_squared_error

print(mean_absolute_error(r, r_hat))
```

<h4>MSE</h4>
<p>

$$ \text{MSE}=\frac{1}{n} \sum_{i=1}^n(r_i-\hat r_i)^2 $$

MSE(Mean Squared Error, 평균 제곱 오차)는 예측값과 실측값 차이의 제곱 평균을 나타내는 지표이다. 제곱을 하기에 음수 부호를 없애며 차이가 크면 더 커지고 차이가 작으면 작아지는 특징이 있다.

</p>

```python
from sklearn.metrics import mean_squared_error

print(mean_squared_error(r, r_hat))
```

<h4>RMSE</h4>
<p>

$$ \text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(r_i-\hat r_i)^2} $$

RMSE(Root Mean Squared Error, 평균 제곱근 오차)는 <strong>MSE의 제곱근</strong>을 구해 예측값과 실측값의 <strong>차원을 맞춘 지표</strong>이다.

</p>

```python
from sklearn.metrics import mean_squared_error
import numpy as np

mean_squared_error(r, r_hat)
print(np.sqrt(mean_squared_error(r, r_hat)))
```

<h3>2-4-2. 집합 평가 지표</h3>
<ul>
  <li>  
    사용자에게 클릭된다고 예측한 예측 아이템 집합이 실제로 클릭되는지를 평가하는 평가 지표로 <strong>Precision</strong>, <strong>Recall</strong> 그리고 <strong>F1-measure</strong> 지표를 학습한다.
  </li>
</ul>

```python
# 예측 아이템 집합. 예측 점수가 높은 순으로 배열한다.
pred_item = [1, 2, 3, 4, 5]

# 적합 아이템 집합
true_items = [2, 4, 6, 8]
```
 
<h4>Precision</h4>
<p>

$$ \text{Precision@K}=\frac{|C\cap R_K|}{K} $$

- $ C $: 적합 아이템 집합
- $ K $: 순위의 길이
- $ R_K $: 예측 아이템 집합의 K번째 이내 아이템

Precision은 예측 아이템 집합 안에 존재하는 적합 아이템 비율을 의미하며 순위 길이가 K일 때 Precision@K로 표현한다. 즉, 예측한 것중 진짜 맞는 비율이다. 얼마나 정확한가를 의미한다.

</p>

```python
def precision_at_k(true_items: List[int], pred_items: List[int], 
                   k: int) -> float:
    if k == 0:
        return 0.0

    p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k

print(precision_at_k(true_items, pred_items, 3))
```

<h4>Recall</h4>
<p>

$$ \text{Recall@K}=\frac{|C \cap R_K|}{|C|} $$

- $ C $: 적합 아이템 집합
- $ K $: 순위의 길이
- $ R_K $: 예측 아이템 집합의 K번째 이내 아이템

Recall(재현율)은 예측 아이템 집합의 요소가 얼마나 적합 아이템 집합의 요소를 커버할 수 있는가의 비율이다. 즉, 전체 실제 정답 중 몇 개나 맞췄는가 이다. 정답을 얼마나 놓치지 않았는지를 의미한다.

Precision과 trade-off 관계에 있다 하지만 Recall은 $ R_k $에 포함되는 적합 아이템의 수에 따라 일정할 수도 있다.

</p>

```python
def recall_at_k(true_items: List[int], pred_items: List[int],
                k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0
    
    r_at_k = (len(set(true_items) & set(pred_items[:k]))) / len(true_items)
    return r_at_k

print(recall_at_k(true_items, pred_items, 3))
```

<h4>F1-measure</h4>
<p>

$$ \text{F1}=\frac{2 \cdot \text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}} $$

F1-measure(F1값)는 Precision과 Recall의 조화 평균으로 표현되며 Precision과 Recall 양쪽을 고려하는 평가 방식이다.

</p>

```python
def f1_at_k(true_items: List[int], pred_items: List[int],
            k: int) -> float:
    precision = precision_at_k(true_items, pred_items, k)
    recall = recall_at_k(true_items, pred_items, k)

    if precision + recall == 0.0:
        return 0.0
    
    return 2*precision*recall / (precision + recall)

print(f1_at_k(l, 3))
```

<h3>2-4-3. 순위 평가 지표</h3>
<h4>PR 곡선</h4>
<ul>
  <li>
    Top@K를 Top@1, Top@2, ..., Top@N으로 바꾸면 대응하는 <strong>Recall과 Precision의 여러 조합</strong>을 얻을 수 있다.
  </li>
  <li>
    Recall과 Precision의 조합을 <strong>Recall을 가로 축</strong>으로, <strong>Precision을 세로 축</strong>으로 plot해 각 점을 연결한 것을 <strong>PR 곡선</strong>이라 한다. 
  </li>
  <li>
    Recall과 Precision이 모두 높은 경우는 오른쪽 위 영역에 해당하기에 <strong>오른쪽 위로 PR 곡선이 위치할수록 정밀도가 높은 추천</strong>이라 볼 수 있다.
  </li>
    <ul>
      <li>
        곡선으로 둘러 쌓인 영역을 AUC(Area Under Curve)라 하며 0과 1사이의 값을 갖고 면적이 넓을수록 예측 정확도가 높다.
      </li>
    </ul>
  <li>
    <strong>ROC의 AUC</strong>도 존재하는데 이는 <strong>음의 예와 양의 예가 비슷</strong>할 때 적합하다. 추천 시스템의 데이터셋이 음<strong>의 예가 양의 예보다 많을 경우</strong>에는 <strong>PR 곡선의 AUC</strong>를 보는 편이 적절하다.
  </li>
</ul>

<h4>MRR@K</h4>
<p>

$$ \text{MRR@K}=\frac{1}{|U|}\sum_{u\in U} \frac{1}{k_u} $$

- $ U $: 사용자 전체의 집합
- $ k_u $: 사용자 𝑢 에 대해 순위 K 이내에서 최초로 등장한 적합 아이템의 순위 위치

MRR(Mean Reciprocal Rank)은 사용자 순위에 대해 최초의 적합 아이템이 순위에서 얼마나 상위에 위치하는지를 평가한다. 최초 적합 아이템의 등장을 사용하기에 정확도 보다는 응답 반환 속도 측정에 적합하다.

</p>

```python
def rr_at_k(user_relevances: List[int], k: int) -> float:
    nonzero_indices = np.asarray(user_relevances).nonzero()[0]
    if nonzero_indices.size > 0 and nonzero_indices[0] + 1 <= k:
        return 1.0 / (nonzero_indices[0] + 1.0)
    return 0.0
print(rr_at_k([0, 1, 0], 2))

def mrr_at_k(users_relevances: List[List[int]], k: int) -> float:
    return float(
        np.mean(
            [rr_at_k(user_relevances, k) for user_relevances in 
            users_relevances]
        )
    )

print(mrr_at_k([[1, 0, 0], [0, 1, 0], [0, 0, 1]], 2))
```

<h4>AP@K</h4>
<p>

$$ AP(u)@K=\frac{1}{\sum_{k=1}^Kc_{u,k}}\sum_{k=1}^Kc_{u,k}\cdot Precision@K $$

AP@K(평균 정밀도, Average Precision)는 K번째까지에 대한 각 적합 아이템까지의 Precision을 평균한 값이다.

- $ K $: 순위의 길이
- $ c_{u,k}=\begin{cases}1: \text{사용자} u\text{의 } k\text{번째 아이템이 적합} \\ 0:\text{그 외}\end{cases}$

정답이 나올 때마다 해당 시점의 Precision을 계산하고 평균한 값으로 정답을 얼마나 일찍 그리고 꾸준히 잘 맞추고 있는가를 의미한다.

</p>

```python
def ap_at_k(user_relevances: List[int], k: int) -> float:
    # K개 안에 적합 아이템이 하나도 없으면 0
    if sum(user_relevances[:k]) == 0:
        return 0.0

    # 적합(1)인 위치의 인덱스만 추출
    nonzero_indices = np.asarray(user_relevances[:k]).nonzero()[0]

    # 각 적합 위치에서의 Precision 계산 후 평균
    return sum(
        [sum(user_relevances[:idx + 1]) / (idx + 1) for idx in nonzero_indices]
    ) / len(nonzero_indices)

print(ap_at_k([0, 1, 0, 1, 0], 5))
```

<h4>MAP@K</h4>
<p>

$$ \text{MAP@K}=\frac{1}{|U|}\sum_{u \in U}\text{AP}(u)@K $$

MAP(Mean Average Precision, 중앙 평균 정밀도)는 AP를 각 사용자에 대해 평균한 값이다.

</p>

```python
def map_at_k(users_relevances: List[List[int]], k: int) -> float:
    return float(
        np.mean(
            [ap_at_k(user_relevances, k) for user_relevances in 
            users_relevances]
        )
    )

print(map_at_k([[1, 0, 0], [0, 1, 0], [0, 0, 1]], 3))
```

<p>

MAP와 MRR의 식의 형태가 다르지만 다음과 같은 형식으로 일반화할 수 있다.

$$ \text{M}(u)@K=\frac{1}{|U|}\sum_{u\in U}\sum_{k=1}^KP_u(k)G_u(k) $$

- $ P_u(k) $: 사용자가 위치 $ k $에서 정지할 확률. ($ k $ 순위의 위치에서 아이템 열람을 마치고 이탈하는 것)
- $ G_u(k) $: 위치 $ k $에서 얻을 수 있는 이익

식은 확률과 이득의 곱이기에 MAP, MRR은 이득의 기댓값이라 볼 수 있다. 일반화 식은 사용자에 대해 다음과 같은 기준을 가정한다.

- <strong>선형 횡단</strong>: 횡단 사용자는 최상위부터 순서대로 1건씩 아이템을 열람한다.
- <strong>횡단 정지</strong>: 사용자는 확률적 또는 결정적으로 어떤 아이템을 클릭한 후 만족하고 정지한다.
- <strong>이익 획득</strong>: 사용자는 행동 의도에 맞춰 클릭한 아이템으로부터 적합도에 따른 이익을 얻는다.

MAP는 모든 적합 아이템에 대해 확률적으로 균일하게 정정지하며 위치 $ r $에서 정지할 때 상위 $ r $ 이내 순위의 Precision과 동일한 이익을 얻는다. MRR은 위치 $ k $에서 정지할 때 상위 $ r $ 이내 순위의 Precision과 같은 이익을 얻는다.

</p>

<h4>nDCG</h4>
<p>

nDCG는 아이템이 클릭된 후의 구매 유무 등 <strong>클릭 이외의 행동</strong>까지 합쳐 가중치를 붙여 여러 값을 평가하고 싶을 때 활용한다.

$$ \text{nDCG}@K=\frac{\text{DCG}@K}{\text{DCG}_{\text{ideal}}@K} $$

nDCG(normalized Discounted Cumulative Gain)는 DCG(Discounted Comulative Gain)라고 불리는 값을 순위가 이성적으로 나열됐을 때의 DCG로 나누는 것(normalization: 정규화)이라고 정의한다. DCG는 순위 위치에 대한 이익(gain)을 분모로 하는 log항에 따라 할인(discount)한다.

$$ \text{DCG}@K=r_1+\sum_{i=2}^2\frac{r_i}{\log_2i} $$

- $ r $: 순위의 각 아이템의 이익을 나타낸다.

</p>

```python
def dcg_at_k(user_relevances: List[int], k: int) -> float:
    user_relevances = user_relevances[:k]
    if len(user_relevances) == 0:
        return 0.0

    return user_relevances[0] + np.sum(user_relevances[1:]) /
    np.log2(np.arange(2, len(user_relevances) + 1))

def ndcg_at_k(user_relevances: List[int], k: int) -> float:
    dcg_max = dcg_at_k(sorted(user_relevances, reverse=True), k)
    if not dcg_max:
        return 0.0
    return dcg_at_k(user_relevances, k) / dcg_max

print(ndcg_at_kl([0, 2, 0, 1, 0], 5))
```

<h3>2-4-4. 기타 지표</h3>
<ul>
  <li>
    앞서 설명한 평가 지표들은 <strong>정밀도</strong>에 관련된 평가 지표이다.
  </li>
  <li>
    정밀도만 고려하는 것은 비슷한 상품만 추천해 버리는 <strong>필터 버블(filter bubble)</strong> 등의 문제를 내포하기에 정밀도 외의 지표를 사용할 필요가 있다.
  </li>
</ul>

<h4>카탈로그 커버리지</h4>
<p>

$$ \text{Catalogue Converge}=\frac{|\hat I|}{|I|} $$

- $ \hat I $: 실제로 추천된 아이템
- $ I $: 모든 아이템 집합커버

커버리지는 추천 범위(폭)을 측정하는 지표이다. 이를통해 인기 상품에 치추친 추천을 검출할 수 있다. 최근에는 추천 시스템 내의 사용자 뿐만 아니라 이해관계자(multi-stakeholder)를 포함해 추천 시스템을 개선한다.

</p>

<h4>사용자 커버리지</h4>
<p>

$$ \text{User Converge}=\frac{|\hat U|}{|U|} $$

- $ |\hat U| $: 실제로 추천이 수행된 사용자 집합.
- $ U $: 모든 사용자 집합

사용자 커버리지(User Coverage)는 어느 정도의 사용자에게 추천되었는지를 측정하는 지표히다.

사용자 커버리지는 <strong>초기 사용자에게 얼마나 추천</strong>했는지 확인할 수 있으며 그 결과 <strong>콜드 스타트 문제 검출</strong>에 사용할 수 있다. 커버리지가 낮은 경우 초기 사용자에게 추천이 잘 안된 것으로 콜드 스타트 문제가 발생한 것이다.

</p>

<h4>신규성</h4>
<p>

$$ \text{Novelty}(R)=\frac{\sum_{i\in R}-\log_2p(i)}{|R|} $$

- $ R $: 순위
- $ p(i)=\frac{\sum_{u\in U}\text{imp}(u, i)}{|U|} $, 모든 사용자에 대해 과거 그 아이템이 추천된 확률을 의미한다.
- $ \text{imp}(u, i)=\begin{cases}1: \text{사용자 } u\text{에게 아이템 } i\text{가 과거에 추천된 경우} \\ 0\text{:그 외} \end{cases}$ 

신규성(Novelty)은 순위에 대한 추천 아이템이 정말 새로운 것인지를 나타낸다.

</p>

<h4>다양성</h4>

<p>

$$ \text{Diversity(R)}=\frac{\sum_{i\in R}\sum_{j\in R \setminus i} \text{sim}(i,j)}{|R|(|R|-1)} $$

- $ R $: 순위
- $ \text{sim}(i,j) $: 아이템 $ i $와 아이템 $ j $의 유사도 거리

순위 R에서의 <strong>다양성(diversity)</strong>은 <strong>순위의 각 아이템 간 유사도 거리의 평균값</strong>에 따라 정의된다. 해당 정의는 <strong>순위</strong>에만 존재하는 다양성 지표(individual, intra-list diversity)이다.

</p>

<h4>흥미로움</h4>
<p>

$$ \text{Serendipity}(R,u)=\frac{|R_{\text{unexp}}\cap R_{\text{useful}}|}{|R|} $$

- $ R $: 사용자에 대한 순위
- $ R_{\text{unexp}} $: 순위 $ R $에서 의외성이 있는 아이템 집합
- $ R_{\text{useful}} $: 순위 $ R $에서 유용한 아이템 집합

요약하면 흥미로움은 순위에 대해 의외성을 가지면서도 유용한 아이템의 비율을 측정한다.

</p>

<h3>2-4-5. 평가 지표 선정 방법</h3>
<ul>
  <li>
    <strong>정밀도 지표</strong>상 가장 성능이 좋은 모델을 배포하고 매출 등 지표가 좋은 모델을 조사한다.
  </li>
  <li>
    온라인 평가와 일치하도록 여러 오프라인 지표를 조합해 <strong>새로운 오프라인 지표</strong>를 만드는 접근 방법 또한 존재한다. 이렇게 <strong>조합하는 방식의 평가</strong>가 실제 서비스에서 유용한 경우가 많다.
  </li>
  <li>
    사용자의 <strong>행동 이력을 가정</strong>해 지표를 선택하는 방법도 존재한다.
  </li>
    <ul>
      <li>
        <strong>유도형(Navigational)</strong>: <strong>특정 사이트를 방문</strong>하려는 의도
      </li>
      <li>
        <strong>정보 수집(Informational)</strong>: <strong>1개 이상의 웹 페이지</strong>에 기록되어 있다고 생각되는 정보를 얻으려는 의도.
      </li>
      <li>
        <strong>거래형(Transaction)</strong>: 웹을 매개로 한 <strong>액션을 실행</strong>하려는 의도.
      </li>
    </ul>
  <li>
    순위 지표를 행동 의도별로 분류한 그림은 p230에 있다.
  </li>
  <li>
    <strong>ERR(Expected Reciprocal Rank)</strong>은 <strong>정보 검색 분야</strong>에서 유명한 지표이다. ERR은 주목한 아이템의 순위에 <strong>위치를 추가</strong>해 해당 아이템보다 <strong>위에 있는 아이템으로부터 정의된 정지 확률에 의존</strong>한다.
  </li>
  <li>
    필자의 경험상 평가 지표를 구현할 때 버그가 포함되는 경우가 많기에 검증된 구현을 사용하는 것이 좋다.
  </li>  
    <ul>
      <li>
        https://github.com/moseskim/RecommenderSystems
      </li>
    </ul>
</ul>

<br><br>

<h1>3. 온라인 평가</h1>
<ul>
  <li>
    오프라인 평가는 주로 지표를 활용한 평가로 사용자에게 영향을 주지 않지만 온라인 평가는 직접적으로 UI 등을 변경하여 수행한다. 즉, 시스템의 변경점을 <strong>실제로 사용자에게 제시</strong>해 평가하는 방법이다.
  </li>
</ul>

<br>

<h2>3-1. A/B 테스트</h2>
<ul>
  <li>
    A/B 테스트는 무작위 비교 실험(Randomized Controlled Trial, RCT)라 불리는 평가 방법이다.
  </li>
  <li>
    A/B 테스트에는 테스트 대상의 기능에 <strong>변경을 추가한 결과를 보여주는 Treatment 그룹(실험군)</strong>과 <strong>변경하지 않은 결과를 보여주는 Control(대조군)</strong>의 두 개 그롭으로 사용자를 나누어 수행하는 방식이다.
  </li>
</ul>

<h3>3-1-1. 가설</h3>
<ul>
  <li>
    A/B 테스트를 시작할 때에는 테스트 대상의 효과에 대해 <strong>가설</strong>을 세우는 것이 좋으며 다음과 같은 템플릿을 예로 들 수 있다.
  </li>
    <ul>
      <li>
        <strong>콘텍스트</strong>: 실험 배경은 무엇인가?
      </li>
      <li>
        <strong>변경점</strong>: 무엇을 어떻게 바꾸는가?
      </li>
      <li>
        <strong>대상</strong>: 대상 사용자는 누구인가?
      </li>
      <li>
        <strong>지표 영향</strong>: 변경점이 지표에 어떤 영향을 미치는가?
      </li>
      <li>
        <strong>비즈니스 목표</strong>: 이니셔티브를 통해 달성하고 싶은 최종 비즈시느 목표는 무엇인가?
      </li>
    </ul>
</ul>

<h4>진행 방법</h4>
<ul>
  <li>
    템플릿을 결정했다면 구현할 테스트를 실시한다. Treatment와 Control 사용자 그룹은 <strong>난수</strong>를 사용하여 나눈다.
  </li>
    <ul>
      <li>
        사용자의 만족도에 영향을 미칠 수 있기 때문에 소수의 사용자 집단에서부터 시작하는 것이 좋다.
      </li>
    </ul>
</ul>

<h4>주의점</h4>
<ul>
  <li>
    구룹을 나누기 전부터 이미 차이가 있다면 <strong>그룹 편향(group bias)</strong>이 발생할 수 있다. 이 경우 A와 B그룹에 차이가 없음을 확인하는 <strong>A/A 테스트</strong>를 수행할 수 있다.
  </li>
  <li>
    두 개 이상의 모델을 테스트 할 때 테스트 군이 적어 <strong>로그를 혼합하여 테스트</strong>를 한다면 로그가 섞여 평가 결과가 정확하지 않을 수 있다.
  </li>
    <ul>
      <li>
        특정 모델의 로그가 더 많을 수 있으며 이 경우 편향이 발생하기 때문이다.
      </li>
    </ul>
  <li>
    <strong>집계 기간</strong>을 설계할 때에 서비스의 특징을 고려하여 설계해야 한다.
  </li>
</ul>

<h3>3-1-2. 지표의 역할</h3>
<ul>
  <li>
    테스트의 성공/실패를 최종적으로 판단하는 <strong>OEC 지표</strong>와 테스트 시 대상 사용자에게 나쁜 영향을 미치지 않았는가를 측정하는 <strong>가드레일 지표</strong>를 살펴본다.
  </li>
</ul>

<h4>OEC 지표</h4>
<ul>
  <li>
    A/B 테스트의 성공과 실패를 최종적으로 판단하는 지표를 <strong>OEC(Overall Evaluation Criteria) 지표</strong>라 한다.
  </li>
    <ul>
      <li>
        OEC 지표는 <strong>서비스/비즈니스 성공</strong>을 위해 시스템을 움직이는 것을 돕기 위한 정의된 지표이다.
      </li>
      <li>
        OEC 지표는 <strong>장기적인 서비스 KPI와 연관</strong>되어야 하며 동시에 단기적으로는 팀원이 행동할 때 충분히 <strong>공감</strong>되어야 한다.
      </li>
      <li>
        팀 단위 뿐 아니라 <strong>조직 전체</strong>가 OEC 지표에 대해 합의하는 것이 바람직하다.
      </li>
    </ul>
  <li>
    OEC 지표는 <strong>사용자에 맞추어 설계</strong>를 하며 다음과 같은 관점을 염두할 수 있다.
  </li>
    <ul>
      <li>
        Happiness: 서비스에 대한 사용자 감각
      </li>
      <li>
        Engagement: 상품에 대한 사용자의 관여 수준
      </li>
      <li>
        Adoption: 어떤 기간에 새로 사품을 사용하기 시작한 신규 사용자
      </li>
      <li>
        Retention: 어떤 기간의 사용자 중 몇 명이 여전히 서비스 사용을 지속하고 있는가.
      </li>
      <li>
        Task success: 호율성(태스트 종료까지의 기간), 유효성(태스크 완료율 등), 에러율 등 기존 사용자 만족도와 연관된다.
      </li>
    </ul>
</ul>

<h4>가드레일 지표</h4>
<ul>
  <li>
    OEC 지표가 개선하고자 하는 지표에 관한 것이라면 <strong>가드레인(guardrail)지표</strong>는 <strong>저하되어서는 안 되는 제약</strong>을 표시한다. 주로 성능과 관련된 리소스가 해당한다.
  </li>
  <li>
    사용자 행동에는 다양한 <strong>트레이드 오프</strong>가 존재함을 주의해야 한다. 예를 들어 개인화 행동이 증가하면 일반 검색에 대한 행동이 감소할 가능성이 있다.
  </li>
</ul>

<h3>3-1-3. 지표 설계 방침</h3>
<h4>감도(Sensitivity)</h4>
<ul>
  <li>
    감도는 두 개의 요소로 이루어진다.
  </li>
    <ul>
      <li>
        <strong>이동 확률(Movement Probability)</strong>
      </li>
        <ul>
          <li>
            <strong>업데이트</strong>에 대해 지표가 얼마나 빈번하게 <strong>변동</strong>하는가를 의미한다.
          </li>
        </ul>
      <li>
        <strong>통계력(statistical Power)</strong>
      </li>
        <ul>
          <li>
            <strong>효과에 변동</strong>이 있을 때 얼마나 <strong>정확하게 특정</strong>할 수 있는가를 의미한다.
          </li>
          <li>
            <strong>검출력</strong>이라고도 불리며 A/B 테스트에서는 최소 80%를 요구한다.
          </li>
        </ul>
    </ul>
</ul>

<h4>신뢰성(Trustworthiness)</h4>
<ul>
  <li>
    지표는 <strong>신뢰성이 높아야 하며</strong>, 봇이나 비정상적인 접근으로 인한 <strong>노이즈를 제거하는 것이 중요하다.</strong>  
    이러한 데이터 품질 관리 과정은 <strong>별도의 이니셔티브(initiative)</strong>로 수행된다.
  </li>
</ul>

<h4>효율성(Efficiency)</h4>
<ul>
  <li>
    효율에는 <strong>시간</strong>, <strong>복잡성</strong> 그리고 <strong>비용</strong> 세 가지가 있으며 이를 고려하여 결정해야 한다.
  </li>
</ul>

<h4>디버깅 가능성(Debuggability)과 액션 가능성(Actionability)</h4>
<ul>
  <li>
    디버그와 관련된 지표는 테스트에 이상이 있을 때 그 이상의 <strong>원인을 추적해 수정하는 것과 연결되는 지표</strong>도 준비해야 한다.
  </li>
</ul>

<h4>해석 가능성(Interpretability)과 방향성(Directionality)</h4>
<ul>
  <li>
    지표는 쉽게 해석할 수 있어야 하며 <strong>방향성</strong>은 해당 변수가 개선되었을 때 <strong>비즈니스 목표가 달성되는가</strong>를 나타낸다.
  </li>
</ul>

<br>

<h2>3-2. 인터리빙</h2>
<ul>
  <li>
    인터리빙(interleaving)은 온라인 평가 방법 중 하나로 A/B 테스트 처럼 사용자 <strong>그룹을 나누지 않고 테스트 대상만 분류</strong>하여 제공한 뒤 평가한다.
  </li>
  <li>
    두 개의 순위를 비교하는 것을 인터리빙이라 하고, 세 개 이상의 순위를 섞어 평가하는 방식을 <strong>멀티리빙(multileaving)</strong>이라 한다.
  </li>
    <ul>
      <li>
        멀티리빙은 많은 모델이나 파라미터를 동시에 테스하고 싶을 데 <strong>A/B 테스트 실시 전에 테스트 대상의 필터링</strong>에 사용되기도 한다.
      </li>
    </ul>
  <li>
    인터리빙 방법들 중에는 다음이 있다.
  </li>
    <ul>
      <li>
        <strong>Team Draft Multileaving(TDM)</strong>: 두 개의 검토를 진행할 때 <strong>선공/후공</strong>을 무작위로 결정하여 하나씩 아직 사용되지 않은 검색 결과를 상위부터 순서대로 선택한다.
      </li>
      <li>
        <strong>Probabilistic Multileaving(PM)</strong>: 가능한 한 각 순위 안의 <strong>검토 결과 순서를 유지</strong>하면서 상대적으로 낮은 확률로 임의의 순서에서 검토 결과를 선택하도록 허용한다.
      </li>
      <li>
        <strong>Optimized Multileaving(OM)</strong>: 먼저 출력 후보가 되는 <strong>순위를 다수 준비</strong>한다. 이후 <strong>최적화 문제</strong>를 풀어내어 출력 후보의 <strong>순위 출력 확률을 조정</strong>한다.
      </li>
    </ul>
</ul>

<h4>Pairwise Preference Multileaving(PPM)</h4>
<ul>
  <li>
    속도가 빠르면서 Considerateness와 Fidelity라는 이론적 보증이 가능한 방법이다.
  </li>
    <ul>
      <li>
        <strong>Considerateness</strong>
      </li>
        <ul>
          <li>
            섞어서 만든 순위는 원래의 입력 순위보다 품질이 나쁘지 않다라는 성질을 의미한다.
          </li>
          <li>
            즉, 테스트 시 입력 순위 이상으로는 사용자 만족도를 저하시키지 않는다는 것이다.
          </li>
        </ul>
      <li>
        <strong>Fidelity</strong>
      </li>
        <ul>
          <li>
            무작위로 클릭한 경우 <strong>모든 순위 점수</strong>의 <strong>기댓값이 같아지고</strong> 동시에 보다 <strong>우수한 순위 점수의 기댓값</strong>은 <strong>더 높아진다</strong>는 성질이다.
          </li>
        </ul>
    </ul>
  <li>
    PPM은 속도가 빠르면서 이론적 보증이 되지만 구현이나 집계가 상당히 복잡하기에 TDM부터 도입하는 것이 권장된다.
  </li>
  <li>
    인터리빙 방식은 효율적이지만 UI 등은 평가할 수 없다.
  </li>
</ul>

<br><br>

<h1>4. 사용자 스터디를 통한 평가</h1>
<ul>
  <li>
    사용자에게 직접 <strong>인터뷰나 설문 조사</strong>를 실시하는 사용자 스터디를 통해 추천 시스템이 사용자에게 어떤 느낌을 주는지에 관한 <strong>정성적인 시사점</strong>을 얻을 수 있다.
  </li>
</ul>

<br>

<h2>4-1. 조사 설계</h2>
<h3>참가자 선정</h3>
<ul>
  <li>
    사용자 스터디의 대상은 가급적 <strong>실제 사용자에 가까운 사람</strong>을 선택하는 것이 좋다.
  </li>
  <li>
    사용자 스터디의 중요한 관점 중 하나는 <strong>샘플을 통해 얻어진 결과를 모집단에 적용</strong>할 수 있다는 점이다. 샘플링 방법으로는 다음과 같은 것들이 있다.
  </li>
    <ul>
      <li>
        <strong>단순 무작위 추출법</strong>: 모집단에 포함된 사용자를 같은 확률로 무작위 추출한다.
      </li>
      <li>
        <strong>계통적 추출법</strong>: 모집단에 포함된 사용자에 <strong>번호</strong>를 붙인 뒤 첫 번째 조사 대상자를 무작위 선정하고 이후부터 부여한 번호대로 간격을 두어 추출한다.
      </li>
      <li>
        <strong>층화 추출법</strong>: 이미 알고 있는 모집단의 정보(나이, 성별 등)를 사용해 <strong>몇 개의 층</strong>을 만들고 해당 층에 대해 필요한 샘플 수를 얻을 때까지 <strong>무작위 추출</strong>한다.
      </li>
      <li>
        <strong>임의 추출법</strong>: <strong>조사에 참가하고 싶은 사람</strong>에게 참가하도록 하는 방법이다. 임의로 추출한 사용자가 얼마나 모집단을 반영하고 있는지 이해하는 것이 중요하다.
      </li>
    </ul>
</ul>

<h3>참가자 수</h3>
<ul>
  <li>
    사용자 스터디의 참가자 수는 <strong>조사 목적</strong>과 <strong>오차 허용 정도</strong>에 따라 결정된다.
  </li>
    <ul>
      <li>
        오차 허용 정도란 얻어진 <strong>결과가 얼마나 확실</strong>해야 하는가를 의미하며 <strong>통계량에 대한 신뢰 구간</strong>을 그리거나 <strong>통계적 검정</strong>을 수행할 수 있다.
      </li>
    </ul>
</ul>

<h3>조사 시점</h3>
<ul>
  <li>
    조사 시점은 서비스, 조직의 규모 등에 따라 다양하다.
  </li>
</ul>

<h3>피험자 내 측정/피험자 간 측정</h3>
<ul>
  <li>
    <strong>피험자 내 측정</strong>은 <strong>참가자별</strong>로 여러 개 얻은 데이터를 비교하는 방법이고, <strong>피헌자 간 측정</strong>은 특정 참가자의 데이터를 <strong>다른 참자가의 데이터와 비교</strong>하는 방식이다.
  </li>
  <li>
    피험자 내 측정은 참가자가 여러 평가를 <strong>순서대로 수행</strong>하기 때문에 평가 과정에서 학습이 진행되거나 피로로 인해 수행 능력 및 감각이 달라질 수 있다. 이러한 문제를 <strong>캐리 오버 효과(carry over effect)</strong>라 한다.
  </li>
    <ul>
      <li>
        캐리 오버 효과를 줄이는 방법으로는 <strong>카운터 밸런스(counter balance)</strong>가 있다. 이 방법은 각 참가자가 태스크를 수행하기 전에 <strong>태스크의 순서를 무작위로 뒤섞는 것</strong>이다.
      </li>
    </ul>
  <li>
    피험자 간 측정은 참가자 간의 비교이기에 분산이 클 수 있으며 따라서 <strong>큰 크기의 샘플</strong>이 필요하다. 반면 <strong>캐리 오버 효과가 적다</strong>는 장점이 있다.
  </li>
</ul>

<br>

<h2>4-2. 설문 조사 예</h2>
<ul>
  <li>
    설문 조사를 통해 사용자 중심으로 평가를 수행하는 프레임워크 <strong>ResQue</strong>를 다룬다.
  </li>
  <li>
    ResQue는 다음과 같은 네 개의 대항목으로 분류된다.
  </li>
    <ul>
      <li>
        <strong>시스템 품질</strong>: 사용자가 직접 <strong>경험</strong>하며 느끼는 품질 평가이다.
      </li>
      <li>
        <strong>사용자의 신념</strong>: 시스템 품질 경험을 바탕으로 형성된 <strong>인지적 신념이나 판단</strong>
      </li>
      <li>
        <strong>사용자의 태도</strong>: <strong>신념이 축적</strong>되어 형성된 <strong>감정적 반응</strong>이나 <strong>전반적인 호감도</strong>
      </li>
      <li>
        <strong>행동 의도</strong>: 사용자가 일으킨 <strong>행동</strong>.
      </li>
    </ul>
  <li>
    추천 시스템은 정밀도만으로 평가할 수 없는 부분(e.g. UX/UI)이 있는데 이렇게 정밀도만으로 평가하기 어려운 부분을 확인하기 위해 사용자 스터디를 활용한다.
  </li>
</ul>