<h1>1. 3가지 평가 방법</h1>
<h4>오프라인 평가</h4>
<ul>
  <li>
    오프라인 평가란 실제 서비스상에서 열람, 구매 등의 <strong>사용자 행동 이력에서 얻은 과거의 로그</strong>를 사용해 모델의 예측 정확도를 평가하는 것을 의미한다.
  </li>
  <li>
    서비스 로그를 사용하기에 <strong>평가 비용이 낮고</strong> 데이터양이 풍부하여 <strong>평가 결과의 분산이 적다</strong>는 장점이 있다.
  </li>
  <li>
    오프라인 평가에서는 평가가 좋았으나 실제 결과는 좋지 못한 경우가 있을 수 있기에 오프라인 평가에서는 비즈니스 목적의 <strong>대체 지표</strong>가 있으면 좋다.
  </li>
</ul>

<h4>온라인 평가</h4>
<ul>
  <li>
    온라인 평가는 <strong>새로운 테스트 대상</strong>의 추천 모델이나 <strong>새로운 사용자 인터페이스</strong>를 일부 사용자에게 실제로 표시함으로써 <strong>평가를 수행</strong>한다.
  </li>
  <li>
    온라인으로 직접적으로 확인할 수 있기에 보다 정확한 평가를 수행할 수 있으나 구현 비용이 높고 모델 성능이 좋지 못하다면 만족도가 떨어질 수 있다.
  </li>
</ul>

<h4>사용자 스터디</h4>
<ul>
  <li>
    사용자 스터디에 의한 평가는 사용자에게 <strong>인터뷰나 설문</strong>을 하는 것으로 추천 모델이나 사용자 인터페이스의 <strong>정성적인 성질을 조사</strong>한다.
  </li>
  <li>
    <strong>사용자 경험에 관한 피드백</strong>을 직접 얻을 수 있어 서비스 로그만으로 알 수 없는 개선점을 발견할 수 있다.
  </li>
  <li>
    단, 사용자의 주관에 기반한 경우가 많기에 개인 취향에 따른 대답의 분산이 크고 데이터의 양을 충분히 얻기 어려워 재현성이 떨어질 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 오프라인 평가</h1>
<h2>2-1. 모델 정밀도 평가</h2>
<ul>
  <li>
    추천 시스템에서 모델의 주요 목적은 <strong>과거 사용자 행동을 학습하고 미지의 사용자 행동을 예측</strong>하는 것이다. 예측 능력은 일반화 성능이라고도 불린다.
  </li>
</ul>

<br>

<h2>2-2. 모델 밸리데이션</h2>
<ul>
  <li>
    데이터의 시계열성 등 데이터의 특징에 따라 데이터를 <strong>train, valid 그리고 test로 분할</strong>한다.
  </li>
    <ul>
      <li>
        valid는 학습 데이터에 대한 <strong>정밀도 검증 데이터</strong>에 해당한다.
      </li>
    </ul>
  <li>
    미지의 데이터에 대한 일반화 성능을 검증하는 것을 밸리데이션(validation)이라 한다.
  </li>
</ul>

<br>

<h2>2-3. 모델 튜닝</h2>
<ul>
  <li>
    모델 튜닝이란 예측 성능이 높아지도록 모델이 갖는 <strong>파라미터를 조정</strong>하는 것을 의미하며 <strong>valid 데이터</strong>에 대한 벨리데이션을 통해 수행한다.
  </li>
  <li>
    파라미터 최적화는 그리드 서치나 베이지안 최적화 등이 있으며 수동으로 수행할 수도 있다.
  </li>
</ul>

<br>

<h2>2-4. 평가 지표</h2>
<h3>2-4-1. 예측 오차 지표</h3>
<p>

- $ r_i $: 실제 평갓값
- $ \hat r_i $: 예측한 평갓값
- $ n $: 아이템 수

</p>

```python
r = [0, 1, 2, 3, 4]
r_hat = [0.1, 1.1, 2.1, 3.1, 4.1]
```


<h4>MAE</h4>
<p>

$$ \text{MAE}=\frac{1}{n} \sum_{i=1}^n(r_i-\hat r_i)^2 $$

MAE(Mean Absolute Error, 평균 절댓값 오차)는 예측값과 실측값 차이의 <strong>절댓값 평균</strong>을 나타내는 지표이다.

</p>

```python
from sklearn.metrics import mean_squared_error

print(mean_absolute_error(r, r_hat))
```

<h4>MSE</h4>
<p>

$$ \text{MSE}=\frac{1}{n} \sum_{i=1}^n(r_i-\hat r_i)^2 $$

MSE(Mean Squared Error, 평균 제곱 오차)는 예측값과 실측값 차이의 제곱 평균을 나타내는 지표이다. 제곱을 하기에 음수 부호를 없애며 차이가 크면 더 커지고 차이가 작으면 작아지는 특징이 있다.

</p>

```python
from sklearn.metrics import mean_squared_error

print(mean_squared_error(r, r_hat))
```

<h4>RMSE</h4>
<p>

$$ \text{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n(r_i-\hat r_i)^2} $$

RMSE(Root Mean Squared Error, 평균 제곱근 오차)는 <strong>MSE의 제곱근</strong>을 구해 예측값과 실측값의 <strong>차원을 맞춘 지표</strong>이다.

</p>

```python
from sklearn.metrics import mean_squared_error
import numpy as np

mean_squared_error(r, r_hat)
print(np.sqrt(mean_squared_error(r, r_hat)))
```

<h3>2-4-2. 집합 평가 지표</h3>
<ul>
  <li>  
    사용자에게 클릭된다고 예측한 예측 아이템 집합이 실제로 클릭되는지를 평가하는 평가 지표로 <strong>Precision</strong>, <strong>Recall</strong> 그리고 <strong>F1-measure</strong> 지표를 학습한다.
  </li>
</ul>

```python
# 예측 아이템 집합. 예측 점수가 높은 순으로 배열한다.
pred_item = [1, 2, 3, 4, 5]

# 적합 아이템 집합
true_items = [2, 4, 6, 8]
```
 
<h4>Precision</h4>
<p>

$$ \text{Precision@K}=\frac{|C\cap R_K|}{K} $$

- $ C $: 적합 아이템 집합
- $ K $: 순위의 길이
- $ R_K $: 예측 아이템 집합의 K번째 이내 아이템

Precision은 예측 아이템 집합 안에 존재하는 적합 아이템 비율을 의미하며 순위 길이가 K일 때 Precision@K로 표현한다. 즉, 예측한 것중 진짜 맞는 비율이다. 얼마나 정확한가를 의미한다.

</p>

```python
def precision_at_k(true_items: List[int], pred_items: List[int], 
                   k: int) -> float:
    if k == 0:
        return 0.0

    p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k

print(precision_at_k(true_items, pred_items, 3))
```

<h4>Recall</h4>
<p>

$$ \text{Recall@K}=\frac{|C \cap R_K|}{|C|} $$

- $ C $: 적합 아이템 집합
- $ K $: 순위의 길이
- $ R_K $: 예측 아이템 집합의 K번째 이내 아이템

Recall(재현율)은 예측 아이템 집합의 요소가 얼마나 적합 아이템 집합의 요소를 커버할 수 있는가의 비율이다. 즉, 전체 실제 정답 중 몇 개나 맞췄는가 이다. 정답을 얼마나 놓치지 않았는지를 의미한다.

Precision과 trade-off 관계에 있다 하지만 Recall은 $ R_k $에 포함되는 적합 아이템의 수에 따라 일정할 수도 있다.

</p>

```python
def recall_at_k(true_items: List[int], pred_items: List[int],
                k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0
    
    r_at_k = (len(set(true_items) & set(pred_items[:k]))) / len(true_items)
    return r_at_k

print(recall_at_k(true_items, pred_items, 3))
```

<h4>F1-measure</h4>
<p>

$$ \text{F1}=\frac{2 \cdot \text{Recall} \cdot \text{Precision}}{\text{Recall} + \text{Precision}} $$

F1-measure(F1값)는 Precision과 Recall의 조화 평균으로 표현되며 Precision과 Recall 양쪽을 고려하는 평가 방식이다.

</p>

```python
def f1_at_k(true_items: List[int], pred_items: List[int],
            k: int) -> float:
    precision = precision_at_k(true_items, pred_items, k)
    recall = recall_at_k(true_items, pred_items, k)

    if precision + recall == 0.0:
        return 0.0
    
    return 2*precision*recall / (precision + recall)

print(f1_at_k(l, 3))
```

<h3>2-4-3. 순위 평가 지표</h3>
<h4>PR 곡선</h4>
<ul>
  <li>
    Top@K를 Top@1, Top@2, ..., Top@N으로 바꾸면 대응하는 <strong>Recall과 Precision의 여러 조합</strong>을 얻을 수 있다.
  </li>
  <li>
    Recall과 Precision의 조합을 <strong>Recall을 가로 축</strong>으로, <strong>Precision을 세로 축</strong>으로 plot해 각 점을 연결한 것을 <strong>PR 곡선</strong>이라 한다. 
  </li>
  <li>
    Recall과 Precision이 모두 높은 경우는 오른쪽 위 영역에 해당하기에 <strong>오른쪽 위로 PR 곡선이 위치할수록 정밀도가 높은 추천</strong>이라 볼 수 있다.
  </li>
    <ul>
      <li>
        곡선으로 둘러 쌓인 영역을 AUC(Area Under Curve)라 하며 0과 1사이의 값을 갖고 면적이 넓을수록 예측 정확도가 높다.
      </li>
    </ul>
  <li>
    <strong>ROC의 AUC</strong>도 존재하는데 이는 <strong>음의 예와 양의 예가 비슷</strong>할 때 적합하다. 추천 시스템의 데이터셋이 음<strong>의 예가 양의 예보다 많을 경우</strong>에는 <strong>PR 곡선의 AUC</strong>를 보는 편이 적절하다.
  </li>
</ul>

<h4>MRR@K</h4>
<p>

$$ \text{MRR@K}=\frac{1}{|U|}\sum_{u\in U} \frac{1}{k_u} $$

- $ U $: 사용자 전체의 집합
- $ k_u $: 사용자 𝑢 에 대해 순위 K 이내에서 최초로 등장한 적합 아이템의 순위 위치

MRR(Mean Reciprocal Rank)은 사용자 순위에 대해 최초의 적합 아이템이 순위에서 얼마나 상위에 위치하는지를 평가한다. 최초 적합 아이템의 등장을 사용하기에 정확도 보다는 응답 반환 속도 측정에 적합하다.

</p>

```python
def rr_at_k(user_relevances: List[int], k: int) -> float:
    nonzero_indices = np.asarray(user_relevances).nonzero()[0]
    if nonzero_indices.size > 0 and nonzero_indices[0] + 1 <= k:
        return 1.0 / (nonzero_indices[0] + 1.0)
    return 0.0
print(rr_at_k([0, 1, 0], 2))

def mrr_at_k(users_relevances: List[List[int]], k: int) -> float:
    return float(
        np.mean(
            [rr_at_k(user_relevances, k) for user_relevances in 
            users_relevances]
        )
    )

print(mrr_at_k([[1, 0, 0], [0, 1, 0], [0, 0, 1]], 2))
```

<h4>AP@K</h4>
<p>

$$ AP(u)@K=\frac{1}{\sum_{k=1}^Kc_{u,k}}\sum_{k=1}^Kc_{u,k}\cdot Precision@K $$

AP@K(평균 정밀도, Average Precision)는 K번째까지에 대한 각 적합 아이템까지의 Precision을 평균한 값이다.

- $ K $: 순위의 길이
- $ c_{u,k}=\begin{cases}1: \text{사용자} u\text{의 } k\text{번째 아이템이 적합} \\ 0:\text{그 외}\end{cases}$

정답이 나올 때마다 해당 시점의 Precision을 계산하고 평균한 값으로 정답을 얼마나 일찍 그리고 꾸준히 잘 맞추고 있는가를 의미한다.

</p>

```python
def ap_at_k(user_relevances: List[int], k: int) -> float:
    # K개 안에 적합 아이템이 하나도 없으면 0
    if sum(user_relevances[:k]) == 0:
        return 0.0

    # 적합(1)인 위치의 인덱스만 추출
    nonzero_indices = np.asarray(user_relevances[:k]).nonzero()[0]

    # 각 적합 위치에서의 Precision 계산 후 평균
    return sum(
        [sum(user_relevances[:idx + 1]) / (idx + 1) for idx in nonzero_indices]
    ) / len(nonzero_indices)

print(ap_at_k([0, 1, 0, 1, 0], 5))
```