<ul>
  <li>
    https://github.com/moseskim/RecommenderSystems 에서 도서의 코드 원본을 제공한다. 주석 설명 표함 미포함 코드도 많기에 꼭 참고할 것을 권한다.
  </li>
  <li>
    알고리즘에 사용되는 수식은 개요만 다루고 있기에 수식적으로 더 알고 싶다면 다른 도서를 참고하는 것을 권한다.
  </li>
</ul>

<br><br>

<h1>1. 알고리즘 비교 (P102)</h1>
<ul>
  <li>
    앞으로 설명할 추천 알고리즘들이 장표로 정리되어 있다.
  </li>
</ul>

<br><br>

<h1>2. MovieLens 데이터셋</h1>
<ul>
  <li>
    영화에 대한 태그 정보가 있어 협조 필터링 뿐만 아니라 콘텐츠 기반 추천 알고리즘도 실험할 수 있어 선정되었다.
  </li>
</ul>

<br>

<h2>2-1. 데이터 다운로드</h2>

```bash
# 실습을 진행할 디렉터리에서 다음을 수행한다.
# 1) 폴더 만들기 (프로젝트 내부)
mkdir -p ./data

# 2) 다운로드
curl -L -o ./data/ml-10m.zip https://files.grouplens.org/datasets/movielens/ml-10m.zip

# 3) 압축 해제
unzip -n ./data/ml-10m.zip -d .
```

<br>

<h2>2-2. MovieLens 데이터 개요</h2>
<ul>
  <li>
    탐색적 데이터 분석(EDA, Exploratory Data Analytics)를 위한 데이터 로드
  </li>
</ul>

```python
import pandas as pd
# 1. 영화 정보 로딩(10681 작품)
# movieID와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv("data/ml-10M100K/movies.dat",
                    names=m_cols,
                    sep'::',
                    encoding='latin-1',
                    engine='python')

# genre를 list 형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))
movies.head()
```

```python
# 2. 사용자가 부여한 영화의 태그 정보 코딩
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat',
                                names=t_cols,
                                sep="::",
                                engine='python')

# tag를 소문자로 변경한다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()
user_tagged_movies.head()
```

```python
# 3. 태그 정보 확인
# 태그 종류=15324
print(f'태그 종류={len(user_tagged_movies.tag.unique())}')
# 태그 레코드 수=95500
print(f'태그 레코드 수={len(user_tagged_movies)}')
# 태그가 붙어 있는 영화 수=7601
print(f'태그가 붙어 있는 영화 수={len(user_tagged_movies.movie_id.unique())}')
```

```python
# 4. 태그 데이터도 다루기 쉽게 영화 ID별로 부여된 태그를 리스트 형시긍로 저장.
# tag를 영화별로 list 형식으로 저장.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

# 태그 정보를 결합.
movies = movies.merge(movie_tags, on='movie_id', how='left')
movies.head()
```

<h3>평갓값 데이터</h3>

```python
# 1. 평갓값 데이터 로딩
# 평갓값 데이터 로딩(데이터양이 많으므로 환경에 따라 로딩에 시간이 걸린다).
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', 
                        names=r_cols,
                        sep='::',
                        engine='python')
ratings.head()
```

```python
# 2. 데이터양이 많기에 사용자 수를 1000명으로 줄여 시험.
#     - 실무에서 데이터양이 많으면 일부 샘플링하여 테스트하고 성능이 좋은 모델을 채택.
#     - 단, sampling 과정에서 편향이 생기지 않도록 하는 것이 중요하다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합.
movielens = ratings.merge(movies, on='movie_id')
movielens.head()
```

<h3>사용자</h3>

```python
# 3. 사용자 1000 명이 영화를 평가한 결과
import numpy as np
movielens.groupby('user_id'). \
                  agg({'movie_id': len}). \
                  agg({'movie_id':[min, max, np.mean, len]})
```

<h3>영화</h3>

```python
# 4. 평가된 영화에 대한 결과.
movielens.groupby('movie_id'). \
                  agg({'user_id':len}). \
                  agg({'user_id':['min', 'max', 'mean', 'len']})
```

```python
# 5. 평갓값 결과 확인.
print(f'평갓값 수={len(movielens)}')
movielens.groupby('rating').agg({'movie_id':len})
```

<br>

<h2>2-3. 평가 방법</h2>
<ul>
  <li>
    각 추천 알고리즘의 성능을 측정하는 방법에 대해 설명한다. (평가 방법 상세는 7장).
  </li>
  <li>
    사용자당 최소 20개 이상의 영화를 평가 했기에 사용자가 최근 평가한 5개의 영화의 평갓값을 테스트용으로 사용하고, 나머지 데이터를 학습용으로 활용한다.
  </li>
</ul>

```python
# 1. 평가 준비를 위한 데이터 처리
"""
- 학습용과 테스트용으로 데이터를 분할한다.
- 각 사용자가 가장 최근에 평가한 5건의 영화를 평가용으로, 그외를 학습용으로 사용한다.
- 각 사용자가 평가한 영화의 순서를 계산한다.
- 직전에 평가한 영화부터 순서를 부여해 나간다.
"""

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 2. 추천 지표를 활용하여 평가
#   - RMSE: 기본적으로 사용하는 평가 지표
#   - Precision@K: 사용자에게 K 개의 아이템을 추천했을 때 그 중 실제로 선호하는 아이템의 
#                  비율 지표
#   - Recall@K: 사용자에게 K 개의 아이템을 추천했을 때 `사용자가 선호하는 아이템 그룹` 중 
#               몇 개가 맞았는지를 나타내는 비율 지표
from typing import List
from sklearn.metrics import mean_squared_error
def calc_rmse(self, true_rating: List[float],
              pred_rating: List[float]) -> float:
    return np.sqrt(mean_squared_error(true_rating, pred_rating))

# Precision@K와 Recall@K의 평균을 평가지표를 실무에서는 함께 사용하는 것을 권장.
```

```python
# 3. rmse외의 평가지표 함수 작성.
# 개별 사용자의 Recall@K 계산
def calc_recall_at_k(
    true_user2items: Dict[int, List[int]],
    pred_user2items: Dict[int, List[int]],
    k: int
) ->float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 recall@k를 계산
    for user_id in true_user2items.keys():
        r_at_k = _recall_at_k(true_user2items[user_id],
                              pred_user2items[user_id], k)
        scores.append(r_at_k)
    return np.mean(scores)

# 전체 Recall@K 평균 계산
def _recall_at_k(self, true_items: List[int],
                 pred_items: List[int], k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0

    r_at_k = (len(set(true_items) & set(pred_items[:k]))) / len(true_items)
    return r_at_k

# 개별 Precision@K 계산.
def calc_precision_at_k(
    true_user2items: Dict[int, List[int]],
    pred_user2items: Dict[int, List[int]],
    k: int
) ->float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 precision@k을 계산
    for user_id in true_user2items.keys():
        p_at_k = _precision_at_k(true_user2items[user_id],
                                 pred_user2items[user_id], k)
        scores.append(p_at_k)
    return np.mean(scores)

# 전체 Precision@K 평균 계산.
def _precision_at_k(true_items: List[int],
                    pred_items: List[int], k: int) -> float:
    if k == 0:
        return 0.0
    p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k
    return p_at_k
```

<br>

<h2>2-4. 통일된 포맷을 활용한 계산</h2>
<ul>
  <li>
    다양한 모델을 하나의 통합된 클래스에 적용해서 간단하게 적용하도록 하기 위해서는 통일된 형식을 잘 구성해 두어야 한다.
  </li>
</ul>

```python
# 1. 다양한 모델 적용을 위한 통합된 형식.

# 데이터 로딩과 평가 지표 계산 공통 모듈 로딩
from util.data_loader import DataLoader
from util.metric_calculator import MetricCalculator

# 1-1) MovieLens 데이터 로딩
data_loader = DataLoader(num_users=1000, 
                         num_test_items=5,
                         data_path='../data/ml-10M100K/')
movielens = data_loader.load()

# 1-2) 각종 알고리즘 구현
recommender = XXXRecommender()
recommend_result = recommender.recommend(movielens)

# 1-3) 평가 지표 계산
metric_calculator = MetricCalculator()
metrics = metric_calculator.calc(
    movielens.test.rating.tolist(), recommend_result.rating.tolist(),
    movielens.test_user2items, recommend_result.user2item, k=10)
print(metrics)
```

<h3>2-4-1. MovieLens 데이터 로딩</h3>
<ul>
  <li>
    앞서 소개한 데이터 전처리 과정을 하나의 클래스(DataLoader)로 구현한다.
  </li>
  <li>
    train_test_split과 같은 방법을 사용하지 않고 직접 클래스를 구현하는 이유는 추천 시스템에서는 일반적인 분할보다 <strong>사용자 단위로 특수한 로직</strong>이 필요하기 때문이다.
  </li>
</ul>

```python
import pandas as pd
import os
from util.models import Dataset

class DataLoader:
    def __init__(self, num_users: int = 1000, num_test_items: int = 5, data_path: str = "../data/ml-10M100K/"):
        self.num_users = num_users
        self.num_test_items = num_test_items
        self.data_path = data_path

    def load(self) -> Dataset:
        ratings, movie_content = self._load()
        movielens_train, movielens_test = self._split_data(ratings)
        # 순위용 평가 데이터는 각 사용자의 평갓값이 4 이상인 영화만 정답으로 한다.
        # 키는 사용자 ID, 값은 사용자가 고평가한 ID 리스트.
        movielens_test_user2item = (
            movielens_test[movielens_test.rating >= 4].groupby(
                "user_id").agg({"movie_id": list}).to_dict()
        )
        return Dataset(movielens_train, movielens_test, movielens_test_user2item, movie_content)

    def _split_data(self, movielens: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):
        # 학습용과 테스트용 데이터를 분할한다.
        # 각 사용자가 가장 최근에 평가한 5건을 평가용으로 사용하고 그 외는 학습용으로 사용한다.
        # 먼저 각 사용자가 평가한 영화의 순서를 계산한다.
        # 직전에 평가한 영화부터 순서를 부여한다. (0 부터 시작).
        movielens["rating_order"] = movielens. \
                                    groupby("user_id")["timestamp"]. \
                                    rank(ascending=False, method="first")
        movielens_train = movielens[movielens["rating_order"] > self.num_test_items]
        movielens_test = movielens[movielens["rating_order"] <= self.num_test_items]
        return movielens_train, movielens_test

    def _load(self) -> (pd.DataFrame, pd.DataFrame):
        # 영화 정보 로딩(10197 작품)
        # movie_id와 제목만 사용
        m_cols = ["movie_id", "title", "genre"]
        movies = pd.read_csv(
            os.path.join(self.data_path, "movie.dat"), names=m_cols,
            sep="::", encoding="latin1-1", engine="python"
        )
        # genre를 list 형식으로 저장.
        movies["genre"] = movies.genre.apply(lambda x:list(x.split("|")))
        # 사용자가 부여한 영화 태그 정보 로딩
        t_cols = ["user_id", "movie_id", "tag", "timestamp"]
        user_tagged_movies = pd.read_csv(
            os.path.join(self.data_path, "tags.dat"), names = t_cols,
            sep = "::", engine = "python"
        )
        
        # tag를 소문자로 한다.
        user_tagged_movies["tag"] = user_tagged_movies["tag"].str.lower()
        movie_tags = user_tagged_movies.groupby("movie_id").agg({"tag":list})
        
        # 태그 정보를 결합한다.
        movies = movies.merge(movie_tags, on="movie_id", how="left")
        
        # 평가 데이터 로딩
        r_cols = ["user_id", "movie_id", "rating", "timestamp"]
        ratings = pd.read_csv(os.path.join(self.data_path,
        "ratings.dat"), names=r_cols, sep="::", engine="python")
        # 사용자 수를 num_users로 줄인다.
        valid_user_ids = sorted(ratings.user_id.unique())[:
                        self.num_users]
        ratings = ratings[ratings.user_id <= max(valid_user_ids)]
        
        # 위의 데이터를 결합한다.
        movielens_ratings = ratings.merge(movies, on="movie_id")
        return movielens_ratings, movies
```

```python
# 2. 데이터셋 생성자
#   - @dataclasses가 생성자를 자동 생성한다.
import dataclasses
from typing import Dict, List
@dataclasses.dataclass(frozen=True)

# 추천 시스템 학습과 평가에 사용하는 데이터셋
class Dataset:
    
    # 학습용 평갓값 데이터셋
    train: pd.DataFrame
    # 테스트용 평갓값 데이터 셋
    test:pd.DataFrame

    # 순위 지표 테스트 데이터셋
    #   - key: 사용자 ID, value: 사용자가 높에 평가한 아이템 ID 리스트
    test_user2item: Dict[int, List[int]]
    # 아이템 콘텐츠 정보
    item_content: pd.DataFrame
```

<h3>2-4-2. 각종 알고리즘 구현</h3>
<ul>
  <li>
    dataset을 받아 테스트 데이터의 추천 결과를 반환하는 알고리즘을 구현한다.
  </li>
  <li>
    각 알고리즘은 BaseRecommender 클래스를 상속하는 형태로 구현한다.
  </li>
</ul>

```python
# 1. 각 알고리즘을 적용할 공통 클래스 생성
import sys, os
sys.path.append(os.path.abspath("..")) 

from abc import ABC, abstractmethod
from util.data_loader import DataLoader
from util.metric_calculator import MetricCalculator
from util.models import Dataset, RecommendResult

class BaseRecommender(ABC):
    @abstractmethod
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        pass

    def run_sample(self) -> None:
        # MovieLens의 데이터 취득
        movielens = DataLoader(num_users=1000, num_test_items=5,
                               data_path="../data/ml-10M100K/").load()

        # 추천 계산
        recommend_result = self.recommend(movielens)

        # 추천 결과 평가
        metrics = MetricCalculator().calc(
            movielens.test.rating.tolist(),
            recommend_result.rating.tolist(),
            movielens.test_user2item,
            recommend_result.user2item,
            k=10
        )
        print(metrics)
```

<h3>2-4-3. 평가 지표 계산</h3>
<ul>
  <li>
    추천 결과를 기반으로 MetricCaculator를 사용해 RMSE와 순위 지표의 Precision@K 그리고 Recall@K를 계산한다.
  </li>
</ul>

```python
# 1. 평가 지표 계산 클래스
import numpy as np
from sklearn.metrics import mean_squared_error
from util.models import Metrics
from typing import Dict, List

class MetricCalculator:
    def calc(
        self,
        true_rating: List[float],
        pred_rating: List[float],
        true_user2item: Dict[int, List[int]],
        pred_user2item: Dict[int, List[int]],
        k: int,
    ) -> Metrics:
        rmse = self._calc_rmse(true_rating, pred_rating)
        precision_at_k = self._calc_precision_at_k(true_user2items,
                         pred_user2items, k)
        recall_at_k = self._calc_recall_at_k(true_user2items, pred_user2items, k)
        return Metrics(rmse, precision_at_k, recall_at_k)

    def _precision_at_k(self, true_items: List[int], pred_items: List[int], k:int) -> float:
        if k == 0:
            return 0.0

        p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k
        return p_at_k

def _recall_at_k(self, true_items: List[int], pred_items: List[int], k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0

    r_at_k = len(set(true_items) & set(pred_items[:k])) / len(true_items)
    return r_at_k

def _calc_rmse(self, true_rating: List[float], pred_rating: List[float]) -> float:
    return np.sqrt(mean_squared_error(true_rating, pred_rating))

def _calc_recall_at_k(
    self, true_user2items: Dict[int, List[int]], pred_user2items: Dict[int, List[int]], k: int) -> float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 recall@k를 계산한다.
    for user_id in true_user2items.keys():
        r_at_k = self._recall_at_k(true_user2items[user_id], pred_user2items[user_id], k)
        scores.append(r_at_k)
    return np.mean(scores)

def _calc_precision_at_k(
    self, true_user2items: Dict[int, List[int]], pred_user2items: Dict[int, List[int]], k:int) -> float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 precision@k를 계산한다.
    for user_id in true_user2items.keys():
        p_at_k = self._precision_at_k(true_user2items[user_id], pred_user2items[user_id], k)
        scores.append(p_at_k)
    return np.mean(scores)
```

<br><br>

<h1>3. 무작위 추천</h1>
<ul>
  <li>
    난수 데이터를 활용한 무작위 모델 테스트
  </li>
    <ul>
      <li>
        MovieLens의 평가값이 0.5 ~ 5.0 이기에 <strong>0.5 ~ 5.0의 난수</strong>를 생성한다.
        <br>→ 학습용 데이터의 사용자와 아이템으로 <strong>사용자 X 아이템 행렬</strong>을 만들고 난수를 저장한다.
        <br>→ <strong>순위 지표 계산용</strong>으로 pred_user2items 딕셔너리를 생성한다.
        <br>→ 생성한 dict의 <strong>key에 user_id</strong>, <strong>value에 아직 사용자가 평가하지 않은 영화</strong>부터 무작위로 10</strong>개의 영화를 저장한다.
      </li>
    </ul>
</ul>

```python
# 1. 테스트를 위한 데이터 준비
import pandas as pd

# movieId와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names = m_cols, sep='::', engine='python')

# genre를 list 형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩한다.
t_cols = ['user_id', 'movie_id', 'tag', 'timestmamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

# 태그 정보를 결합한다.
movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩한다.
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터량이 많기에 사용자수를 1000으로 줄여서 시험한다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합한다.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movies.movie_id.unique())}')

# 학습용과 데이터용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평가용으로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작).

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp'] <= 5]
```

```python
# 2. 사용자 ID와 아이템 ID에 대해 0부터 시작하는 인덱스를 할당한다.
unique_user_ids = sorted(movielens_train.user_id.unique())
unique_movie_ids = sorted(movielens_train.movie_id.unique())
user_id2index = dict(zip(unique_user_ids, range(len(unique_user_ids))))
movie_id2index = dict(zip(unique_movie_ids, range(len(unique_movie_ids))))
```

```python
# 3. 사용자 X 아이템 행렬에서 각 예측 평갓값은 0.5 ~ 5.0의 균등 난수로 한다.
import numpy as np

pred_matrix = np.random.uniform(0.5, 5.0, (len(unique_user_ids), len(unique_movie_ids)))
```

```python
# 4. rmse 평가용으로 테스트 데이터에 나타나는 사용자와 아이템의 예측 평갓값을 저장한다.
movie_rating_predict = movielens_test.copy()
pred_results = []
for i, row in movielens_test.iterrows():
    user_id = row["user_id"]
    # 테스트 데이터의 아이템 ID가 학습용에 나타나지 않는 경우에도 난수를 저장한다
    if row["movie_id"] not in movie_id2index:
        pred_results.append(np.random.uniform(0.5, 5.0))
        continue
    # 테스트 데이터에 나타난 사용자 ID와 아이템 ID의 인덱스를 얻어, 평갓값 행렬의 값을 얻는다
    user_index = user_id2index[row["user_id"]]
    movie_index = movie_id2index[row["movie_id"]]
    pred_score = pred_matrix[user_index, movie_index]
    pred_results.append(pred_score)
movie_rating_predict["rating_pred"] = pred_results
```

```python
# 5. 순위 평가용 데이터를 작성한다.
from collections import defaultdict

# 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서 10작품을 무작위로 선택.
# key는 사용자 ID, value는 추천 아이템 ID 리스트.
pred_user2items = defaultdict(list)
# 사용자는 이미 평가한 영화를 얻는다.
user_evaluated_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()
for user_id in unique_user_ids:
    user_index = user_id2index[user_id]
    movie_indexes = np.argsort(-pred_matrix[user_index, :])
    for movie_index in movie_indexes:
        movie_id = unique_movie_ids[movie_index]
        if movie_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break
pred_user2items
```

```python
# 6. user_id=2인 사용자가 학습 데이터에서 평가를 부여한 영화 목록
movielens_train[movielens_train.user_id==2]
```

<br><br>

<h1>4. 통계 정보나 특정 규칙에 기반한 추천</h1>
<ul>
  <li>
    <strong>통계 추천 예</strong>
  </li>
    <ul>
      <li>
        직전 1개월의 총 매출 수, 열람 수, 사용자에 따른 평갓값의 평균 등 <strong>서비스의 통계 정보</strong>를 사용해 아이템을 나열하여 사용자에게 추천.
      </li>
      <li>
        아이템 가격이나 크기와 같이 <strong>특정 속성 순서</strong>로 나열해 사용자에게 추천
      </li>
      <li>
        사용자의 나이 등과 같은 <strong>특정 속성 정보</strong>에 기반해 다른 아이템 추천.
      </li>
    </ul>
  <li>
    통계 정보나 아이템의 속성 정보에 기반해 나열하는 추천은 사용자에 의존하지 않기에 <strong>개인화를 수행하지 않는다</strong>.
  </li>
    <ul>
      <li>
        따라서 추천 시스템의 콘텍스트에 관계없이 시스템이 갖고 있는 데이터만을 활용하기에 구현하기가 비교적 쉽다.
      </li>
    </ul>
  <li>
    비교적 단순한 알고리즘은 <strong>사용자 또한 어떤 구조로 추천되는지 이해하기가 쉽기 떄문에</strong> 사용자의 구매 행동과 연결되는 경우가 많아 얕볼 수 없다.
  </li>
  <li>
    사용자의 속성 정보를 기반으로 다른 아이템을 추천하는 경우, 사용자의 속성 정보를 기반으로 몇 세그먼트로 나누어 각각의 <strong>세그먼트 별로 추천을 진행</strong>한다.
  </li>
  <li>
    사용자의 나이나 성별, 거주지 등의 인구 <strong>통계학적 데이터</strong>에 기반해 아이템을 추천하는 것을 <strong>데모그래픽 필터링(demographic filtering)</strong>이라 한다.
  </li>
    <ul>
      <li>
        서비스나 사용자의 성질에 따라 데모그라픽 정보에 <strong>정확하지 않은 정보</strong>가 입력될 수 있음을 주의해야 한다.
      </li>
      <li>
        <strong>공평성(fairness) 관점</strong>에서 데모그래픽 데이터를 사용할 때 주의가 필요하다. 예를 들면 사용자에 성별을 묻는 것이 해당한다.
      </li>
    </ul>
</ul>

```python
# 1. 과거에 남긴 평갓값 중 값이 높은 순서대로 추천하는 예
import numpy as np

# 평갓값이 높은 영화 확인.
movie_stats = movielens.train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
movie_stats.sort_values(by=('rating', 'mean'), ascending=False).head()
```

```python
# 2. 평가 수가 적은 데이터가 많기 때문에 임곗값을 활용해 일정 이상의 평가 수가 있는 영화만 필터링한다.
#   - 추천 규칙을 정할 때에는 집계 기간과 다양성을 고려해야 한다.
movie_stats = movielens.train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
atleast_flg = movie_stats['rating']['size'] >= 100
movies_sorted_by_rating = movie_stats[atleast_flg].sort_values(
    by=('rating', 'mean'), ascending=False)
movies_sorted_by_rating.head()
```

```python
# 3. 평갓값이 높은 순의 추천 시스템 구현을 위해 데이터 전처리
import pandas as pd

# movieID와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names=m_cols, sep='::', encoding='latin-1', engine='python')

# genre를 list 형식으로 저장한다.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩한다.
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag': list})

# 태그 정보를 결합한다.
movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩한다.
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터양이 많으므로 사용자수를 1000으로 줄여서 시험한다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터화 평가 데이터를 결합한다.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movielens.movie_id.unique())}')

# 학습용과 테스트용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평가용으로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작)

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 4. 평가 수의 임곗값
minimum_num_rating = 200
```

```python
# 5. 평갓값의 평균이 높은 영화를 확인한다.
import numpy as np

# 평가 수가 1건인 영화가 상위에 여럿 나타난다.
movie_stats = movielens_train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
movie_stats.sort_values(by=('rating', 'mean'), ascending=False).head()
```

```python
# 6. 임곗값을 도입해 평가 수가 적은 영화를 제거한다.
#   - 쇼생크 탈출이나 7인의 사무라이 등 익숙한 영화가 상위에 나타난다.
movie_stats = movielens_train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
atleast_flg = movie_stats['rating']['size'] >= 100
movies_sorted_by_rating = movie_stats[atleast_flg].sort_values(by=('rating', 'mean'), ascending=False)
movies_sorted_by_rating.head()
```

```python
# 7. 평갓값이 높은 순의 추천 시스템의 성능을 측정.
import numpy as np

# 각 아이템별로 평균 평갓값을 계산하고, 해당 평균 평갓값을 예측값으로 사용한다.
movie_rating_average = movielens_train.groupby("movie_id").agg({"rating": np.mean})

# 테스트 데이터에 예측값을 저장한다. 테스트 데이터에만 존재하는 아이템의 예측 평갓값은 0으로 한다.
movie_rating_predict = movielens_test.merge(
    movie_rating_average, on="movie_id", how="left", suffixes=("_test", "_pred")
).fillna(0)
```

```python
# 8. 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서 평갓값이 높은 10작품으로 한다

from collections import defaultdict

# 단, 평가 건수가 작으면 노이즈가 크므로 minimum_num_rating건 이상 평가가 있는 영화로 필터링한다
pred_user2items = defaultdict(list)
user_watched_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()
movie_stats = movielens_train.groupby("movie_id").agg({"rating": [np.size, np.mean]})
atleast_flg = movie_stats["rating"]["size"] >= minimum_num_rating
movies_sorted_by_rating = (
    movie_stats[atleast_flg].sort_values(by=("rating", "mean"), ascending=False).index.tolist()
)

for user_id in movielens_train.user_id.unique():
    for movie_id in movies_sorted_by_rating:
        if movie_id not in user_watched_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break
pred_user2items
```

```python
# 9. user_id=2인 사용자가 학습 데이터에 평가를 부여한 영화 목록
movielens_train[movielens_train.user_id==2]
```

```python
# 10. 특정 사용자 혹은 아이템의 추천 목록
pred_user2items[2]

# user_id=2에 대한 추천(318, 50, 527)
movies[movies.movie_id.isin([318, 50, 527])]
```

<br><br>

<h1>5. 연관 규칙</h1>
<ul>
  <li>
    <strong>연관 규칙(association rule)</strong>에서는 대량의 구매 이력 데이터로부터 '아이템 A와 아이템 B는 <strong>동시에 구입</strong>하는 경우가 많다'는 규칙을 발견한다.
  </li>
    <ul>
      <li>
        의회의 조합을 발견할 수 있다는 특징이 있다.
      </li>
      <li>
        연관 규칙 자체는 오래전부터 있었으나 계산 방법 자체는 매우 간단하여 SQL로도 구현할 수 있어 널리 사용되고 있다.
      </li>
    </ul>
  <li>
    연관 규칙에는 <strong>지지도(support)</strong>, <strong>확신도(confidence)</strong> 그리고 <strong>리프트값(lift)</strong>이라는 중요한 세 가지 개념이 있다.
  </li>
</ul>

<br>

<h2>5-1. 지지도</h2>
<ul>
  <li>
    <strong>지지도</strong>란 어떤 아이템이 <strong>전체 중에서 출현한 비율</strong>을 의미한다.
  </li>
  <li>
    동시에 출현하는 빈도를 계산할 때에도 <strong>and 조건</strong>을 적용해 계산할 수 있다.
  </li>
</ul>

$$ \text{Support(A)} = \frac{\text{Count}(A)}{\text{Total count}} $$

<br>

<h2>5-2. 확신도</h2>
<ul>
  <li>
    <strong>확신도</strong>는 <strong>아이템 A가 나타났을 때 아이템 B가 나타날 비율</strong>을 의미한다.
  </li>
  <li>
    A를 <strong>조건부(antecedents)</strong>, B를 <strong>귀결부(consequents)</strong>라 한다.
  </li>
</ul>

$$ \text{Confidence}(A \Rightarrow B) = \frac{\text{Count}(A \text{ and } B)}{\text{Count}(A)} $$

<br>

<h2>5-3. 리프트값</h2>
<ul>
  <li>
    <strong>리프트값</strong>이란 아이템 A와 아이템 B의 출현이 어느 정도 <strong>상관관계</strong>를 갖는지를 의미한다.
  </li>
    <ul>
      <li>
        <strong>리프트 = 1</strong>: 아이템 A와 아이템 B는 <strong>독립적</strong>이다.
      </li>
      <li>
        <strong>리프트 > 1</strong>: 아이템 A와 아이템 B는 <strong>양의 상관관계</strong>를 갖는다.
      </li>
      <li>
        <strong>0 <= 리프트 < 1</strong>: 아이템 A와 아이템 B는 <strong>음의 상관관계</strong>를 갖는다.
      </li>
    </ul>
  <li>
    리프트값에 <strong>log</strong>를 취하면 <strong>점별 상호정보량(Pointwise Mutual Information, PMI)</strong>이 되며 <strong>word2vec</strong> 알고리즘이 이를 활용한다.
  </li>
  <li>
    아이템이 세 개 이상인 경우에도 리프트값을 계산할 수 있다.
  </li>
</ul>

$$ \text{Lift}(A \Rightarrow B) = \frac{\text{Support}(A \text{ and } B)}{\text{Support}(A) * \text{Support}(B)} $$


$$ \text{Lift}((A \text{ and } B) \Rightarrow C) = \frac{\text{Support}(A \text{ and } B \text{ and } C)}{\text{Support}(A \text{ and } B) * \text{Support}(C)} $$

<br>

<h2>5-4. Apriori 알고리즘을 활용한 고속화</h2>
<ul>
  <li>
    리프트값의 문제는 아이템의 수와 사용자의 수가 커질수록 <strong>조합 방법이 기하급수적으로 늘어나기에</strong> 계산이 어렵다는 것이다.
  </li>
  <li>
    위의 문제를 개선하기 위해 <strong>Apriori 알고리즘</strong>이 제안되었다. Apriori 알고리즘은 <strong>지지도가 일정 이상</strong>인 아이템이나 아이템의 조합만 계산하기에 비교적 빠르다.
  </li>
    <ul>
      <li>
        <strong>지지도의 임곗값</strong>을 결정하는 것이 중요하다. 너무 임곗값이 높으면 일부 인기 아이템만 추천되고, 반대로 낮다면 다시 계산 시간이 증가한다.
      </li>
    </ul>
</ul>

```python
# 1. Movielens 데이터 로딩
import pandas as pd

# movieID와 제목만 사용.
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names=m_cols, sep='::', encoding='latin-1', engine='python')

# genre를 list형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩.
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터량이 많으므로 사용자 수를 1000으로 줄여서 시험.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movielens.movie_id.unique())}')

# 학습용과 테스트용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평강용르로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작)

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 사용자 X 영화 행렬 형식으로 변환한다.
user_movie_matrix = movielens_train.pivot(index='user_id', columns='movie_id', values='rating')

# 라이브러리를 사용하기 위해 4 이상의 평갓값은 1, 4 미만의 평갓값과 결손값은 0으로 한다.
user_movie_matrix[user_movie_matrix < 4] = 0
user_movie_matrix[user_movie_matrix.isnull()] = 0
user_movie_matrix[user_movie_matrix >= 4] = 1

user_movie_matrix.head()
```

```python
# Association 규칙 라이브러리 설치
# !pip install mlxtend
```

```python
from mlxtend.frequent_patterns import apriori

# 지지도가 높은 영화를 표시
freq_movies = apriori(
    user_movie_matrix, min_support=0.1, use_colnames=True)
freq_movies.sort_values('support', ascending=False).head()
```

```python
# movie_id = 593의 제목 확인(양들의 침묵)
movies[movies.movie_id == 593]
```

```python
from mlxtend.frequent_patterns import association_rules

# Association 규칙 계산(리프트 값이 높은 순으로 표시).
rules = association_rules(freq_movies, metric='lift', min_threshold=1)
rules.sort_values('lift', ascending=False).head()[['antecedents', 'consequents', 'lift']]
```

```python
# movie_id = 4993, 5952의 제목 확인(반지의 제왕)
movies[movies.movie_id.isin([4993, 5952])]
```

```python
# 학습용 데이터 평갓값이 4 이상인 것만 얻는다.
movielens_train_high_rating = movielens_train[movielens_train.rating >= 4]
```

```python
# user_id=2인 사용자가 4이상의 평가를 남긴 영화 목록
movielens_train_high_rating[movielens_train_high_rating.user_id==2]
```

```python
# user_id=2의 사용자가 4 이상의 평가를 남긴 영화 목록
user2_data = movielens_train_high_rating[movielens_train_high_rating.user_id==2]

# 사용자가 최근 평가한 4개의 영화 얻기
input_data = user2_data.sort_values("timestamp")["movie_id"].tolist()[-5:]

# 해당 영화들이 조건부로 포함된 Association 규칙을 추출.
matched_flags = rules.antecedents.apply(lambda x: len(set(input_data) & x)) >= 1
rules[matched_flags]
```

```python
from collections import defaultdict, Counter

# Association 규칙의 귀결부의 영화를 리스트에 저장.
# 같은 영화가 여러 차례 귀결부에 나타날 수 있다.

consequent_movies = []
for i, row in rules[matched_flags].sort_values("lift", ascending=False).iterrows():
    consequent_movies.extend(row["consequents"])

# 귀결부에서의 출현 빈도 카운트
counter = Counter(consequent_movies)
counter.most_common(10)
```

```python
# movie_id=1196이 92번 귀결부에 출현하기에 user_id=2에는 movie_id=1196(Star Wars: Episode V)가 추천 후보가 된다.
# (user_id=2의 학습 데이터에는 Star Wars 에피소드 4, 6의 평가가 높다)
movies[movies.movie_id == 1196]
```

```python
# 추천하는 방법에는 lift 값이 높은 것을 추출하는 방법 등이 있다.
# 몇 가지 방법을 시도해 보고 자사의 데이터에 맞는 방법을 선택한다.

# Association 규칙을 사용해 각 사용자가 아직 평가하지 않은 영화 10편을 추천한다.
pred_user2items = defaultdict(list)
user_evaluated_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()

for user_id, data in movielens_train_high_rating.groupby("user_id"):
    # 사용자가 최근 5편의 영화를 얻는다.
    input_data = data.sort_values("timestamp")["movie_id"].tolist()[-5:]
    # input_data의 영화들이 조건부에 1편이라도 포함되어 있는 Association 규칙을 추출한다.
    matched_flags = rules.antecedents.apply(lambda x: len(set(input_data) & x)) >= 1

    # Association 규칙의 귀결부의 영화를 리스트로 저장하고, 출판 빈도 순으로 배열한다.
    # 사용자가 아직 평가하지 않았다면 추천 리스트에 추가한다.
    consequent_movies = []
    for i, row in rules[matched_flags].sort_values("lift", ascending=False).iterrows():
        consequent_movies.extend(row["consequents"])
    # 출현 빈도를 센다.
    counter = Counter(consequent_movies)
    for movie_id, movie_cnt in counter.most_common():
        if movie_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        # 추천 리스트가 10편이 되면 종료.
        if len(pred_user2items[user_id]) == 10:
            break

# 각 사용자에 대한 추천 리스트
pred_user2items
```

```python
# user_id=2에 대한 추천(1196, 593, 1198)
movies[movies.movie_id.isin([1196, 593, 1198])]

# apriori(user_movie_matrix, min_support=0.1, use_colnames=True)
# association_rules(freq_movies, metric='lift', min_threshold=1)
# min_support와 min_threshold가 중요한 파라미터이기에 바꾸어 가면서 테스트.
```

<br><br>

<h1>6. 사용자-사용자 메모리 기반 방법 협조 필터링</h1>
<ul>
  <li>
    메모리 기반 방식은 사용자 데이터를 축적하고 있다 <strong>추천되는 시점</strong>에 모든 데이터를 계산한다. 따라서 다른 알고리즘에 비해 <strong>시간</strong>이 더 소모된다.
  </li>
  <li>
    추천의 순서는 다음과 같다.
  </li>
    <ul>
      <li>
        미리 얻은 평갓값을 사용해 <strong>사용자 사이의 유사도를 계산</strong>하고 추천받을 사용자와 <strong>기호 경향이 비슷한 사용자</strong>를 찾는다.
      </li>
      <li>
        기호 경향이 비슷한 사용자의 평갓값으로 추천받을 사용자의 <strong>미지의 아이템에 대한 예측 평갓값을 계산</strong>한다.
      </li>
      <li>
        예측 평갓값이 높은 아이템을 사용해 사용자에게 추천한다.
      </li>
    </ul>
  <li>
    사용자 사이의 유사도를 계산할 때에는 <strong>피어슨 상관 계수</strong>를 사용한다.
  </li>
</ul>

$$ p_{ax}=\frac{\sum_{y \in Y_{ax}}(r_{ay}-\bar r_a)(r_{xy}-\bar r_x)}{\sqrt{\sum_{y \in Y_{ax}}(r_{ay}-\bar r_a)^2} \sqrt{\sum_{y \in Y{ax}}(r_{xy}- \bar r_x)^2} } $$

```python
# 1. 피어슨 상관 계수 함수.
def pearson_coefficient(u: np.ndarry, v: np.ndarray) -> float:
    u_diff = u - np.mean(u)
    v_diff = v - np.mean(v)
    numerator = np.dot(u_diff, v_diff)
    denominator = np.sqrt(sum(u_diff**2))*np.sqrt(sum(v_diff**2))
    if denominator == 0:
        return 0.0
    return numerator / denominator
```

```python
# 2. 평갓값을 사용자 X 영화의 행렬로 변환
user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
user_id2index = dict(zip(user_movie_matrix.index,
                range(len(user_movie_matrix.index))))
movie_id2index = dict(zip(user_movie_matrix.columns,
                  range(len(user_movie_matrix.columns))))

# 예측 대상 사용자와 영화의 조합
movie_rating_predict = dataset.test.copy()
pred_user2items = defaultdict(list)

# 예측 대상 사용자 ID
test_users = movie_rating_predict.user_id.unique()

# 예측 대상 사용자(사용자 1)에게 주목한다.
for user1_id in test_users:
    similar_users = []
    similarities = []
    avgs = []

    # 사용자 1과 평갓값 행렬 내 다른 사용자(사용자 2)의 유사도를 산출한다.
    if user1_id == user2_id:
      continue

    # 사용자 1과 사용자 2의 평갓값 벡터
    u_1 = user_movie_matrix.loc[user1_id, :].to_numpy()
    u_2 = user_movie_matrix.loc[user2_id, :].to_numpy()

    # u_1과 u_2 모두 결손값이 없는 것만 추출한 벡터를 얻는다.
    common_items = (~np.isnan(u_1) & ~np.isnan(u_2))
    # 공통으로 평가한 아이템이 없으면 스킵한다.
    if not common_items.any():
        continue
    u_1, u_2 = u_1[common_items], u_2[common_items]

    # 피어슨 상관 계수를 사용해 사용자 1과 사용자 2의 유사도를 산출한다.
    rho_12 = pearson_coefficient(u_1, u_2)

    # 사용자 1과의 유사도가 0보다 크면 사용자 2를 비슷한 사용자로 간주한다.
    if rho_12 > 0:
        similar_users.append(user2_id)
        similarities.append(rho_12)
        avgs.append(np.mean(u_2))
```

<h4>사용자별 평균 평갓값에서 해당 아이템에 대한 평가가 얼마나 높은지 상대 적인 평갓값에 주목해 해당 값의 가중 평균을 얻어 평가한다.</h4>

$$ \hat r_{ay}=\hat r_a + \frac{\sum_{x \in X_y}p_{ax}(r_{xy}-\hat r_x)}{\sum_{x \in X_y}|p_{ax}|} $$

```python
# 사용자 1의 평균 평갓값
avg_1 = np.mean(user_movie_matrix.loc[user1_id, :].dropna().to_numpy())

# 예측 대상 영화의 ID
test_movies = movie_rating_predict[movie_rating_predict[
                  "user_id"]==user1_id].movie_id.values

# 예측할 수 없는 영화에 대한 평갓값은 사용자 1의 평균 평갓값으로 한다.
movie_rating_predict.loc[(movie_rating_predict[
"user_id"]==user1_id, "rating_pred"] = avg_ 1

if similar_users:
    for movie_id in test_movies:
        if movie_id in movie_id2index:
            r_xy = user_movie_matrix.loc[similar_users,
                    movie_id].to_numpy()
            rating_exists = ~np.isnan(r_xy)

            # 비슷한 사용자가 대상 영화에 대한 평갓값을 갖고 있지 않으면 skip
            if not rating_exists.any():
                continue
            
            r_xy = r_xy[rating_exists]
            rho_1x = np.array(similarities)[rating_exists]
            avg_x = np.array(avgs)[rating_exists]
            r_hat_1y = avg_1 + np.dot(rho_1x, (r_xy - avg_x)) / 
                      rho_1x.sum()
            
            # 예측 평갓값을 저장한다.
            movie_rating_predict.loc[(movie_rating_predict[
            "user_id"]==user1_id) & (movie_rating_predict[
            "movie_id"]==movie_id), "rating_pred"] = r_hat_1y
```

<p>다양한 라이브러리를 통해서도 메모리 기반 방법 협조 필터링을 구현할 수 있다. 하단은 Surprise를 사용한다.</p>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np

from surprise import KNNWithMeans, Reader
from surprise import Dataset as SurpriseDataset

np.random.seed(0)

class UMCFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        
        # 평갓값을 사용자 X 영화 행렬로 변환한다.
        user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))

        # 평갓값을 예측할 테스트용 데이터
        movie_rating_predict = dataset.test.copy()
        # 각 사용자에 대한 순위 형식의 추천 리스트를 저장하는 딕셔너리
        pred_user2items = defaultdict(list)

        # Surprise용으로 데이터를 가공한다.
        reader = Reader(rating_scale=(0.5, 5))
        data_train = SurpriseDataset.load_from_df(
            dataset.train[["user_id", "movie_id", "rating"]], reader
        ).build_full_trainset()

            sim_options = {
                "name": "pearson", # 유사도 계산 방법을 지정한다.
                "user_based": True # False로 하면 아이템 기반이 된다.
            }

            # 유사도 상위 30명의 사용자를 유사 사용자로 다룬다.
            knn = KNNWithMeans(k=30, min_k=1, sim_options=sim_options)
            knn.fit(data_train)

            # 학습용 데이터에 대해 평갓값이 없는 사용자와 아이템의 조합에 대한
            # 평갓값을 예측한다.
            data_test = data_train.build_anti_testset(None)
            predictions = knn.test(data_test)

            def get_top_n(predictions, n=10):
                # 각 사용자별로 예측된 아이템을 저장한다.
                top_n = defualtdict(list)
                for uid, iid, true_r, est, _ in predictions:
                    top_n[uid].append((iid, est))
                
                # 사용자별로 아이템을 예측 평갓값 순으로 나열하고 상위 n개를 저장한다.
                for uid, user_ratings in top_n.items():
                    user_ratings.sort(key=lambda x: x[1], reverse=True)
                    top_n[uid] = [d[0] for d in user_ratings[:n]]
                  
                return top_n

            pred_user2items = get_top_n(predictions, n=10)

            average_score = dataset.train.rating.mean()
            pred_results = []
            for _, row in dataset.test.iterrows():
                user_id = row["user_id"]
                movie_id = row["movie_id"]
                # 학습 데이터에 존재하지 않고 테스트 데이터에만 존재하는 사용자나
                # 영화에 관한 예측 평갓값은 전체 평균 평갓값으로 한다.
                if user_id not in user_id2index or \
                  movie_id not in movie_id2index:
                     pred_socre = knn.predict(uid=user_id, iid=movie_id).est
                     pred_results.append(pred_score)
                movie_rating_predict["rating_pred"] = pred_results

                return RecommendResult(movie_rating_predict.rating_pred,
                pred_user2items)
```

<br><br>

<h1>회귀 모델</h1>
<ul>
  <li>
    예측 대상의 평갓값을 랜덤 포레스트(random forest) 모델을 통한 회귀 분석을 수행한다.
  </li>
    <ul>
      <li>
        회귀 분석이기에 다른 방법의 알고리즘을 사용할 수도 있다.
      </li>
    </ul>
</ul>

```python
# 1. 회귀 모델 적용

# 모델 import
import sklearn.ensemble import RandomForestRegressor as RFR

# 학습에 사용하는 학습용 데이터 중 평갓값
train_y = dataset.train.rating.values

# 평갓값을 예측할 테스트용 데이터 중 사용자와 영화의 조합
test_keys = dataset.test[["user_id", "movie_id"]]

# 순위 형식의 추천 리스트 작성을 위한 학습용 데이터에 존재하는 모든 사용자와
# 모든 영화의 조합
train_all_keys = \
user_movie_matrix.stack(dropna=False).reset_index()[["user_id", "movie_id"]]

# 특징량(feature) 작성.
#   - 사용자별 평갓값의 최솟값, 최댓값, 평균값 및 영화별 최솟값, 최댓값을 특징량 추가.
#   - 학습 데이터에는 테스트 데이터에만 있는 특징량이 없기에 평균 평갓값으로 대체
train_x = train_keys.copy()
test_x = test_keys.copy()
train_all_x = train_all_keys.copy()

# 학습용 데이터에 존재하는 사용자별 평갓값의 최솟값, 최댓값, 평균값 및
# 영화별 평갓값의 최솟값, 최댓값, 평균값을 특징으로 추가한다.
aggregators = ["min", "max", "mean"]
user_features = dataset.train.groupby("movie_id").rating.agg(
                aggregators).to_dict()
movie_features = dataset.train.groupby("movie_id").rating.agg(
                 aggregators).to_dict()
for agg in aggregators:
    train_x[f"u_{agg}"] = train_x["user_id"].map(user_features[agg])
    test_x[f"u_{agg}"] = test_x["user_id"].map(user_features[agg])
    train_all_x[f"u_{agg}"] = train_all_x["user_id"].map(user_features[agg])
    train_x[f"m_{agg}"] = train_x["movie_id"].map(movie_features[agg])
    test_x[f"m_{agg}"] = test_x["movie_id"].map(movie_features[agg])
    train_all_x[f"m_{agg}"] = train_all_x["movie_id"].map(movie_features[agg])

    # 테스트용 데이터네만 존재하는 사용자와 영화의 특징량을 
    # 학습용 데이터 전체의 평균 평갓값으로 채운다.
    average_rating = train_y.mean()
    test_x.fillna(average_rating, inplace=True)

# 영화가 특정 장르에 속하는지 boolean 특징량 추가.
movie_genres = dataset.item_content[["movie_id", "genre"]]
genres = set(itertools.chain(*movie_genre.genre))
for genre in genres:
    movie_genres[f"is_{genre}"] = movie_genres.genre.apply(lambda x:
                                  genre in x)
movie_genres.drop("genre", axis=1, inplace=True)
train_x = train_x.merge(movie_genre, on="movie_id")
test_x = test_x.merge(movie_genres, on="movie_id")
train_all_x = train_all_x.merge(movie_genres, on="movie_id")

# 특징량을 사훃아지 않는 정보 삭제
train_x = train_x.drop(columns=["user_id", "movie_id"])
test_x = test_x.drop(columns=["user_id", "movie_id"])
train_all_x = train_all_x.drop(columns["user_id", "movie_id"])

# Random Forest를 사용한 학습
reg = RFR(n_jobs=-1, random_state=0)
reg.fit(train_x.values, train_y)

# 테스트용 데이터 안의 사용자의 영화의 조합에 대해 평갓값을 예측한다.
test_pred = reg.predict(test_x.values)

movie_rating_predict = test_keys.copy()
movie_rating_predict["rating_pred"] = test_pred

# 학습용 데이터에 존재하는 모든 사용자와 모든 영화의 조합에 대해
# 평갓값을 예측한다.
train_all_pred = reg.predict(train_all_x.value)

pred_train_all = train_all_keys.copy()
pred_train_all["rating_pred"] = train_all_pred
pred_matrix = pred_train_all.pivot(index="user_Id", columns="movie_id",
              values="rating_pred")

# 사용자가 학습용 데이터 안에서 평가하지 않은 영화 중
# 예측 평갓값이 높은 순으로 10편의 영화를 순위 형식의 추천 리스트로 만든다.
pred_user2items = defaultdict(list)
user_evlauated_movies = dataset.train.groupby("user_id").agg({
                        "movie_id": list})["movie_id"].to_dict()
for user_id in dataset.train.user_id.unique():
    movie_indexes = np.argsort(-pred_matrix.loc[user_id, :]).values
    for movie_index in movie_indexes:
        movie_id = user_movie_matrix.columns[movie_index]
        if moive_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].ap pend(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break

# - 무작위 추천보다 RMSE 결과는 좋다.
# - Precision@K와 Recall@K는 비슷하다.
#   → 평가 수는 적지만 평가가 높은 아이템에 의해 추천 정확도가 떨어짐
#   → 평가 수가 일정한 임곗값을 설정하여 편향을 보정할 수 있다.
# - 실제 추천에서는 다양한 특징량(feature)을 고려한다.
```

<br><br>

<h1>8. 행렬 분해</h1>
<ul>
  <li>
    <strong>모델 기반형 협조 필터링</strong> 방법인 <strong>행렬 분해</strong>에 대해 다룬다.
  </li>
    <ul>
      <li>
        메모리 기반 협조 필터링보다 복잡하지만 추천 성능은 일반적으로 좋다 알려져 있다.
      </li>
    </ul>
  <li>
    추천 시스템에서 행렬 분해는 넓은 의미에서 평갓값 행렬을 저차원의 <strong>사용자 인자 행렬</strong>과 <strong>아이템 인자 행렬</strong>로 분해하는 것을 의미한다.
  </li>
    <ul>
      <li>
        사용자와 아이템을 100 차원 정도의 <strong>저차원 벡터로 표현</strong>하고 <strong>벡터의 내적값을 사용자와 아이템의 상성</strong>으로 한다.
      </li>
    </ul>
  <li>
    SVD, NMF, MF, IMF, BPR 그리고 FM 행렬 분해 방법을 순서대로 설명한다.
  </li>
  <li>
    행렬 분해를 실무에서 사용할 때에는 <strong>'결손값 취급'</strong>과 <strong>'평갓값이 명시적인가 암묵적인가'</strong>라는 관점이 매우 중요하다.
  </li>
  <li>
    <strong>차원을 축소</strong>한 뒤 각 영화와의 상성은 <strong>벡터의 내적</strong>으로 계산한다.
  </li>
</ul>

```python
# 1. 평갓값이 명시적인 경우의 행렬 분해
user_movie_matrix = movielens.train.pivot(index="user_id",
                    columns="movie_id", values="ratings")
user_movie_matrix

# 모든 사용자가 모든 아이템을 평가하지 않기에
# 기본적으로 사용자 X 모든 아이템 행렬은 희소 행렬이다.
user_num = len(user_movie_matrix.index)
item_num = len(user_moive_matrix.columns)
non_null_num = user_num * itemnum_num - user_movie_matrix.isnull().sum().sum()
non_null_ratio = non_null_num / (user_num * item_num)

print(f"사용자 수={user_num}, 아이템 수={user_num},
      밀도={non_null_ratio:.2f}")
  
# 사용자 수: 1000 / 아이템 수: 6674 / 밀도: 0.03
```

<br>

<h2>8-2. 특잇값 분해</h2>

$$ R=PSQ^T=\hat P \hat Q^T $$

<p>
행렬 분해는 “거대한 <strong>평점 행렬 𝑅</strong>을 저차원 공간의 <strong>사용자 행렬 𝑃</strong>와 <strong>아이템 행렬 Q</strong>로 나눠, <strong>내적을 통해 평점을 근사</strong>하는 과정”이다.

각 사용자 행렬 𝑃, S 그리고 아이템 행렬 Q는 다음 목적 함수를 최소화 해서 얻을 수 있다.

하단의 목적 함수는 실제 평점 행렬 𝑅 을 <strong>두 잠재 행렬의 곱으로 근사</strong>하면서 오차(Frobenius norm)를 최소화하는 최적화 문제다.
</p>

$$ ||R-PSQ^T||^2_{Fro}=||R- \hat P \hat Q^T||^2_{Fro}$$

<p>
위 식을 Frobenius norm(오차)으로 정의하여 최적화 할 수 있다.
</p>

$$ ||A||^2_{Fro}=\sum_{i, j}A^2_{i,j} $$

```python
# 1. 견손값에는 0 또는 평균값을 대입하고, 
# 특잇값 분해(Singular Value Decomposition, SVD)로 차원 축소
user_movie_matrix.fillna(0)

# scipy 라이브러리를 사용한 특잇값 분해
import scipy
import numpy as np

# 평갓값을 사용자 X 영화 행렬로 변환한다. 결손값은 평균으로 채운다.
user_movie_matrix = movielens.train.pivot(index="user_id",
                    columns="movie_id", values="rating")
user_id2index = dict(zip(user_movie_matrix.index,
                range(len(user_movie_matrix.index))))
movie_id2index = dict(zip(user_movie_matrix.columns,
                 range(len(user_movie_matrix.columns))))
matrix = user_movie_matrix.fillna(
         movielens.train.rating.mean()).to_numpy()

# 인자 수 k로 특잇값 분해
P, S, Qt = scipy.sparse.linalg.svds(matrix, k=5)

# 예측 평갓값 행렬
pred_matrix = np.dot(np.dot(P, np.diag(S)), Qt)

print(f"P: {P.shape}, S: {S.shape}, Qt: {Qt.shape}, pred_matrix:
      {pred_matrix.shape}")

# P: (1000, 5) / S: (5,)  Qt: (5, 6673), pred_matrix: (1000, 6673)

# 위의 결과를 활용해 추천 수행.
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import scipy
import numpy as np
np.random.seed(0)

class SVDRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 결손값을 채우는 방법
        fillna_with_zero = kwargs.get("fillna_with_zero", True)
            factors = kwargs.get("factors", 5)
        # 평갓값을 사용자 X 아이템 행렬로 변환, 평갓값 또는 0으로 채운다.
        user_movie_matrix = dataset.train.pivot(index=="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))
        if fillna_with_zero:
            matrix = user_movie_matrix.fillna(0).to_numpy()
        else:
            matrix = user_movie_matrix. \
            fillna(dataset.train.rating.mean()).to_numpy()
        
        # 인자 수 k로 특잇값 분해를 수행
        P, S, Qt = scipy.sparse.linalg.svds(matrix, k=factors)
        
        # 예측 평갓값 행렬
        pred_matrix = np.dot(np.dot(P, np.diag(S)), Qt)
        
        # 학습용에 나오지 않는 사용자나 영화의 예측 평갓값은 평균값으로 한다.
        average_score = dataset.train.rating.mean()
        movie_rating_predict = dataset.test.copy()
        pred_result = []
        for i, row in dataset.test.iterrows():
            user_id = row["user_id"]
            if user_id not in user_id2index or \
                row["movie_id"] not in movie_id2index:
                pred_results.append(average_score)
                continue
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            pred_score = pred_matrix[user_index, movie_index]
            pred_result.append(pred_score)
        movie_rating_predict["rating_pred"] = pred_results

        # 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서
        # 예측값이 높은 순으로 한다.
        pred_user2items = defaultdict(list)
        user_evaluated_movies = dataset.train.groupby("user_id").agg({
                                    "movie_id": list})["movie_id"].to_dict()
        for user_id in dataset.train.user_id.unique():
            if user_id not in user_id2index:
                continue
            user_index = user_id2index[row["user_id"]]
            movie_indexes = np.argsort(-pred_matrix[user_index, :])
            for movie_index in movie_indexes:
                movie_id = user_movie_matrix.columns[movie_index]
                if movie_id not in user_evaluated_movies[user_id]:
                    pred_user2items[user_id].append(movie_id)
                if len(pred_user2items[user_id]) == 10:
                    break
        return RecommenResult(movie_rating_predict.rating_pred,
        pred_user2items)

if __name__ == "__main__":
    SVDRecommender().run_sample()

# RMSE=3.335, Precision@K=0.009, Recall@K=0.027

# - 0으로 결손값을 채우면 모두 가장 낮은 평가를 한 것이기 예측 성능이 나빠진다.
# - 대신 평균값으로 결측치를 채우는 방법을 사용할 수도 있다.
# - 잠재 인자 수를 튜닝할 수도 있다.
#   - 잠재 인자 수가 클수록 원래 행렬을 복구할 때 유리해 성능은 좋으나 overfitting 문제가 
#     생길 수 있다.
# - 일반적으로 행렬 전체를 사용할 경우는 scipy.sparse를 통해 희소 행렬 형식으로 0외의 값을 
#   갖는 셀 정보만 유지하는 경우가 많다.
```

<br>

<h2>8-3. 비음수 행렬 분해</h2>
<ul>
  <li>
    SVD는 행렬을 직교 벡터의 선형 결합으로 분해하기에 벡터 방향성(부호)을 고려하며, 그 결과 <strong>음수 값</strong>이 나올 수 있다.
  </li>
  <li>
    반면, <strong>NMF(Non-negative Matrix Factorization)</strong>는 모든 구성 요소를 <strong>비음수로 제한</strong>하여 분해하기에 각 행렬의 성분이 <strong>해석 가능한 비음수값(가중치)</strong>로 표현된다.
  </li>
    <ul>
      <li>
        최적화 과정에서 해를 찾을 때 <strong>음수해가 채택되는 것을 방지</strong>한다.
      </li>
    </ul>
  <li>
    단, 마찬가지로 <strong>결손값을 0</strong>으로 채우기에 추천 성능이 좋지는 못하다.
  </li>
  <li>
    최적화 식은 SVD와 동일하며 <strong>음수 제약</strong>만 걸린다는 차이가 있다.
  </li>
</ul>

$$
\min_{W, H} \; ||R - W H||_F^2 
\quad \text{subject to} \quad W \ge 0, \; H \ge 0
$$

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
from sklearn.decomposition import NMF
np.random.seed(0)

class NMFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 결손값을 채우는 방법
        fillna_with_zero = kwargs.get("fillna_with_zero", True)
        factors = kwargs.get("factors", 5)
        # 평갓값을 사용자 X 영화의 행렬로 변환한다. 결손값은 평균값 또는 0으로 차운다.
        user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))
        if fillna_with_zero:
            matrix = user_movie_matrix.fillna(0).to_numpy()
        else:
            matrix = user_movie_matrix. \
                fillna(dataset.train.rating.mean().to_numpy())
        nmf = NMF(n_components=factors)
        nmf.fit(matrix)
        P = nmf.fit_transform(matrix)
        Q = nmf.components_
        # 예측 평갓값 행렬
        average_scroe = dataset.train.rating.mean()
        movie_rating_predict = dataset.test.copy()
        pred_result = []
        for i, row in dataset.test.iterrows():
            user_id = row["user_id"]
            if user_id not in user_id2index or \
            row["movie_id"] not in movie_iddex:
                pred_results.append(average_score)
                continue
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            pred_score = pred_matrix[user_index, movie_index]
            pred_results.append(pred_score)
        movie_rating_predict["rating_pred"] = pred_results
        # 각 사용자에 대한 추천 영화는 그 사용자가 아직 평가하지 않은 영화 중에서 
        # 예측값이 높은 순으로 한다.
        pred_user2items = defaultdict(list)
        user_evaluated_movies = dataset.train.groupby("user_id").agg([
                                "movie_id": list])["movie_id"].to_dict()
        for user_id in dataset.train.user_in.unique():
            if user_id not in user_id2index:
                continue
            user_index = user_id2index[row["user_id"]]
            movie_indexes = np.argsort(-pred_matrix[user_index, :])
            for movie_index in movie_indexes:
                movie_id = user_movie_matrix.columns[movie_index]
                if movie_id not in user_evaluated_movies[user_id]:
                    pred_user2items[user_id].append(movie_id)
                if len(pred_user2item[user_id]) == 10:
                    break
        return RecommendResult(movie_rating_predict.rating_pred, pred_user2items)

if __name__ == "__main__":
    NMFRecommender().run_sample()
```

<br>

<h2>8-4. 명시적인 평갓값에 대한 행렬 분해</h2>
<p>
추천 시스템에서 <strong>행렬 분해(Matrix Factorization, MF)</strong>는 SVD와 달리 <strong>결손값을 메꾸지 않고 관측된 평갓값만</strong>을 사용해 행렬 분해하는 방법을 나타내는 경우가 많다.

MF는 대규모 데이터에서도 <strong>고속으로 계산</strong>이 가능하도록 개선된 방법이 많아 <strong>Spark</strong>나 <strong>BigQuery</strong> 등에도 구현되고 있다.

MF의 최적화 문제는 다음과 같다.

$$ min_{p, q} \sum_{u, i \in R^+} (r_{ui}-p^T_uq_i)^2+\lambda (||p_u||^2+||q_i||^2)$$

실제 평점 $ r_{ui} $와 예측 평점 $ \hat r_{ui}=p^T_uq_i $의 <strong>차이(오차)
를 최소화</strong>하되, 너무 큰 가중치(복잡한 벡터)를 방지하기 위해 $ p_u,\text{ }q_i $의 크기에 <strong>패널티(정규화)</strong>를 주는 방식이다.

단, 이는 이차식이 아닌 쌍선형(bilinear form) 함수이기 때문에 일반적으로 해석적으로 풀기는 어렵다. 이에 대한 해법으로는 <strong>Alternating Least Square(ALS)</strong>와 <strong>Stochastic Gradient Descent (SGD)</strong>를 사용하는 방법이 제안되었다.

<strong>ALS</strong>는 목적 함수를 최소화 하도록 사용자 인자 행렬과 아이템 인자 행렬을 <strong>교대로 최적화</strong> 한다. <strong>SGD</strong>는 입력 데이터를 <strong>샘플링</strong>해서 샘플 데이터에 대한 사용자 인자 행렬과 아이템 인자 행렬의 <strong>기울기를 계산</strong>하고, <strong>$ p $와 $ q $를 기울기 방향으로 업데이트</strong> 해나간다. 

<strong>SVD</strong>나 <strong>NMF</strong>는 <strong>꽉 찬(dense)</strong> 형태의 행렬을 다루기에 <strong>결측값을 0이나 평균으로 채우지만</strong>, <strong>MF</strong>는 결측값은 제외하고 <strong>실제 관측된 평점 $ R^+ $</strong>만을 이용해 최적화를 수행하기 때문에 불필요한 노이즈를 줄이고 현실적인 사용자-아이템 관계를 학습할 수 있다.

특정 사용자는 높게 평가한다는 편향, 특정 아이템이 높게 평가되기 쉽다는 편향을 고려한 모델 또한 존재하며 $ b_u $를 사용자 편향으로, $ b_i $를 아이템 편향을 포함한다. $ \mu $는 모든 평갓값의 편향이다.

$$ min_{p,q} \sum_{u,i \in R^+}(r_{ui}-(p^T_uq_i)+b_{ui})^2+\lambda(||p_u||^2+||q_i||^2+b^2_u+b^2_i) $$
</p>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import deafultdict
import numpy as np
from surprise import SVD, Reader
import pandas as pd
from surprise import Dataset as SurpriseDataset
np.random.seed(0)

class MFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 5)
        
        # 평갓값의 임곗값
        minimum_num_rating = kwargs.get("minimum_num_rating", 100)

        # 바이어스 항 사용
        use_biase = kwargs.get("user_biase", False)

        # 학습률
        lr_all = kwargs.get("lr_all", 0.005)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 50)

        # 평갓값이 minimum_num_rating건 이상 있는 영화로 필터링한다.
        filtered_movielens_train = dataset.train.groupby("movie_id").filter(
          lambda x: len(x["movie_id"]) >= minimum_num_rating
        )
        
        # Surprise용으로 데이터 가공
        reader = Reader(rating_scale=(0.5, 5))
        data_train = SurpriseDataset.load_from_df(
            filtered_movielens_train[["user_id", "movie_id", "rating"]],\
            reader).build_full_trainset()
        
        # Surprise 해열 분해 학습
        # SVD라는 이름을 사용하지만 특이점 분해가 아니라 Matrix Factorization이 실행됨.
        matrix_factorization = SVD(n_factors=factors, n_epochs=n_epochs,
                               lr_all=lr_all, biased=use_biase)
        matrix_factorization.fit(data_train)
        def get_top_n(predictions, n=10):
            
            # 각 사용자별로 예측된 아이템을 저장한다.
            top_n = defaultdict(list)
            for uid, iid, true_r, est, _ in predictions:
                top_n[uid].append((iid, est))
            
            # 사용자별로 아이템을 예측 평갓값순으로 나열하고 상위 n개를 저장한다.
            for uid, user_ratings in top_n.items():
                user_ratings.sort(key=lambda x: x[1], reverse=True)
                top_n[uid] = [d[0] for d in user_ratings[:n]]
            return top_n
        
        # 학습 데이터에 나오지 않은 사용자의 아이템의 조합을 준비한다.
        data_test = data_train.build_anti_testset(None)
        predictions = matrix_factorization.test(data_test)
        pred_user2items = get_top_n(predictions, n=10)
        test_data = pd.DataFrame.from_dict(
            [{"user_id": p.uid, "movie_id": p.iid, "rating_pred": p.est} \
             for p in predictions]
        )
        movie_rating_predict = dataset.test.merge(test_data,
                               on=["user_id", "movie_id"], how="left")
        
        # 예측할 수 없는 위치에는 평균값을 저장한다.
        movie_rating_predict.rating_pred.fillna(
        filtered_movielens_train.rating.mean(), inplace=True)
        return RecommendResult(movie_rating_predict.rating_pred, pred_user2items)

if __name__ == "__main__":
    MFRecommender().run_sample()

# - MF는 관측된 데이터를 기반으로 하기에 임곗값을 조정할 필요가 있다.
#   - 임곗값이 낮다면 과적합으로 RMSE는 높지만 Precision@K와 Recall@K가 낮아진다.
#   - 주어진 파리미터의 최적화는 grid search나 beize optimization 등을 사용한다.
```

<br>

<h2>8-5. 암묵적인 평갓값에 대한 행렬 분해</h2>
<p>
암묵적인 평갓값이란 사용자의 행동 이력을 사용하는 방법이며 <strong>비교적 얻기 쉽고 편향이 적다</strong>는 특징이 있다.

단, 명시적인 평갓값과는 다른 특징을 갖기 때문에 명시적 평가용 공식은 암묵적 평가에 적용할 수 없다. 대신 암묵적 평갓값에는 <strong>암묵적인 평갓값에 대한 행렬분해(Implicit Matrix Factorization, IMF)</strong>를 사용한다.

IMF에서는 $ \mathbf{r_{ui}} $를 사용하며 $ \textbf{사용자 } \mathbf{u} $가 $ \textbf{아이템 } \mathbf{i} $에 대해 몇 번 행동했는가를 나타낸다. 즉, $ \textbf{사용자 } \bf{u} \textbf{가 상품 } \bf{i} \textbf{를 열람한 횟수}  $이다.

그리고 $ \hat r_{ui} $를 사용하여 한 번이라도 아이템을 열람했는가의 여부에 따라 다름과 같이 정의한다. 한 번이라도 열람한 경우 사용자가 관심이 있는 것을 의미한다.

$$ \hat r_{ui} = \begin{cases} 1(r_{ui}>0) \\ 0(r_{ui}=0) \end{cases} $$

관심에 대한 기준을 정의한 뒤에는 신뢰도를 정의한다. 신뢰도란 해당 호의에 대해 얼마나 신뢰해야 좋은가를 의미하며 다음과 같다.

$$ c_{ui}=1+\alpha r_{ui} $$

$ c_{ui} $는 신뢰도를 의미하며 열람 횟수인 $ r_{ui} $에 상수 $ \alpha $를 곱해 결정한다. 한 번도 열람되지 않는 경우에는 1이 된다. 요약하면 IMF는 $ r_{ui} $를 선호도 $ \hat r_{ui} $와 해당 선호에 대한 신뢰도 $ c_{ui} $로 분해하는 것이 기초적인 사고 방식이다.

IMF의 최적화 식은 다음과 같다.

$$ min_{p,q}\sum_u^N \sum_i^Mc_{ui}(\hat r+{ui}-p^T_{u}q_{i})+\lambda(\sum_u ||p_u||^2+\sum_i ||q_i||^2) $$

MF와 IMF의 차이는 두 가지 이다. 첫 번째로 MF는 관측된 평갓값으로 총합을 구하지만 IMF는 <strong>사용자와 아이템의 모든 조합</strong>으로 총합을 구한다. 이때 암묵적인 평가는 부정적인 예시로 관측되지 않음을 학습한다. 두 번째는 <strong>신뢰도</strong> $ \mathbf{c_{ui}} $가 가중치로 들어 있어 암묵적인 평갓값의 $ \textbf{신뢰를 모델링} $ 했다는 점이다.

여러 차례 열람된 경우 $ c_{ui} $가 커지고 $ r_{ui}=1$이기에 $ p_uq^T_i $가 1에 보다 가까워지게 학습한다.

또한 Spark나 Google ML 등의 라이브러리를 사용할 경우 명시적, 암시적 행렬 분해가 구분되어 있지 않기에 주의해야 한다.

IMF에서는 신뢰도 $ c_{ui} $의 튜닝도 중요하다. $ \alpha $를 바꾸거나 대수를 얻어 $ c_{ui} =log(1+\alpha_ir_{ui}) $로 하거나, 가중치 $ \alpha $를 아이템별로 변화시켜 $ c_{ui}=log(1+\alpha_ir_{ui}) $ 등으로 할 수도 있다.
</p>

```python
# implicit 라이브러리를 사용해 계산
from util.models import RecommendeResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
import implicit
from scipy.sparse import lil_matrix
np.random.seed(0)

class IMFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 10)

        # 평갓값의 임곗값
        minimum_num_rating = kwargs.get("minimum_num_rating", 0)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 50)

        # alpha
        alpha = kwargs.get("alpha", 1.0)
        filtered_movielens_train = dataset.train.groupby("movie_id").filter(
          lambda x: len(x["movie_id"]) >= minimum_num_rating
        )

        # 행렬 분석용으로 행렬을 작성한다.
        movielens_train_high_rating =
        filtered_movielens_train[dataset.train.rating >= 4]
        unique_user_ids = sorted(movielens_train_high_rating.user_id.unique())
        unique_movie_ids = sorted(movielens_train_high_rating. \
            movie_id.unique())
        user_id2index = dict(zip(unique_user_ids, range(len(unique_user_ids))))
        movie_id2index = dict(zip(unique_movie_ids, \
            range(len(unique_movie_ids))))
        movielens_matrix = lil_matrix((len(unique_movie_ids), \
            len(unique_user_ids)))

        for i, row in movielens_train_high_rating.iterrows():
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            movielens_matrix[movie_index, user_index] = 0.1 * alpha
        
        # 모델 초기화
        model = implicit.als.AlternatingLeastSquares(
            factors=factors, iterations=n_epochs,
            calculate_training_loss=True, random_state=1
        )

        # 학습
        model.fit(movielens_matrix)

        # 추천
        recommendations = model.recommend_all(movielens_matrix.T)
        pred_user2items = defaultdict(list)
        for user_id, user_index in user_id2index.items():
            movie_indexs = recommendations[user_index, :]
            for movie_index in movie_indexes:
                movie_id = unique_movie_ids[movie_index]
                pred_user2items[user_id].append(movie_id)
        
        # IMF에서는 평갓값 예측이 어려우므로 RMSE 평가는 수행하지 않는다.
        # (편의상 테스트 데이터를  그대로 반영한다.)
        return RecommendResult(dataset.test.rating, pred_user2items)

if __name__ == "__main__":
    IMFRecommender().run_sample()
```

<br>

<h2>8-6. BPR</h2>
<p>

암묵적 평갓값을 사용한 다른 방법으로는 Bayesian Personalized Ranking(BPR)이 있다. 사용자($ u $)에는 $ p_u $라는 $ k $ 차원 벡터가 할당되고, 아이템($ i $)에는 $ q_i $라는 $ k $ 차원의 벡터가 할당된다. BPR에서는 해당 벡터들(사용자 $ u $, 암묵적으로 평가된 아이템 $ i $, 관측되지 않은 아이템 $ j $)라는 세 가지 데이터를 기반으로 학습한다. 목적 함수는 다음과 같으며 $ p_u $와 $ q_i $를 학습한다.

$$ \sum_{(u,i,j)\in R} \log \sigma(p^T_uq_i-p^T_uq_j)-\lambda(\sum_u ||p_u||^2+\sum_i ||q_i||^2) $$

$ \sigma $는 로지스틱 함수이며 다음과 같다.

$$ \sigma(x)=\frac{1}{1+\exp (-x)} $$

목적함수는 암묵적으로 평가한 아이템 $ i $의 경우 관측되지 않은 아이템 $ j $에 비해 사용자가 선호한다는 가정으로 계산한다. 벡터로 설명하면 $ u $와 $ i $는 가까워지도록, $ u $와 $ j $는 멀어지도록 학습한다. (실무에서는 관측되는 아이템이 많기에 아이템 $ j $를 샘플링하는 방법을 고려한다).
</p>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
import implicit
from scipy.sparse import lil_matirx
np.random.seed(0)

class BPRRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 10)

        # 평갓값의 평균값
        minimum_num_rating = kwargs.get("minimum_num_rating", 0)

        # 에폭 수
        n_epochs = kwargs.get("n_epoch", 50)

        # 행렬 분해용 행렬을 작성한다.
        filtered_movielens_train = dataset.train.groupby("movie_id").filter(
            lambda x: len(["movie_id"]) >= minimum_num_rating
        )
        movielens_train_high_rating = filtered_movielens_train[dataset.train
                                      rating >= 4]
        unique_user_ids = sorted(movielens_train_high_rating.user_id.unique())
        unique_movie_ids = sorted(movielens_train_high_rating. \
                                  movie_id.unique())
        user_id2index = dict(zip(unique_user_ids, range(len(unique_user_ids))))
        movie_id2index = dict(zip(unique_movie_ids, \
                              range(len(unique_movie_ids))))
        movielens_matrix = lil_matrix((len(unique_movie_ids), \
                                      len(unique_user_ids)))
        for i, row in movielens_train_high_rating.iterrows():
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            movielens_matrix[movie_index, user_index] = 1.0
        
        # 모델 초기화
        model = implicit.bpr.BayesianPersonalizedRanking(
                factors=factors, iterations=n_epochs
        )

        # 학습
        model.fit(movielens_matrix)

        # 추천
        recommendations = model.recommend_all(movielens_matrix.T)
        pred_user2items = defaultdict(list)
        for user_id, user_index in user_id2index.items():
            movie_indexes = recommendations[user_index, :]
            for movie_index in movie_indexes:
                movie_id = unique_movie_ids[movie_index]
                pred_user2items[user_id].append(movie_id)
        
        # BPR에서는 평갓값을 예측하기 어려우므로 RMSE 평가는 수행하지 않는다.
        # (편의상 테스트 데이터의 예측값을 그대로 반환한다.)
        return RecommendResult(dataset.test.rating, pred_user2items)

if __name__ = "__main__":
    BPRRecommender().run_sample()
```

<br>

<h2>8-7. FM</h2>
<p>
  <strong>아이템 속성</strong>을 사용하는 방법은 콜드 스타트 문제를 겪지 않기에 신규 아이템이나 사용자에 사용하기 좋다. 이번 장에서는 Factorization Machines(FM)에 대해 소개 한다.

  FM은 한 개의 평가에 대한 정보를 한 개의 행으로 표시하며 결과적으로 매트릭스는 $ \textbf{평갓값 } \times \textbf{ 특징량} $이 된다. 특징량은 사용자 ID와 아이템 ID를 One-Hot encoding한 것과 사용자와 아이템의 속성 정보 등의 보조 정보를 연결한 것이 된다. 이후 데이터를 활용해 평갓값 $ y $를 예측한다.

  예를 들어 사용자 $ u_1 $이 아이템 $ i_1 $을 평가했다면 $ u_1 $과 $ i_1 $로된 영역이 1이되고 그 뒤에 보조 정보인 사용자 나이, 아이템 추가 후 경과일 수 등이 들어온다.

  FM의 또 다른 특징은 특징량들을 나열하여 데이터셋을 생성하기에 <strong>특징량들끼리의 조합</strong>도 고려할 수 있다는 것이다. FM의 식은 다음과 같다.

  $$ \hat y = w_0+\sum_{j=1}^nw_jx_j+\sum_{j=1}^n\sum_{k=j+1}^n(f_j,f_k)x_jx_K $$

  FM의 식에는 이차항($ x_jx_k $)이 포함된다. 일반적으로 이차항은 가중치 파라미터 수가 2제곱에 비례해 증가하기에 풀기 어렵지만 FM은 가중치를 $ <f_j,f_k> $로 표현하여 문제를 해결한다.

  즉, $ f $는 벡터를 의미하기에 특징량들의 내적을 기울기로 사용하며 내적 값이 크다면 해당 특징들의 상호작용이 중요하게 작용함을 의미한다. 또한 내적이기에 파라미터가 제곱에 비례하여 증가하지 않고 선형으로 늘어나기에 다양한 특징량을 실험을 하기에도 좋다는 장점이 있다.

  보조 정보의 유형이나 작업 방법에 따라 정확도가 달라지기에 보유 데이터에 맞추어 시행착오를 하는 것이 중요하다.

</p>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
import xlearn as xl
from sklearn.feature_extraction import DictVectorizer
np.random.seed(0)

class FMRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 10)

        # 평갓값의 임곗값
        minimum_num_rating = kwargs.get("minimum_num_rating", 200)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 50)

        # 학습률
        lr = kwargs.get("lr", 0.01)

        # 보조 정보 사용
        use_side_information = kwargs.get("use_side_information", False)

        # 평갓값이 minimum_num_rating건 이상인 영화로 필터링한다.
        filtered_movielens_train = dataset.train.groupby("movie_id").filter(
            lambda x: len(x["movie_id"]) >= minimum_num_rating
        )

        # 사용자가 평가한 영화
        user_evaluated_movies = (
            filtered_movielens_train = dataset.train.groupby("movie_id"). \
            agg({"movie_id": list})["movie_id"].to_dict()
        )
        train_data_for_fm = []
        y = []
        for i, row in filtered_movielens_train.iterrows():
            x = {"user_id": str(row["user_id"]), \
                "movie_id": str(row["movie_id"])}
            if use_side_information:
                x["tag"] = row["tag"]
                x["user_rating_avg"] = \
                    np.mean(user_evaluated_movies[row["user_id"]])
            train_data_for_fm.append(x)
            y.append(row["rating"])
        y = np.array(y)
        vectorizer = DictVectorizer()
        X = vectorizer.fit_transform(train_data_for_fm).toarray()
        fm_model = xl.FMModel(
                        task="reg", 
                        metric="rmse", 
                        lr=lr, 
                        opt="sgd",
                        k=factors,
                        epoch=n_epochs
                   )    
        
        # Start to train
        fm_model.fit(X, y, is_lock_free=False)
        unique_user_ids = sorted(filtered_movielens_train.user_id.unique())
        unique_movie_ids = sorted(filtered_movielens_train.movie_id.unique())
        user_id2index = dict(zip(unique_user_ids, range(len(unique_user_ids))))
        movie_id2index = dict(zip(unique_movie_ids, \
            range(len(unique_movie_ids))))
        test_data_row_fm = []
        for user_id in unique_user_ids:
            for movie_id in unique_movie_ids:
                x = {"user_id": str(user_id), "movie_id": str(movie_Id)}
                if use_side_information:
                    tag = dataset.item_content[dataset.item_content.movie_id 
                    == movie_id].tag.tolist()[0]
                    x["tag"] = tag
                    x["user_rating_avg"] = \
                        np.mean(user_evaluated_movies[row["user_id"]])
                test_data_for_fm.append(x)
        X_test = vectorizer.transform(test_data_for_fm).toarray()
        y_pred = fm_model.predict(X_test)
        pred_matrix = y_pred.reshape(len(unique_user_ids), 
            len(unique_movie_ids))
        # 학습용에 나오지 않는 사용자나 영화의 예측 평갓값은 평균 평갓값으로 한다.
        average_score = dataset.train.rating.mean()
        movie_rating_predict = dataset.test.copy()
        pred_results = []
        for i, row in dataset.test.iterrows():
            user_id  = row["user_id"]
            if user_id not in user_id2index or \
                row["movie_id"] not in movie_id2index:
                pred_results.append(average_score)
                continue
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            pred_score = pred_matrix[user_index, movie_index]
            pred_results.append(pred_score)
        movie_rating_predict["rating_pred"] = pred_results
        pred_user2items  defaultdict(list)
        for user_id in unique_user_ids:
            user_index = user_id2index[user_id]
            movie_indexes = np.argsort(-pred_matrix[user_index, :])
            for movie_index in movie_indexes:
                movie_id = unique_movie_ids[movie_index]
                if movie_id not in user_evaluated_movies[user_id]:
                    pred_user2items[user_id].append(movie_id)
                    if len(pred_user2items[user_id]) == 10:
                        break
        
        return RecommenResult(movie_rating_predict.rating_pred, pred_user2items)

if __name__ == "__main__":
    FMRecommender().run_sample()
```

<br><br>

<h1>9. 자연어 처리 방법에 대한 추천 시스템 응용</h1>
<ul>
  <li>
    <strong>상품 설명문</strong>이나 <strong>사용자 리뷰</strong>를 분석하여 콘텐츠 기반 추천으로 비슷한 상품을 찾을 수 있다.
  </li>
    <ul>
      <li>
        사용자 행동 이력 정보와 결합하면 <strong>협조 필터링 기반 추천</strong>도 가능하다.
      </li>
    </ul>
</ul>

<br>

<h2>9-1. 토픽 모델</h2>
<ul>
  <li>
    토픽 모델에서 하나의 문장은 여러 <strong>토픽</strong>으로 구성되며 <strong>각 토픽에서 단어가 선택되어 문장이 구성되는 것을 모델화</strong> 한다.
  </li>
  <li>
    토픽 모델 중 실무에서 가장 많이 사용되는 모델은 잠재 디리클레 할당(Latent Dirichlet Allocation)이다.
  </li>
    <ul>
      <li>
        토픽을 할당할 때 <strong>디리클레 분포</strong>를 사전 분포로 가정해 <strong>베이즈 추정을 하는 모델</strong>이다.
      </li>
      <li>
        LDA는 문장을 적용할 때 먼저 MeCab 등의 <strong>형태소 분석 라이브러리</strong>를 사용해 문장을 분할한다.
      </li>
      <li>
        이후 조사나 구두점을 제거하는 등 분석 목적에 따라 필요한 <strong>전처리를 수행</strong>한다.
      </li>
      <li>
        전처리가 완료된 데이터를 LDA에 입력하면 <strong>각 토픽별 단어 분포</strong>와 <strong>문장 토픽 분포</strong>가 계산된다.
      </li>
        <ul>
          <li>
            <strong>토픽별 단어 분포</strong>: 각 단어가 생성될 확률로 <strong>해당 토픽에서의 단어 분포</strong>를 의미한다.
          </li>
          <li>
            <strong>문장의 토픽 분포</strong>: 문장이 특정 토픽일 확률로 <strong>각 토픽의 비율</strong>을 의미한다.
          </li>
        </ul>
      <li>
        LDA에 데이터를 입력할 때에는 <strong>토픽 수</strong>라는 파라미터를 미리 결정해야하며 토픽 수만큼 각 <strong>토픽이 반환</strong>되며 나오기 쉬운 단어를 알 수 있다.
      </li>
      <li>
        LDA는 <strong>비지도 학습(Unsupervised Learning)</strong>으로 실제 계산되어 나온 토픽이 어디에 대응하는 토픽인지는 사람이 분간한다.
      </li>
    </ul>
</ul>

```python
import gensim
import logging 
from gensim.corpora.dictionary import Dictionary

movie_content = movielens.item_content

# tag가 부여되어 있지 않은 영화는 있지만 genre는 모든 영화에 부여되어 있다.
# tag와 genre를 결합한 것을 영화의 콘텐츠 정보로 하며 비슷한 영화를 찾아서 추천한다.
# tag가 없는 영화의 경우 NaN으로 되어 있기에 빈 리스트로 변환해서 처리한다.

movie_content['tag_genre'] = movie_content['tag'].fillna("").apply(list)
                             + movie_content['genre'].apply(list)
movie_content['tag_genre'] = movie_content['tag_genre'].\
                                 apply(lambda x:list(map(str, x)))

# tag와 genre 데이터를 사용해 LDA를 학습한다.
tag_genre_data = movie_content.tag_genre.tolist()

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',
level=logging.INFO)
common_dictionary = Dictionary(tag_genre_data)
common_corpus = [common_dictionary.doc2bow(text) for text in tag_genre_data]

lda_model = gensim.models.LdaModel(common_corpus, id2word=common_dictionary, 
num_topics=50, passes=30)
lda_topics = lda_model[common_corpus]
```

<br>

<h2>9-2. LDA를 사용한 콘텐츠 기반 추천</h2>
<ul>
  <li>
    가장 많이 평가한 아이템의 n 개의 토픽을 확인하고, 해당 토픽 중 아직 보지 않은 아이템 n 개를 추천한다.
  </li>
</ul>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
import gensim
import logging
from gensim.corpora.dictionary import Dictionary
from collections import Counter
np.random.seed(0)

class LDAContentRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 50)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 30)
        movie_content = dataset.item_content.copy()

        # tag는 부여되지 않은 영화도 있지만 genre는 모든 영화에 부여되어 있다.
        # tag와 genre가 결합한 것을 영화 콘텐츠 정보로하여 비슷한 영화를 찾아 추천한다.
        # tag가 없는 영화는 NaN으로 되어 있으므로 빈 리스트로 변환하여 처리한다.
        movie_content["tag_genre"] = movie_content["tag"].
        fillna("").apply(list) + movie_content["genre"].apply(list)
        movie_content["tag_genre"] = movie_content["tag_genre"].
        apply(lambda x: list(map(str, x)))

        # tag와 genre 데이터를 사용해 word2vec을 학습한다.
        tag_genre_data = movie_content.tag_genre.tolist()
        logging.basicConfig(format="%(asctime)s : %(levelname)s : %(message)s", level=logging.INFO)
        common_dictionary = Dictionary(tag_genre_data)
        common_corpus = [common_dictionary.doc2bow(text) for text
                        in tag_genre_data]
        lda_model = gensim.models.LdaModel(
            common_corpus, id2word=common_dictionary,
            num_topics=factors, passes=n_epochs
        )
        lda_topics = lda_model[common_corpus]
        movie_topics = []
        movie_topic_scores = []
        for movie_index, lda_topic in enumerate(lda_topics):
            sorted_topic = sorted(lda_topics[movie_index], key=lambda x: -x[1])
            movie_topic, topic_score = sorted_topic[0]
            movie_topics.append(movie_topic)
            movie_topic_scores.append(topic_score)
        movie_content["topic"] = movie_topics
        movie_content["topic_score"] = movie_topic_scores
        movielens_train_high_rating = dataset.train[dataset.train.rating >= 4]
        user_evaluated_movies = dataset.train.groupby("user_id").agg({
            "movie_id": list})["movie_id"].to_dict()
        movie_id2index = dict(zip(movie_content.movie_id.tolist(),
                         range(len(movie_content))))
        pred_user2items = defaultdict(list)
        for user_id, data in movielens_train_high_rating.groupby("user_id"):
            evaluated_movie_ids = user_evaluated_movies[user_id]
            movie_ids = data.sort_values("timestamp")["movie_id"]. \
            tolist()[-10:]
            movie_indexes = [movie_id2index[id] for id in movie_ids]
            topic_counter = Counter([movie_topics[i] for i in movie_indexes])
            frequent_topic = topic_counter.most_common(1)[0][0]
            topic_movies = (
                movie_content[movie_content.topic == frequent_topic]
                .sort_values("topic_score", ascending=False)
                .movie_id.tolist()
            )
            for movie_id in topic_movies:
                if movie_id not in evaluated_movie_ids:
                    pred_user2items[user_id].append(movie_id)
                if len(pred_user2items[user_id]) == 10:
                    break
            
            # LDA에서는 평갓값 예측이 어려우므로 RMSE의 평가는 수행하지 않는다.
            # (편의상 테스트 데이터의 예측값을 그대로 반환한다.)
            return RecommendResult(dataset.test.rating, pred_user2item)

if __name__ == "__main__":
    LDAContentRecommender().run_sample()
```

<br>

<h2>9-3. LDA를 사용한 협조 필터링 추천</h2>
<ul>
  <li>
    사용자의 구입 이력이나 열람 이력 데이터는 dict 형태로 key에 사용자 고유 번호를, value에 구매 혹은 열람 이력을 주어 저장할 수 있다.
  </li>
    <ul>
      <li>
        각 <strong>아이템</strong>을 <strong>단어</strong>로 보고, <strong>아이템의 집합</strong>을 <strong>문장</strong>으로 하여 LDA를 적용할 수 있다.
      </li>
    </ul>
  <li>
    LDA와 행동 이력을 사용하는 장점은 추천 시스템 외에도 EDA에 활용할 수 있다.
  </li>
</ul>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
import gensim
import logging
from gensim.corpora.dictionary import Dictionary
np.random.seed(0)

class LDACollaborateRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 50)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 30)
        logging.basicConfig(format="%(asctime)s : %(levelname)s : \
                            %(message)s", level=logging.INFO)
        lda_data = []
        movielens_train_high_rating = dataset.train[dataset.train.rating >= 4]
        for user_id, data in movielens_train_high_rating.groupby("user_id"):
            lda_data.append(data["movie_id"].apply(str).tolist())
        common_dictionary = Dictionary(lda_data)
        common_corpus = [common_dictionary.doc2bow(text) for text in lda_data]
        lda_model = gensim.models.LdaModel(
            common_corpus, id2word=common_dictionary,
            num_topics=factors, passes=n_epochs
        )
        lda_topics = lda.model[common_corpus]
        user_evaluated_movies = dataset.train.groupby("user_id").agg({
                                "movie_id": list})["movie_id"].to_dict()
        pred_user2items = defaultdict(list)
        for i, (user_id, data) in enumerate(
        movielens_train_high_rating.groupby("user_id")):
            evaluated_movie_ids = user_evaluated_movies[user_id]
            user_topic = sorted(lda_topics[i], key=lambda x: -x[1])[0][0]
        topic_movies = lda_model.get_topic_terms(user_topic, 
                       topn=len(dataset.item_content))
        for token_id, score in topic_movies:
            movie_id = int(common_dictionary.id2token[token_id])
            if movie_id not in evaluated_movie_ids:
                pred_user2items[user_id].append(movie_id)
            if len(pred_user2items[user_id]) == 10:
                break
        
        # LDA에서는 평갓값 평측이 어려우므로 RMSE 평가를 수행하지 않는다.
        # (편의상 테스트 데이터를 그대로 반환한다.)
        return RecommendResult(dataset.test.rating, pred_user2items)

if __name__ = "__main__":
    LDACollaborationRecommender().run_sample()
```

<br>

<h2>9-4. word2vec</h2>
<ul>
  <li>
    <strong>word2vec</strong>은 <strong>분포 가설</strong>을 사용한다. 분포 가설이란 단어의 의미는 해당 단어의 <strong>주변 단어</strong>에 따라 결정된다라는 가설을 따른다.
  </li>
  <li>
    word2vec을 학습시키면 출력으로 각 단어의 벡터를 얻을 수 있으며 이 벡터의 유사돌르 통해 추천을 수행할 수 있다.
  </li>
</ul>

<br>

<h2>9-5. word2vec을 사용한 콘텐츠 기반 추천</h2>
<ul>
  <li>
    word2vec의 발전된 형태로는 <strong>doc2vec</strong>이 있으며 <strong>문장에도 벡터를 부여</strong>한다는 특징이 있다.
  </li>
    <ul>
      <li>
        단, doc2vec은 <strong>속도가 느리다</strong>는 문제로인하여 실무에서는 보통 word2vec을 사용한다.
      </li>
    </ul>
</ul>

```python
import gensim
import logging

# tag는 부여되지 않은 영화도 있지만 genre는 모든 영화에 부여되어 있다.
# tag와 genre가 결합된 것을 콘텐츠 정보로 하여 비슷한 영화를 찾아 추천한다.
# tag가 없는 영화는 NaN으로 되어 있으므로 빈 리스트로 변환하여 처리한다.
movie_content['tag_genre'] = movie_content['tag'].fillna("").apply(list)
                             + movie_content['genre'].apply(list)
movie_content['tag_genre'] = movie_content['tag_genre'].apply(
                             lambda x:set(map(str, x)))

# tag의 genre 데이터를 사용해서 word2vec을 학습한다.
tag_genre_data = movie_content.tag_genre.tolist()
model = gensim.models.word2vec.Word2Vec(tag_genre_data, vector_size=100,
window = 100, sg=1, hs=0, epochs=50, min_count=5)
```

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
import numpy as np
import gensim

np.random.seed(0)

class Word2vecRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 100)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 30)

        # window 크기
        window = kwargs.get("window", 100)

        # 스킵 그램
        use_skip_gram = kwargs.get("user_skip_gram", 1)

        # 계층적 소프트맥스
        use_hierarchial_softmax = kwargs.get("use_hierarchial_softmax", 0)

        # 사용한 단어가 출현한 횟수의 임곗값
        min_count = kwargs.get("min_count", 5)
        movie_content = dataset.item_content.copy()

        # tag는 부여되지 않은 영화도 있지만 genre는 모든 영화에 부여되어 있다.
        # tag와 genre가 결합된 것을 영화 콘텐츠 정보로 하여 비슷한 영화를 찾아 추천한다.
        # tag가 없는 영화의 경우 NaN으로 되어 있으므로 빈 리스트로 변환한 뒤 처리한다.
        movie_content["tag_genre"] = movie_coutent["tag"].fillna("").
            apply(list) + movie_count["genre"].apply(list)
        movie_content["tag_genre"] = movie_content[
            "tag_genre"].apply(lambda x: set(map(str, x)))

        # 태그와 장르 데이터를 사용해 word2vec을 학습한다.
        tag_genre_data = movie_content.tag_genre.tolist()
        model = gensim.models.word2vec.Word2Vec(
            tag_genre_data
            vector_size=factors,
            window=window,
            sg=use_skip_gram,
            hs=use_hierarchial_softmax
            epochs=n_epochs,
            min_count=min_count
    )
    
    # 각 영화의 벡터를 계산한다.
    # 각 영화에 부여될 태그/장르의 벡텨 평균울 영화 벡터로 한다.
    movie_vectors = []
    tag_genre_in_model = set(model.wv.key_to_index.keys())
    titles = []
    ids = []
    for i, tag_genre in enumerate(tag_genre_data):
        # word2vec 모델에서 사용할 수 있는 태그 및 장르로 한정한다.
        input_tag_genre = set(tag_genre) & tag_genre_in_model
        if len(input_tag_genre) == 0:
            # word2vec을 기반으로 벡터 계산할 수 없는 영화에는 무작위 벡터를 부과한다.
            vector = np.random.randn(model.vector_size)
        else:
            vector = model.wv[input_tag_genre].mean(axis=0)
        titles.append(movie_content.iloc[i]["title"])
        ids.append(movie_content.iloc[i]["movie_id"])
        movie_vectors.append(vector)
    
    # 유사도 계산을 쉽게 할 수 있도록 numpy 배열로 저정해둔다.
    movie_vectors = np.array(movie_vectors)

    # 정규화 벡터
    sum_vec = np.sqrt(np.sum(movie_vectors ** 2, axis=1))
    movie_norm_vectors = movie_vectors / sum_vec.reshape((-1, 1))
    def find_similar_items(vec, evaluated_movie_ids, topn=10):
        score_vec = np.dot(movie_norm_vectors, vec)
        similar_indexes = np.argsort(-score_vec)
        similar_items = []
        for similar_index in similar_indexes:
            if ids[similar_index] not in evaluated_movie_ids:
                similar_items.append(ids[similar_index])
            if len(similar_items) == topn:
                break
        return simiar_items
    movielens_train_high_rating = dataset.train[dataset.train.rating >= 4]
    user_evaluated_movies = dataset.train.groupby("user_id").agg({
                            "movie_id": list})["movie_id"].to_dict()
    id2index = dict(zip(ids, range(len(ids))))
    pred_user2items = dict()

    for user_id, data in movielens_train_high_rating.groupby("user_id"):
        evaluated_movie_ids = user_evaluated_movies[user_id]
        movie_ids = data.sort_values("timestamp")["movie_id"].tolist()[-5:]
        movie_indexes = [id2index[id] for id in movie_ids]
        user_vector = movie_norm_vectors[movie_indexes].mean(axis=0)
        recommended_items = find_simiar_items(user_vector,
                            evaluated_movie_ids, topn=10)
        pred_user2items[user_id] = recommended_items
    
    # Word2vec에서는 평갓값 예측이 어려우므로 RMSE의 평가는 수행하지 않는다.
    # (편의상 테스트 데이터의 예측값을 그대로 반환한다.)
    return RecommendResult(dataset.test.rating, pred_user2items)

if __name__ == "__main__":
    Word2vecRecommender().run_sample()
```

<br>

<h2>9-6. word2vec을 사용한 협조 필터링 추천(item2vec)</h2>
<ul>
  <li>
    사용자가 열람하거나 구매하는 등의 행동 이력을 word2vec에 적용하는 방법이 있으며 이때에는 item2vec 또는 prod2vec이라 불린다. 구현이 간단하고 추천 성능이 높다.
  </li>
  <li>
    LDA와 마찬가지로 사용자의 <strong>행동 이력</strong>을 <strong>단어의 집합</strong>으로 간주하고 word2vec을 적용한다.
  </li>
    <ul>
      <li>
        단, 사용자가 행동한 <strong>순서대로</strong> 아이템을 나열하는 것과 <strong>window_size</strong> 파라미터의 <strong>액션 순서</strong>를 고려한다.
      </li>
    </ul>
  <li>
    전체를 사용한 추천은 시계열을 반영하지 못할 수 있기에 최근 정보만을 반영하게 하는 등의 설정이 필요하다.
  </li>
  <li>
    자연어 모델을 사용하는 것과 행동 이력을 사용하는 것은 다르다. 따라서 자연어 논문에서의 파라미터가 최적이 아닐 수 있으며 <strong>자사 데이터로 최적화</strong> 해야한다.
  </li>
</ul>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
import numpy as np
import gensim

np.random.seed(0)

class Item2vecRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 인자 수
        factors = kwargs.get("factors", 100)

        # 에폭 수
        n_epochs = kwargs.get("n_epochs", 30)

        # window 크기
        window = kwargs.get("window", 100)

        # 스킵 그랩
        use_skip_gram = kwargs.get("use_skip_gram", 1)

        # 계층적 소프트웨어
        use_hierarchial_softmax = kwargs.get("use_hierarchial_softmax", 0)

        # 사용할 단어가 출현한 횟수의 임곗값
        min_count = kwargs.get("min_count", 5)
        item2vec_data = []
        movielens_tarin_high_rating = dataset.train[dataset.train.rating >= 4]
        for user_id, data in movielens_train_high_rating.groupby("user_id"):
            # 평가된 순으로 나열한다.
            # item2vec에는 window라는 파라미터가 있으며
            # item의 평가 순서도 중요한 요소가 된다.
            item2vec_data. \
                append(data.sort_values("timestamp")["movie_id".tolist()])
        model = gensim.models.word2vec.Word2Vec(
            item2vec_data,
            vector_size=factors,
            window=window,
            sg=use_skip_gram,
            hs=use_hierarchial_softmax,
            epochs=n_epochs,
            min_count=min_count,
        )
        pred_user2items = dict()
        for user_id, datain movielens_train_high_rating.groupby("user_id"):
            input_data = []
            for item_id in data.sort_values("timestamp")["movie_id"].tolist():
                if item_id in model.wv.key_to_index:
                    input_data.append(item_id)
            
            if len(input_data) == 0:
                # 추천 계산이 안 되는 경우 빈 배열
                pred_user2items[user_id] = []
                continue
            
            recommended_items = model.wv.most_similar(input_data, topn=10)
            pred_user2items[user_id] = [d[0] for d in recommended_items]
        
        # Word2vec에서는 평갓값 예측이 어려우므로 RMSE는 평가하지 않는다.
        # (편의상 테스트 데이터의 예측값을 그대로 반환한다.)
        return RecommendResult(dataset.test.rating, pred_user2items)

if __name__ == "__main__":
    Item2vecRecommender().run_sample()
```

<br><br>

<h1>10. 딥러닝</h1>
<ul>
  <li>
    추천 시스템에 딥러닝이 연구된 것은 2015년 정도 부터이며 진화 속도가 빠르기에 도서에서는 모델을 다루기보다 활용에 초첨을 맞춘다.
  </li>
</ul>

<br>

<h2>10-1. 딥러닝을 활용한 추천 시스템</h2>
<ul>
  <li>
    실무에서는 주로 두 가지 방법으로 딥러닝을 추천 시스템에 활용한다.
  </li>
    <ul>
      <li>
        이미지나 문장 등 <strong>비구조 데이터</strong>의 특징량을 추출기로 활용.
      </li>
      <li>
        <strong>복잡한</strong> 사용자 행동과 아이템 특징량 모델링
      </li>
    </ul>
</ul>

<h3>10-1-1. 이미지나 문장 등 비구조 데이터의 특징량 추출기로 활용</h3>
<ul>
  <li>
    딥러닝의 모델들은 다층 레이어 구조로 되어 있으며 태스크를 해결하는데 필요한 특징이 <strong>각 레이어에서 추출</strong>된다. 즉, 불필요한 정보를 삭제하고 레이어를 경유해서 태스크를 해결하기 위해 정보로 <strong>압축</strong>한다고 생각할 수 있다.
  </li>
  <li>
    딥러닝을 활용하면 카테고리나 태그 정보가 아닌 아이템 콘텐츠 <strong>자체의 유사도</strong>를 사용하기에 비교적 정확도가 높을 수 있다.
  </li>
    <ul>
      <li>
        카테고리나 태그를 이용한 방법은 사람이 직접 정보를 주는 경우가 많기에 시간도 오래 걸리고 정확도도 낮을 수 있다.
      </li>
      <li>
        콘텐츠만으로도 활용이 가능하기에 <strong>콜드 스타트 문제를 해결</strong>할 수 있다.
      </li>
    </ul>
</ul>

<h3>10-1-2. 복잡한 사용자 행동과 아이템 특징량 모델링</h3>
<ul>
  <li>
    추천 시스템에서 딥러닝의 장점은 두 가지로 <strong>비선형 데이터 모델링</strong>과 <strong>시계열 데이터 모델링</strong>으로 볼 수 있다.
  </li>
</ul>

<h4>비선형 데이터 모델링</h4>
<ul>
  <li>
    <strong>Neural Collaborative Filtering</strong>
  </li>
    <ul>
      <li>
        이제까지의 행렬 분해를 포괄하는 일반화된 프레임워크이다. 대신 <strong>신경망이 여러 층</strong>으로 구성되어 있어 사용자와 아이템의 <strong>복잡한 데이터</strong>를 학습할 수 있으며 따라서 기존 행렬 분해보다 <strong>높은 예측 정확도</strong>를 얻을 수 있다.
      </li>
</ul>
    <strong>DeepFM</strong>
  </li>
    <ul>
      <li>
        <strong>Factorization Machines</strong>를 딥러닝화한 방법이다.
      </li>
      <li>
        <strong>특징량 엔지니어링</strong>이 필요 없다.
      </li>
      <li>
        모델 안에서는 <strong>고차원의 각 특징량 조합</strong>도 학습이 가능하다.
      </li>
    </ul>
  <li>
    <strong>Wide and Deep</strong>
  </li>
    <ul>
      <li>
        네트워크는 <strong>Wide 부분</strong>과 <strong>Deep 부분</strong> 두 개로 구성되며 이 둘을 <strong>조합하여 사용</strong>한다.
      </li>
        <ul>
          <li>
            <strong>Wide</strong>: 아이템이나 사용자의 특징량을 입력으로 하여 1층의 선형 모델을 거치며 <strong>함께 자주 발생하는 특징량 조합</strong>을 학습할 수 있다.
          </li>
          <li>
            <strong>Deep</strong>: Embedding층을 내장해 다층으로하여 보다 <strong>일반화된 추상적인 표현</strong>을 얻을 수 있다.
          </li>
        </ul>
      <li>
        <strong>구글 AI Platform</strong>에서 사용할 수 있다.
      </li>
    </ul>
</ul>

<h4>시계열 데이터 모델링</h4>
<ul>
  <li>
    RNN(Recurrent Neural Network) 혹은 LSTM(Long Short-Term Memory)을 시작으로 시계열 데이터를 다루는 뛰어난 방법들이 존재한다.
  </li>
    <ul>
      <li>
        사용자가 클릭한 아이템 리스트를 단어 계열로 보는 등의 방법으로 응용할 수 있다.
      </li>
    </ul>
  <li>
    최근에는 사용자 단위가 아닌 <strong>세션 단위</strong>로 추천하는 수요가 많아지면서 시계열 기반 딥러닝이 많이 활용되고 있다.
  </li>
</ul>

<br>

<h2>10-2. 구현 (p.185)</h2>
<ul>
  <li>
    각 종 딥러닝에 사용할 수 있는 파이썬 라이브러리들이 장표로 추천되어 있다.
  </li>
</ul>

<br>

<h2>10-3. 실무에서의 딥러닝 활용</h2>
<h3>10-3-1. 특징량 추출기로 활용</h3>
<ul>
  <li>
    <strong>학습 완료 모델</strong>을 찾아 자사의 아이템에 적용하고 특징량을 추출할 때 사용한다.
  </li>
  <li>
    학습되어 있는 모델이 없는 경우 자사의 데이터로 딥러닝을 사용해 <strong>처음부터 끝까지 학습</strong> 시키거나 학습 완료된 모델을 세세하게 튜닝해서 대응한다.
  </li>
</ul>

<h3>10-3-2. 예측 모델로 활용</h3>
<ul>
  <li>
    딥러닝 예측을 할 때에는 먼저 <strong>고전적인 방식(k-neareste 등)</strong>을 사용하는 것이 좋다.
  </li>
    <ul>
      <li>
        딥러닝을 사용하는 방법은 구글, 넷틀릭스 그리고 유튜브 등 데이터가 충분한 경우에 예측률이 높다는 결과가 있기 때문이다.
      </li>
    </ul>
</ul>

<br><br>

<h1>11. 슬롯머신 알고리즘(밴딧 알고리즘)</h1>
<ul>
  <li>
    추천 시스템에서는 <strong>탐색과 활용</strong>을 어느 정도 효율적으로 수행해야 서비스의 이익이 최대화 되는가를 최근 중요하게 다루고 있다.
  </li>
  <li>
    위의 대안으로 <strong>다중 슬롯머신</strong>이 있으며 크게 두 가지 목적을 갖고 있다.
  </li>
    <ul>
      <li>
        <strong>특정 기간</strong>의 <strong>누적 이익 최대화(comulative reward maximization)</strong> 혹은 <strong>누적 후회 최소화(cumulative regret minimization)</strong>를 달성하는 것.
      </li>
      <li>
        효율적이면서 정확하게 이익이 최대로 나오는 슬롯 머신을 식별하는 것으로 <strong>최적 슬롯 식별(best arm identification)</strong>이라 한다.
      </li>
        <ul>
          <li>
            기본적으로 누적 이익의 크기를 고려하지 않는 <strong>순수 탐색 문제(pure-exploration problem)</strong>이다.
          </li>
        </ul>
    </ul>
  <li>
    <strong>다중 슬롯머신</strong>은 가장 좋은 슬롯 머신 발견에 특화되어 있지만 <strong>A/B 테스트</strong>와 같이 직관적으로 비교를 하는 방법에서 얻을 수 있는 지식은 얻기 어렵다.
  </li>
  <li>
    추천 시스템에서 슬롯머신 알고리즘이 유용한 대표적인 경우는 콜드 스타트 문제, 개인화 그리고 광고 추천 등이 있다.
  </li>
</ul>