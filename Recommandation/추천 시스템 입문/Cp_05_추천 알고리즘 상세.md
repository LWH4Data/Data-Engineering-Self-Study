<ul>
  <li>
    https://github.com/moseskim/RecommenderSystems 에서 도서의 코드 원본을 제공한다. 주석 설명 표함 미포함 코드도 많기에 꼭 참고할 것을 권한다.
  </li>
  <li>
    알고리즘에 사용되는 수식은 개요만 다루고 있기에 수식적으로 더 알고 싶다면 다른 도서를 참고하는 것을 권한다.
  </li>
</ul>

<br><br>

<h1>1. 알고리즘 비교 (P102)</h1>
<ul>
  <li>
    앞으로 설명할 추천 알고리즘들이 장표로 정리되어 있다.
  </li>
</ul>

<br><br>

<h1>2. MovieLens 데이터셋</h1>
<ul>
  <li>
    영화에 대한 태그 정보가 있어 협조 필터링 뿐만 아니라 콘텐츠 기반 추천 알고리즘도 실험할 수 있어 선정되었다.
  </li>
</ul>

<br>

<h2>2-1. 데이터 다운로드</h2>

```bash
# 실습을 진행할 디렉터리에서 다음을 수행한다.
# 1) 폴더 만들기 (프로젝트 내부)
mkdir -p ./data

# 2) 다운로드
curl -L -o ./data/ml-10m.zip https://files.grouplens.org/datasets/movielens/ml-10m.zip

# 3) 압축 해제
unzip -n ./data/ml-10m.zip -d .
```

<br>

<h2>2-2. MovieLens 데이터 개요</h2>
<ul>
  <li>
    탐색적 데이터 분석(EDA, Exploratory Data Analytics)를 위한 데이터 로드
  </li>
</ul>

```python
import pandas as pd
# 1. 영화 정보 로딩(10681 작품)
# movieID와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv("data/ml-10M100K/movies.dat",
                    names=m_cols,
                    sep'::',
                    encoding='latin-1',
                    engine='python')

# genre를 list 형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))
movies.head()
```

```python
# 2. 사용자가 부여한 영화의 태그 정보 코딩
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat',
                                names=t_cols,
                                sep="::",
                                engine='python')

# tag를 소문자로 변경한다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()
user_tagged_movies.head()
```

```python
# 3. 태그 정보 확인
# 태그 종류=15324
print(f'태그 종류={len(user_tagged_movies.tag.unique())}')
# 태그 레코드 수=95500
print(f'태그 레코드 수={len(user_tagged_movies)}')
# 태그가 붙어 있는 영화 수=7601
print(f'태그가 붙어 있는 영화 수={len(user_tagged_movies.movie_id.unique())}')
```

```python
# 4. 태그 데이터도 다루기 쉽게 영화 ID별로 부여된 태그를 리스트 형시긍로 저장.
# tag를 영화별로 list 형식으로 저장.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

# 태그 정보를 결합.
movies = movies.merge(movie_tags, on='movie_id', how='left')
movies.head()
```

<h3>평갓값 데이터</h3>

```python
# 1. 평갓값 데이터 로딩
# 평갓값 데이터 로딩(데이터양이 많으므로 환경에 따라 로딩에 시간이 걸린다).
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', 
                        names=r_cols,
                        sep='::',
                        engine='python')
ratings.head()
```

```python
# 2. 데이터양이 많기에 사용자 수를 1000명으로 줄여 시험.
#     - 실무에서 데이터양이 많으면 일부 샘플링하여 테스트하고 성능이 좋은 모델을 채택.
#     - 단, sampling 과정에서 편향이 생기지 않도록 하는 것이 중요하다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합.
movielens = ratings.merge(movies, on='movie_id')
movielens.head()
```

<h3>사용자</h3>

```python
# 3. 사용자 1000 명이 영화를 평가한 결과
import numpy as np
movielens.groupby('user_id'). \
                  agg({'movie_id': len}). \
                  agg({'movie_id':[min, max, np.mean, len]})
```

<h3>영화</h3>

```python
# 4. 평가된 영화에 대한 결과.
movielens.groupby('movie_id'). \
                  agg({'user_id':len}). \
                  agg({'user_id':['min', 'max', 'mean', 'len']})
```

```python
# 5. 평갓값 결과 확인.
print(f'평갓값 수={len(movielens)}')
movielens.groupby('rating').agg({'movie_id':len})
```

<br>

<h2>2-3. 평가 방법</h2>
<ul>
  <li>
    각 추천 알고리즘의 성능을 측정하는 방법에 대해 설명한다. (평가 방법 상세는 7장).
  </li>
  <li>
    사용자당 최소 20개 이상의 영화를 평가 했기에 사용자가 최근 평가한 5개의 영화의 평갓값을 테스트용으로 사용하고, 나머지 데이터를 학습용으로 활용한다.
  </li>
</ul>

```python
# 1. 평가 준비를 위한 데이터 처리
"""
- 학습용과 테스트용으로 데이터를 분할한다.
- 각 사용자가 가장 최근에 평가한 5건의 영화를 평가용으로, 그외를 학습용으로 사용한다.
- 각 사용자가 평가한 영화의 순서를 계산한다.
- 직전에 평가한 영화부터 순서를 부여해 나간다.
"""

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 2. 추천 지표를 활용하여 평가
#   - RMSE: 기본적으로 사용하는 평가 지표
#   - Precision@K: 사용자에게 K 개의 아이템을 추천했을 때 그 중 실제로 선호하는 아이템의 
#                  비율 지표
#   - Recall@K: 사용자에게 K 개의 아이템을 추천했을 때 `사용자가 선호하는 아이템 그룹` 중 
#               몇 개가 맞았는지를 나타내는 비율 지표
from typing import List
from sklearn.metrics import mean_squared_error
def calc_rmse(self, true_rating: List[float],
              pred_rating: List[float]) -> float:
    return np.sqrt(mean_squared_error(true_rating, pred_rating))

# Precision@K와 Recall@K의 평균을 평가지표를 실무에서는 함께 사용하는 것을 권장.
```

```python
# 3. rmse외의 평가지표 함수 작성.
# 개별 사용자의 Recall@K 계산
def calc_recall_at_k(
    true_user2items: Dict[int, List[int]],
    pred_user2items: Dict[int, List[int]],
    k: int
) ->float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 recall@k를 계산
    for user_id in true_user2items.keys():
        r_at_k = _recall_at_k(true_user2items[user_id],
                              pred_user2items[user_id], k)
        scores.append(r_at_k)
    return np.mean(scores)

# 전체 Recall@K 평균 계산
def _recall_at_k(self, true_items: List[int],
                 pred_items: List[int], k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0

    r_at_k = (len(set(true_items) & set(pred_items[:k]))) / len(true_items)
    return r_at_k

# 개별 Precision@K 계산.
def calc_precision_at_k(
    true_user2items: Dict[int, List[int]],
    pred_user2items: Dict[int, List[int]],
    k: int
) ->float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 precision@k을 계산
    for user_id in true_user2items.keys():
        p_at_k = _precision_at_k(true_user2items[user_id],
                                 pred_user2items[user_id], k)
        scores.append(p_at_k)
    return np.mean(scores)

# 전체 Precision@K 평균 계산.
def _precision_at_k(true_items: List[int],
                    pred_items: List[int], k: int) -> float:
    if k == 0:
        return 0.0
    p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k
    return p_at_k
```

<br>

<h2>2-4. 통일된 포맷을 활용한 계산</h2>
<ul>
  <li>
    다양한 모델을 하나의 통합된 클래스에 적용해서 간단하게 적용하도록 하기 위해서는 통일된 형식을 잘 구성해 두어야 한다.
  </li>
</ul>

```python
# 1. 다양한 모델 적용을 위한 통합된 형식.

# 데이터 로딩과 평가 지표 계산 공통 모듈 로딩
from util.data_loader import DataLoader
from util.metric_calculator import MetricCalculator

# 1-1) MovieLens 데이터 로딩
data_loader = DataLoader(num_users=1000, 
                         num_test_items=5,
                         data_path='../data/ml-10M100K/')
movielens = data_loader.load()

# 1-2) 각종 알고리즘 구현
recommender = XXXRecommender()
recommend_result = recommender.recommend(movielens)

# 1-3) 평가 지표 계산
metric_calculator = MetricCalculator()
metrics = metric_calculator.calc(
    movielens.test.rating.tolist(), recommend_result.rating.tolist(),
    movielens.test_user2items, recommend_result.user2item, k=10)
print(metrics)
```

<h3>2-4-1. MovieLens 데이터 로딩</h3>
<ul>
  <li>
    앞서 소개한 데이터 전처리 과정을 하나의 클래스(DataLoader)로 구현한다.
  </li>
  <li>
    train_test_split과 같은 방법을 사용하지 않고 직접 클래스를 구현하는 이유는 추천 시스템에서는 일반적인 분할보다 <strong>사용자 단위로 특수한 로직</strong>이 필요하기 때문이다.
  </li>
</ul>

```python
import pandas as pd
import os
from util.models import Dataset

class DataLoader:
    def __init__(self, num_users: int = 1000, num_test_items: int = 5, data_path: str = "../data/ml-10M100K/"):
        self.num_users = num_users
        self.num_test_items = num_test_items
        self.data_path = data_path

    def load(self) -> Dataset:
        ratings, movie_content = self._load()
        movielens_train, movielens_test = self._split_data(ratings)
        # 순위용 평가 데이터는 각 사용자의 평갓값이 4 이상인 영화만 정답으로 한다.
        # 키는 사용자 ID, 값은 사용자가 고평가한 ID 리스트.
        movielens_test_user2item = (
            movielens_test[movielens_test.rating >= 4].groupby(
                "user_id").agg({"movie_id": list}).to_dict()
        )
        return Dataset(movielens_train, movielens_test, movielens_test_user2item, movie_content)

    def _split_data(self, movielens: pd.DataFrame) -> (pd.DataFrame, pd.DataFrame):
        # 학습용과 테스트용 데이터를 분할한다.
        # 각 사용자가 가장 최근에 평가한 5건을 평가용으로 사용하고 그 외는 학습용으로 사용한다.
        # 먼저 각 사용자가 평가한 영화의 순서를 계산한다.
        # 직전에 평가한 영화부터 순서를 부여한다. (0 부터 시작).
        movielens["rating_order"] = movielens. \
                                    groupby("user_id")["timestamp"]. \
                                    rank(ascending=False, method="first")
        movielens_train = movielens[movielens["rating_order"] > self.num_test_items]
        movielens_test = movielens[movielens["rating_order"] <= self.num_test_items]
        return movielens_train, movielens_test

    def _load(self) -> (pd.DataFrame, pd.DataFrame):
        # 영화 정보 로딩(10197 작품)
        # movie_id와 제목만 사용
        m_cols = ["movie_id", "title", "genre"]
        movies = pd.read_csv(
            os.path.join(self.data_path, "movie.dat"), names=m_cols,
            sep="::", encoding="latin1-1", engine="python"
        )
        # genre를 list 형식으로 저장.
        movies["genre"] = movies.genre.apply(lambda x:list(x.split("|")))
        # 사용자가 부여한 영화 태그 정보 로딩
        t_cols = ["user_id", "movie_id", "tag", "timestamp"]
        user_tagged_movies = pd.read_csv(
            os.path.join(self.data_path, "tags.dat"), names = t_cols,
            sep = "::", engine = "python"
        )
        
        # tag를 소문자로 한다.
        user_tagged_movies["tag"] = user_tagged_movies["tag"].str.lower()
        movie_tags = user_tagged_movies.groupby("movie_id").agg({"tag":list})
        
        # 태그 정보를 결합한다.
        movies = movies.merge(movie_tags, on="movie_id", how="left")
        
        # 평가 데이터 로딩
        r_cols = ["user_id", "movie_id", "rating", "timestamp"]
        ratings = pd.read_csv(os.path.join(self.data_path,
        "ratings.dat"), names=r_cols, sep="::", engine="python")
        # 사용자 수를 num_users로 줄인다.
        valid_user_ids = sorted(ratings.user_id.unique())[:
                        self.num_users]
        ratings = ratings[ratings.user_id <= max(valid_user_ids)]
        
        # 위의 데이터를 결합한다.
        movielens_ratings = ratings.merge(movies, on="movie_id")
        return movielens_ratings, movies
```

```python
# 2. 데이터셋 생성자
#   - @dataclasses가 생성자를 자동 생성한다.
import dataclasses
from typing import Dict, List
@dataclasses.dataclass(frozen=True)

# 추천 시스템 학습과 평가에 사용하는 데이터셋
class Dataset:
    
    # 학습용 평갓값 데이터셋
    train: pd.DataFrame
    # 테스트용 평갓값 데이터 셋
    test:pd.DataFrame

    # 순위 지표 테스트 데이터셋
    #   - key: 사용자 ID, value: 사용자가 높에 평가한 아이템 ID 리스트
    test_user2item: Dict[int, List[int]]
    # 아이템 콘텐츠 정보
    item_content: pd.DataFrame
```

<h3>2-4-2. 각종 알고리즘 구현</h3>
<ul>
  <li>
    dataset을 받아 테스트 데이터의 추천 결과를 반환하는 알고리즘을 구현한다.
  </li>
  <li>
    각 알고리즘은 BaseRecommender 클래스를 상속하는 형태로 구현한다.
  </li>
</ul>

```python
# 1. 각 알고리즘을 적용할 공통 클래스 생성
import sys, os
sys.path.append(os.path.abspath("..")) 

from abc import ABC, abstractmethod
from util.data_loader import DataLoader
from util.metric_calculator import MetricCalculator
from util.models import Dataset, RecommendResult

class BaseRecommender(ABC):
    @abstractmethod
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        pass

    def run_sample(self) -> None:
        # MovieLens의 데이터 취득
        movielens = DataLoader(num_users=1000, num_test_items=5,
                               data_path="../data/ml-10M100K/").load()

        # 추천 계산
        recommend_result = self.recommend(movielens)

        # 추천 결과 평가
        metrics = MetricCalculator().calc(
            movielens.test.rating.tolist(),
            recommend_result.rating.tolist(),
            movielens.test_user2item,
            recommend_result.user2item,
            k=10
        )
        print(metrics)
```

<h3>2-4-3. 평가 지표 계산</h3>
<ul>
  <li>
    추천 결과를 기반으로 MetricCaculator를 사용해 RMSE와 순위 지표의 Precision@K 그리고 Recall@K를 계산한다.
  </li>
</ul>

```python
# 1. 평가 지표 계산 클래스
import numpy as np
from sklearn.metrics import mean_squared_error
from util.models import Metrics
from typing import Dict, List

class MetricCalculator:
    def calc(
        self,
        true_rating: List[float],
        pred_rating: List[float],
        true_user2item: Dict[int, List[int]],
        pred_user2item: Dict[int, List[int]],
        k: int,
    ) -> Metrics:
        rmse = self._calc_rmse(true_rating, pred_rating)
        precision_at_k = self._calc_precision_at_k(true_user2items,
                         pred_user2items, k)
        recall_at_k = self._calc_recall_at_k(true_user2items, pred_user2items, k)
        return Metrics(rmse, precision_at_k, recall_at_k)

    def _precision_at_k(self, true_items: List[int], pred_items: List[int], k:int) -> float:
        if k == 0:
            return 0.0

        p_at_k = (len(set(true_items) & set(pred_items[:k]))) / k
        return p_at_k

def _recall_at_k(self, true_items: List[int], pred_items: List[int], k: int) -> float:
    if len(true_items) == 0 or k == 0:
        return 0.0

    r_at_k = len(set(true_items) & set(pred_items[:k])) / len(true_items)
    return r_at_k

def _calc_rmse(self, true_rating: List[float], pred_rating: List[float]) -> float:
    return np.sqrt(mean_squared_error(true_rating, pred_rating))

def _calc_recall_at_k(
    self, true_user2items: Dict[int, List[int]], pred_user2items: Dict[int, List[int]], k: int) -> float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 recall@k를 계산한다.
    for user_id in true_user2items.keys():
        r_at_k = self._recall_at_k(true_user2items[user_id], pred_user2items[user_id], k)
        scores.append(r_at_k)
    return np.mean(scores)

def _calc_precision_at_k(
    self, true_user2items: Dict[int, List[int]], pred_user2items: Dict[int, List[int]], k:int) -> float:
    scores = []
    # 테스트 데이터에 존재하는 각 사용자의 precision@k를 계산한다.
    for user_id in true_user2items.keys():
        p_at_k = self._precision_at_k(true_user2items[user_id], pred_user2items[user_id], k)
        scores.append(p_at_k)
    return np.mean(scores)
```

<br><br>

<h1>3. 무작위 추천</h1>
<ul>
  <li>
    난수 데이터를 활용한 무작위 모델 테스트
  </li>
    <ul>
      <li>
        MovieLens의 평가값이 0.5 ~ 5.0 이기에 <strong>0.5 ~ 5.0의 난수</strong>를 생성한다.
        <br>→ 학습용 데이터의 사용자와 아이템으로 <strong>사용자 X 아이템 행렬</strong>을 만들고 난수를 저장한다.
        <br>→ <strong>순위 지표 계산용</strong>으로 pred_user2items 딕셔너리를 생성한다.
        <br>→ 생성한 dict의 <strong>key에 user_id</strong>, <strong>value에 아직 사용자가 평가하지 않은 영화</strong>부터 무작위로 10</strong>개의 영화를 저장한다.
      </li>
    </ul>
</ul>

```python
# 1. 테스트를 위한 데이터 준비
import pandas as pd

# movieId와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names = m_cols, sep='::', engine='python')

# genre를 list 형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩한다.
t_cols = ['user_id', 'movie_id', 'tag', 'timestmamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

# 태그 정보를 결합한다.
movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩한다.
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터량이 많기에 사용자수를 1000으로 줄여서 시험한다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합한다.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movies.movie_id.unique())}')

# 학습용과 데이터용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평가용으로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작).

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp'] <= 5]
```

```python
# 2. 사용자 ID와 아이템 ID에 대해 0부터 시작하는 인덱스를 할당한다.
unique_user_ids = sorted(movielens_train.user_id.unique())
unique_movie_ids = sorted(movielens_train.movie_id.unique())
user_id2index = dict(zip(unique_user_ids, range(len(unique_user_ids))))
movie_id2index = dict(zip(unique_movie_ids, range(len(unique_movie_ids))))
```

```python
# 3. 사용자 X 아이템 행렬에서 각 예측 평갓값은 0.5 ~ 5.0의 균등 난수로 한다.
import numpy as np

pred_matrix = np.random.uniform(0.5, 5.0, (len(unique_user_ids), len(unique_movie_ids)))
```

```python
# 4. rmse 평가용으로 테스트 데이터에 나타나는 사용자와 아이템의 예측 평갓값을 저장한다.
movie_rating_predict = movielens_test.copy()
pred_results = []
for i, row in movielens_test.iterrows():
    user_id = row["user_id"]
    # 테스트 데이터의 아이템 ID가 학습용에 나타나지 않는 경우에도 난수를 저장한다
    if row["movie_id"] not in movie_id2index:
        pred_results.append(np.random.uniform(0.5, 5.0))
        continue
    # 테스트 데이터에 나타난 사용자 ID와 아이템 ID의 인덱스를 얻어, 평갓값 행렬의 값을 얻는다
    user_index = user_id2index[row["user_id"]]
    movie_index = movie_id2index[row["movie_id"]]
    pred_score = pred_matrix[user_index, movie_index]
    pred_results.append(pred_score)
movie_rating_predict["rating_pred"] = pred_results
```

```python
# 5. 순위 평가용 데이터를 작성한다.
from collections import defaultdict

# 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서 10작품을 무작위로 선택.
# key는 사용자 ID, value는 추천 아이템 ID 리스트.
pred_user2items = defaultdict(list)
# 사용자는 이미 평가한 영화를 얻는다.
user_evaluated_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()
for user_id in unique_user_ids:
    user_index = user_id2index[user_id]
    movie_indexes = np.argsort(-pred_matrix[user_index, :])
    for movie_index in movie_indexes:
        movie_id = unique_movie_ids[movie_index]
        if movie_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break
pred_user2items
```

```python
# 6. user_id=2인 사용자가 학습 데이터에서 평가를 부여한 영화 목록
movielens_train[movielens_train.user_id==2]
```

<br><br>

<h1>4. 통계 정보나 특정 규칙에 기반한 추천</h1>
<ul>
  <li>
    <strong>통계 추천 예</strong>
  </li>
    <ul>
      <li>
        직전 1개월의 총 매출 수, 열람 수, 사용자에 따른 평갓값의 평균 등 <strong>서비스의 통계 정보</strong>를 사용해 아이템을 나열하여 사용자에게 추천.
      </li>
      <li>
        아이템 가격이나 크기와 같이 <strong>특정 속성 순서</strong>로 나열해 사용자에게 추천
      </li>
      <li>
        사용자의 나이 등과 같은 <strong>특정 속성 정보</strong>에 기반해 다른 아이템 추천.
      </li>
    </ul>
  <li>
    통계 정보나 아이템의 속성 정보에 기반해 나열하는 추천은 사용자에 의존하지 않기에 <strong>개인화를 수행하지 않는다</strong>.
  </li>
    <ul>
      <li>
        따라서 추천 시스템의 콘텍스트에 관계없이 시스템이 갖고 있는 데이터만을 활용하기에 구현하기가 비교적 쉽다.
      </li>
    </ul>
  <li>
    비교적 단순한 알고리즘은 <strong>사용자 또한 어떤 구조로 추천되는지 이해하기가 쉽기 떄문에</strong> 사용자의 구매 행동과 연결되는 경우가 많아 얕볼 수 없다.
  </li>
  <li>
    사용자의 속성 정보를 기반으로 다른 아이템을 추천하는 경우, 사용자의 속성 정보를 기반으로 몇 세그먼트로 나누어 각각의 <strong>세그먼트 별로 추천을 진행</strong>한다.
  </li>
  <li>
    사용자의 나이나 성별, 거주지 등의 인구 <strong>통계학적 데이터</strong>에 기반해 아이템을 추천하는 것을 <strong>데모그래픽 필터링(demographic filtering)</strong>이라 한다.
  </li>
    <ul>
      <li>
        서비스나 사용자의 성질에 따라 데모그라픽 정보에 <strong>정확하지 않은 정보</strong>가 입력될 수 있음을 주의해야 한다.
      </li>
      <li>
        <strong>공평성(fairness) 관점</strong>에서 데모그래픽 데이터를 사용할 때 주의가 필요하다. 예를 들면 사용자에 성별을 묻는 것이 해당한다.
      </li>
    </ul>
</ul>

```python
# 1. 과거에 남긴 평갓값 중 값이 높은 순서대로 추천하는 예
import numpy as np

# 평갓값이 높은 영화 확인.
movie_stats = movielens.train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
movie_stats.sort_values(by=('rating', 'mean'), ascending=False).head()
```

```python
# 2. 평가 수가 적은 데이터가 많기 때문에 임곗값을 활용해 일정 이상의 평가 수가 있는 영화만 필터링한다.
#   - 추천 규칙을 정할 때에는 집계 기간과 다양성을 고려해야 한다.
movie_stats = movielens.train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
atleast_flg = movie_stats['rating']['size'] >= 100
movies_sorted_by_rating = movie_stats[atleast_flg].sort_values(
    by=('rating', 'mean'), ascending=False)
movies_sorted_by_rating.head()
```

```python
# 3. 평갓값이 높은 순의 추천 시스템 구현을 위해 데이터 전처리
import pandas as pd

# movieID와 제목만 사용
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names=m_cols, sep='::', encoding='latin-1', engine='python')

# genre를 list 형식으로 저장한다.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩한다.
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag': list})

# 태그 정보를 결합한다.
movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩한다.
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터양이 많으므로 사용자수를 1000으로 줄여서 시험한다.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터화 평가 데이터를 결합한다.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movielens.movie_id.unique())}')

# 학습용과 테스트용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평가용으로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작)

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 4. 평가 수의 임곗값
minimum_num_rating = 200
```

```python
# 5. 평갓값의 평균이 높은 영화를 확인한다.
import numpy as np

# 평가 수가 1건인 영화가 상위에 여럿 나타난다.
movie_stats = movielens_train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
movie_stats.sort_values(by=('rating', 'mean'), ascending=False).head()
```

```python
# 6. 임곗값을 도입해 평가 수가 적은 영화를 제거한다.
#   - 쇼생크 탈출이나 7인의 사무라이 등 익숙한 영화가 상위에 나타난다.
movie_stats = movielens_train.groupby(['movie_id', 'title']).agg({'rating': [np.size, np.mean]})
atleast_flg = movie_stats['rating']['size'] >= 100
movies_sorted_by_rating = movie_stats[atleast_flg].sort_values(by=('rating', 'mean'), ascending=False)
movies_sorted_by_rating.head()
```

```python
# 7. 평갓값이 높은 순의 추천 시스템의 성능을 측정.
import numpy as np

# 각 아이템별로 평균 평갓값을 계산하고, 해당 평균 평갓값을 예측값으로 사용한다.
movie_rating_average = movielens_train.groupby("movie_id").agg({"rating": np.mean})

# 테스트 데이터에 예측값을 저장한다. 테스트 데이터에만 존재하는 아이템의 예측 평갓값은 0으로 한다.
movie_rating_predict = movielens_test.merge(
    movie_rating_average, on="movie_id", how="left", suffixes=("_test", "_pred")
).fillna(0)
```

```python
# 8. 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서 평갓값이 높은 10작품으로 한다

from collections import defaultdict

# 단, 평가 건수가 작으면 노이즈가 크므로 minimum_num_rating건 이상 평가가 있는 영화로 필터링한다
pred_user2items = defaultdict(list)
user_watched_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()
movie_stats = movielens_train.groupby("movie_id").agg({"rating": [np.size, np.mean]})
atleast_flg = movie_stats["rating"]["size"] >= minimum_num_rating
movies_sorted_by_rating = (
    movie_stats[atleast_flg].sort_values(by=("rating", "mean"), ascending=False).index.tolist()
)

for user_id in movielens_train.user_id.unique():
    for movie_id in movies_sorted_by_rating:
        if movie_id not in user_watched_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break
pred_user2items
```

```python
# 9. user_id=2인 사용자가 학습 데이터에 평가를 부여한 영화 목록
movielens_train[movielens_train.user_id==2]
```

```python
# 10. 특정 사용자 혹은 아이템의 추천 목록
pred_user2items[2]

# user_id=2에 대한 추천(318, 50, 527)
movies[movies.movie_id.isin([318, 50, 527])]
```

<br><br>

<h1>5. 연관 규칙</h1>
<ul>
  <li>
    <strong>연관 규칙(association rule)</strong>에서는 대량의 구매 이력 데이터로부터 '아이템 A와 아이템 B는 <strong>동시에 구입</strong>하는 경우가 많다'는 규칙을 발견한다.
  </li>
    <ul>
      <li>
        의회의 조합을 발견할 수 있다는 특징이 있다.
      </li>
      <li>
        연관 규칙 자체는 오래전부터 있었으나 계산 방법 자체는 매우 간단하여 SQL로도 구현할 수 있어 널리 사용되고 있다.
      </li>
    </ul>
  <li>
    연관 규칙에는 <strong>지지도(support)</strong>, <strong>확신도(confidence)</strong> 그리고 <strong>리프트값(lift)</strong>이라는 중요한 세 가지 개념이 있다.
  </li>
</ul>

<br>

<h2>5-1. 지지도</h2>
<ul>
  <li>
    <strong>지지도</strong>란 어떤 아이템이 <strong>전체 중에서 출현한 비율</strong>을 의미한다.
  </li>
  <li>
    동시에 출현하는 빈도를 계산할 때에도 <strong>and 조건</strong>을 적용해 계산할 수 있다.
  </li>
</ul>

$$ \text{Support(A)} = \frac{\text{Count}(A)}{\text{Total count}} $$

<br>

<h2>5-2. 확신도</h2>
<ul>
  <li>
    <strong>확신도</strong>는 <strong>아이템 A가 나타났을 때 아이템 B가 나타날 비율</strong>을 의미한다.
  </li>
  <li>
    A를 <strong>조건부(antecedents)</strong>, B를 <strong>귀결부(consequents)</strong>라 한다.
  </li>
</ul>

$$ \text{Confidence}(A \Rightarrow B) = \frac{\text{Count}(A \text{ and } B)}{\text{Count}(A)} $$

<br>

<h2>5-3. 리프트값</h2>
<ul>
  <li>
    <strong>리프트값</strong>이란 아이템 A와 아이템 B의 출현이 어느 정도 <strong>상관관계</strong>를 갖는지를 의미한다.
  </li>
    <ul>
      <li>
        <strong>리프트 = 1</strong>: 아이템 A와 아이템 B는 <strong>독립적</strong>이다.
      </li>
      <li>
        <strong>리프트 > 1</strong>: 아이템 A와 아이템 B는 <strong>양의 상관관계</strong>를 갖는다.
      </li>
      <li>
        <strong>0 <= 리프트 < 1</strong>: 아이템 A와 아이템 B는 <strong>음의 상관관계</strong>를 갖는다.
      </li>
    </ul>
  <li>
    리프트값에 <strong>log</strong>를 취하면 <strong>점별 상호정보량(Pointwise Mutual Information, PMI)</strong>이 되며 <strong>word2vec</strong> 알고리즘이 이를 활용한다.
  </li>
  <li>
    아이템이 세 개 이상인 경우에도 리프트값을 계산할 수 있다.
  </li>
</ul>

$$ \text{Lift}(A \Rightarrow B) = \frac{\text{Support}(A \text{ and } B)}{\text{Support}(A) * \text{Support}(B)} $$


$$ \text{Lift}((A \text{ and } B) \Rightarrow C) = \frac{\text{Support}(A \text{ and } B \text{ and } C)}{\text{Support}(A \text{ and } B) * \text{Support}(C)} $$

<br>

<h2>5-4. Apriori 알고리즘을 활용한 고속화</h2>
<ul>
  <li>
    리프트값의 문제는 아이템의 수와 사용자의 수가 커질수록 <strong>조합 방법이 기하급수적으로 늘어나기에</strong> 계산이 어렵다는 것이다.
  </li>
  <li>
    위의 문제를 개선하기 위해 <strong>Apriori 알고리즘</strong>이 제안되었다. Apriori 알고리즘은 <strong>지지도가 일정 이상</strong>인 아이템이나 아이템의 조합만 계산하기에 비교적 빠르다.
  </li>
    <ul>
      <li>
        <strong>지지도의 임곗값</strong>을 결정하는 것이 중요하다. 너무 임곗값이 높으면 일부 인기 아이템만 추천되고, 반대로 낮다면 다시 계산 시간이 증가한다.
      </li>
    </ul>
</ul>

```python
# 1. Movielens 데이터 로딩
import pandas as pd

# movieID와 제목만 사용.
m_cols = ['movie_id', 'title', 'genre']
movies = pd.read_csv('../data/ml-10M100K/movies.dat', names=m_cols, sep='::', encoding='latin-1', engine='python')

# genre를 list형식으로 저장.
movies['genre'] = movies.genre.apply(lambda x:x.split('|'))

# 사용자가 부여한 영화의 태그 정보를 로딩.
t_cols = ['user_id', 'movie_id', 'tag', 'timestamp']
user_tagged_movies = pd.read_csv('../data/ml-10M100K/tags.dat', names=t_cols, sep='::', engine='python')

# tag를 소문자로 바꾼다.
user_tagged_movies['tag'] = user_tagged_movies['tag'].str.lower()

# tag를 영화별로 list 형식으로 저장한다.
movie_tags = user_tagged_movies.groupby('movie_id').agg({'tag':list})

movies = movies.merge(movie_tags, on='movie_id', how='left')

# 평갓값 데이터만 로딩
r_cols = ['user_id', 'movie_id', 'rating', 'timestamp']
ratings = pd.read_csv('../data/ml-10M100K/ratings.dat', names=r_cols, sep='::', engine='python')

# 데이터량이 많으므로 사용자 수를 1000으로 줄여서 시험.
valid_user_ids = sorted(ratings.user_id.unique())[:1000]
ratings = ratings[ratings["user_id"].isin(valid_user_ids)]

# 영화 데이터와 평가 데이터를 결합.
movielens = ratings.merge(movies, on='movie_id')
print(f'unique_users={len(movielens.user_id.unique())}, unique_movies={len(movielens.movie_id.unique())}')

# 학습용과 테스트용으로 데이터를 나눈다.
# 각 사용자의 최근 5건의 영화를 평강용르로 사용하고, 나머지는 학습용으로 사용한다.
# 우선, 각 사용자가 평가한 영화의 순서를 계산한다.
# 최근 부여한 영화부터 순서를 부여한다(1에서 시작)

movielens['timestamp_rank'] = movielens.groupby(
    'user_id')['timestamp'].rank(ascending=False, method='first')
movielens_train = movielens[movielens['timestamp_rank'] > 5]
movielens_test = movielens[movielens['timestamp_rank'] <= 5]
```

```python
# 사용자 X 영화 행렬 형식으로 변환한다.
user_movie_matrix = movielens_train.pivot(index='user_id', columns='movie_id', values='rating')

# 라이브러리를 사용하기 위해 4 이상의 평갓값은 1, 4 미만의 평갓값과 결손값은 0으로 한다.
user_movie_matrix[user_movie_matrix < 4] = 0
user_movie_matrix[user_movie_matrix.isnull()] = 0
user_movie_matrix[user_movie_matrix >= 4] = 1

user_movie_matrix.head()
```

```python
# Association 규칙 라이브러리 설치
# !pip install mlxtend
```

```python
from mlxtend.frequent_patterns import apriori

# 지지도가 높은 영화를 표시
freq_movies = apriori(
    user_movie_matrix, min_support=0.1, use_colnames=True)
freq_movies.sort_values('support', ascending=False).head()
```

```python
# movie_id = 593의 제목 확인(양들의 침묵)
movies[movies.movie_id == 593]
```

```python
from mlxtend.frequent_patterns import association_rules

# Association 규칙 계산(리프트 값이 높은 순으로 표시).
rules = association_rules(freq_movies, metric='lift', min_threshold=1)
rules.sort_values('lift', ascending=False).head()[['antecedents', 'consequents', 'lift']]
```

```python
# movie_id = 4993, 5952의 제목 확인(반지의 제왕)
movies[movies.movie_id.isin([4993, 5952])]
```

```python
# 학습용 데이터 평갓값이 4 이상인 것만 얻는다.
movielens_train_high_rating = movielens_train[movielens_train.rating >= 4]
```

```python
# user_id=2인 사용자가 4이상의 평가를 남긴 영화 목록
movielens_train_high_rating[movielens_train_high_rating.user_id==2]
```

```python
# user_id=2의 사용자가 4 이상의 평가를 남긴 영화 목록
user2_data = movielens_train_high_rating[movielens_train_high_rating.user_id==2]

# 사용자가 최근 평가한 4개의 영화 얻기
input_data = user2_data.sort_values("timestamp")["movie_id"].tolist()[-5:]

# 해당 영화들이 조건부로 포함된 Association 규칙을 추출.
matched_flags = rules.antecedents.apply(lambda x: len(set(input_data) & x)) >= 1
rules[matched_flags]
```

```python
from collections import defaultdict, Counter

# Association 규칙의 귀결부의 영화를 리스트에 저장.
# 같은 영화가 여러 차례 귀결부에 나타날 수 있다.

consequent_movies = []
for i, row in rules[matched_flags].sort_values("lift", ascending=False).iterrows():
    consequent_movies.extend(row["consequents"])

# 귀결부에서의 출현 빈도 카운트
counter = Counter(consequent_movies)
counter.most_common(10)
```

```python
# movie_id=1196이 92번 귀결부에 출현하기에 user_id=2에는 movie_id=1196(Star Wars: Episode V)가 추천 후보가 된다.
# (user_id=2의 학습 데이터에는 Star Wars 에피소드 4, 6의 평가가 높다)
movies[movies.movie_id == 1196]
```

```python
# 추천하는 방법에는 lift 값이 높은 것을 추출하는 방법 등이 있다.
# 몇 가지 방법을 시도해 보고 자사의 데이터에 맞는 방법을 선택한다.

# Association 규칙을 사용해 각 사용자가 아직 평가하지 않은 영화 10편을 추천한다.
pred_user2items = defaultdict(list)
user_evaluated_movies = movielens_train.groupby("user_id").agg({"movie_id": list})["movie_id"].to_dict()

for user_id, data in movielens_train_high_rating.groupby("user_id"):
    # 사용자가 최근 5편의 영화를 얻는다.
    input_data = data.sort_values("timestamp")["movie_id"].tolist()[-5:]
    # input_data의 영화들이 조건부에 1편이라도 포함되어 있는 Association 규칙을 추출한다.
    matched_flags = rules.antecedents.apply(lambda x: len(set(input_data) & x)) >= 1

    # Association 규칙의 귀결부의 영화를 리스트로 저장하고, 출판 빈도 순으로 배열한다.
    # 사용자가 아직 평가하지 않았다면 추천 리스트에 추가한다.
    consequent_movies = []
    for i, row in rules[matched_flags].sort_values("lift", ascending=False).iterrows():
        consequent_movies.extend(row["consequents"])
    # 출현 빈도를 센다.
    counter = Counter(consequent_movies)
    for movie_id, movie_cnt in counter.most_common():
        if movie_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].append(movie_id)
        # 추천 리스트가 10편이 되면 종료.
        if len(pred_user2items[user_id]) == 10:
            break

# 각 사용자에 대한 추천 리스트
pred_user2items
```

```python
# user_id=2에 대한 추천(1196, 593, 1198)
movies[movies.movie_id.isin([1196, 593, 1198])]

# apriori(user_movie_matrix, min_support=0.1, use_colnames=True)
# association_rules(freq_movies, metric='lift', min_threshold=1)
# min_support와 min_threshold가 중요한 파라미터이기에 바꾸어 가면서 테스트.
```

<br><br>

<h1>6. 사용자-사용자 메모리 기반 방법 협조 필터링</h1>
<ul>
  <li>
    메모리 기반 방식은 사용자 데이터를 축적하고 있다 <strong>추천되는 시점</strong>에 모든 데이터를 계산한다. 따라서 다른 알고리즘에 비해 <strong>시간</strong>이 더 소모된다.
  </li>
  <li>
    추천의 순서는 다음과 같다.
  </li>
    <ul>
      <li>
        미리 얻은 평갓값을 사용해 <strong>사용자 사이의 유사도를 계산</strong>하고 추천받을 사용자와 <strong>기호 경향이 비슷한 사용자</strong>를 찾는다.
      </li>
      <li>
        기호 경향이 비슷한 사용자의 평갓값으로 추천받을 사용자의 <strong>미지의 아이템에 대한 예측 평갓값을 계산</strong>한다.
      </li>
      <li>
        예측 평갓값이 높은 아이템을 사용해 사용자에게 추천한다.
      </li>
    </ul>
  <li>
    사용자 사이의 유사도를 계산할 때에는 <strong>피어슨 상관 계수</strong>를 사용한다.
  </li>
</ul>

$$ p_{ax}=\frac{\sum_{y \in Y_{ax}}(r_{ay}-\bar r_a)(r_{xy}-\bar r_x)}{\sqrt{\sum_{y \in Y_{ax}}(r_{ay}-\bar r_a)^2} \sqrt{\sum_{y \in Y{ax}}(r_{xy}- \bar r_x)^2} } $$

```python
# 1. 피어슨 상관 계수 함수.
def pearson_coefficient(u: np.ndarry, v: np.ndarray) -> float:
    u_diff = u - np.mean(u)
    v_diff = v - np.mean(v)
    numerator = np.dot(u_diff, v_diff)
    denominator = np.sqrt(sum(u_diff**2))*np.sqrt(sum(v_diff**2))
    if denominator == 0:
        return 0.0
    return numerator / denominator
```

```python
# 2. 평갓값을 사용자 X 영화의 행렬로 변환
user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
user_id2index = dict(zip(user_movie_matrix.index,
                range(len(user_movie_matrix.index))))
movie_id2index = dict(zip(user_movie_matrix.columns,
                  range(len(user_movie_matrix.columns))))

# 예측 대상 사용자와 영화의 조합
movie_rating_predict = dataset.test.copy()
pred_user2items = defaultdict(list)

# 예측 대상 사용자 ID
test_users = movie_rating_predict.user_id.unique()

# 예측 대상 사용자(사용자 1)에게 주목한다.
for user1_id in test_users:
    similar_users = []
    similarities = []
    avgs = []

    # 사용자 1과 평갓값 행렬 내 다른 사용자(사용자 2)의 유사도를 산출한다.
    if user1_id == user2_id:
      continue

    # 사용자 1과 사용자 2의 평갓값 벡터
    u_1 = user_movie_matrix.loc[user1_id, :].to_numpy()
    u_2 = user_movie_matrix.loc[user2_id, :].to_numpy()

    # u_1과 u_2 모두 결손값이 없는 것만 추출한 벡터를 얻는다.
    common_items = (~np.isnan(u_1) & ~np.isnan(u_2))
    # 공통으로 평가한 아이템이 없으면 스킵한다.
    if not common_items.any():
        continue
    u_1, u_2 = u_1[common_items], u_2[common_items]

    # 피어슨 상관 계수를 사용해 사용자 1과 사용자 2의 유사도를 산출한다.
    rho_12 = pearson_coefficient(u_1, u_2)

    # 사용자 1과의 유사도가 0보다 크면 사용자 2를 비슷한 사용자로 간주한다.
    if rho_12 > 0:
        similar_users.append(user2_id)
        similarities.append(rho_12)
        avgs.append(np.mean(u_2))
```

<h4>사용자별 평균 평갓값에서 해당 아이템에 대한 평가가 얼마나 높은지 상대 적인 평갓값에 주목해 해당 값의 가중 평균을 얻어 평가한다.</h4>

$$ \hat r_{ay}=\hat r_a + \frac{\sum_{x \in X_y}p_{ax}(r_{xy}-\hat r_x)}{\sum_{x \in X_y}|p_{ax}|} $$

```python
# 사용자 1의 평균 평갓값
avg_1 = np.mean(user_movie_matrix.loc[user1_id, :].dropna().to_numpy())

# 예측 대상 영화의 ID
test_movies = movie_rating_predict[movie_rating_predict[
                  "user_id"]==user1_id].movie_id.values

# 예측할 수 없는 영화에 대한 평갓값은 사용자 1의 평균 평갓값으로 한다.
movie_rating_predict.loc[(movie_rating_predict[
"user_id"]==user1_id, "rating_pred"] = avg_ 1

if similar_users:
    for movie_id in test_movies:
        if movie_id in movie_id2index:
            r_xy = user_movie_matrix.loc[similar_users,
                    movie_id].to_numpy()
            rating_exists = ~np.isnan(r_xy)

            # 비슷한 사용자가 대상 영화에 대한 평갓값을 갖고 있지 않으면 skip
            if not rating_exists.any():
                continue
            
            r_xy = r_xy[rating_exists]
            rho_1x = np.array(similarities)[rating_exists]
            avg_x = np.array(avgs)[rating_exists]
            r_hat_1y = avg_1 + np.dot(rho_1x, (r_xy - avg_x)) / 
                      rho_1x.sum()
            
            # 예측 평갓값을 저장한다.
            movie_rating_predict.loc[(movie_rating_predict[
            "user_id"]==user1_id) & (movie_rating_predict[
            "movie_id"]==movie_id), "rating_pred"] = r_hat_1y
```

<p>다양한 라이브러리를 통해서도 메모리 기반 방법 협조 필터링을 구현할 수 있다. 하단은 Surprise를 사용한다.</p>

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np

from surprise import KNNWithMeans, Reader
from surprise import Dataset as SurpriseDataset

np.random.seed(0)

class UMCFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        
        # 평갓값을 사용자 X 영화 행렬로 변환한다.
        user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))

        # 평갓값을 예측할 테스트용 데이터
        movie_rating_predict = dataset.test.copy()
        # 각 사용자에 대한 순위 형식의 추천 리스트를 저장하는 딕셔너리
        pred_user2items = defaultdict(list)

        # Surprise용으로 데이터를 가공한다.
        reader = Reader(rating_scale=(0.5, 5))
        data_train = SurpriseDataset.load_from_df(
            dataset.train[["user_id", "movie_id", "rating"]], reader
        ).build_full_trainset()

            sim_options = {
                "name": "pearson", # 유사도 계산 방법을 지정한다.
                "user_based": True # False로 하면 아이템 기반이 된다.
            }

            # 유사도 상위 30명의 사용자를 유사 사용자로 다룬다.
            knn = KNNWithMeans(k=30, min_k=1, sim_options=sim_options)
            knn.fit(data_train)

            # 학습용 데이터에 대해 평갓값이 없는 사용자와 아이템의 조합에 대한
            # 평갓값을 예측한다.
            data_test = data_train.build_anti_testset(None)
            predictions = knn.test(data_test)

            def get_top_n(predictions, n=10):
                # 각 사용자별로 예측된 아이템을 저장한다.
                top_n = defualtdict(list)
                for uid, iid, true_r, est, _ in predictions:
                    top_n[uid].append((iid, est))
                
                # 사용자별로 아이템을 예측 평갓값 순으로 나열하고 상위 n개를 저장한다.
                for uid, user_ratings in top_n.items():
                    user_ratings.sort(key=lambda x: x[1], reverse=True)
                    top_n[uid] = [d[0] for d in user_ratings[:n]]
                  
                return top_n

            pred_user2items = get_top_n(predictions, n=10)

            average_score = dataset.train.rating.mean()
            pred_results = []
            for _, row in dataset.test.iterrows():
                user_id = row["user_id"]
                movie_id = row["movie_id"]
                # 학습 데이터에 존재하지 않고 테스트 데이터에만 존재하는 사용자나
                # 영화에 관한 예측 평갓값은 전체 평균 평갓값으로 한다.
                if user_id not in user_id2index or \
                  movie_id not in movie_id2index:
                     pred_socre = knn.predict(uid=user_id, iid=movie_id).est
                     pred_results.append(pred_score)
                movie_rating_predict["rating_pred"] = pred_results

                return RecommendResult(movie_rating_predict.rating_pred,
                pred_user2items)
```

<br><br>

<h1>회귀 모델</h1>
<ul>
  <li>
    예측 대상의 평갓값을 랜덤 포레스트(random forest) 모델을 통한 회귀 분석을 수행한다.
  </li>
    <ul>
      <li>
        회귀 분석이기에 다른 방법의 알고리즘을 사용할 수도 있다.
      </li>
    </ul>
</ul>

```python
# 1. 회귀 모델 적용

# 모델 import
import sklearn.ensemble import RandomForestRegressor as RFR

# 학습에 사용하는 학습용 데이터 중 평갓값
train_y = dataset.train.rating.values

# 평갓값을 예측할 테스트용 데이터 중 사용자와 영화의 조합
test_keys = dataset.test[["user_id", "movie_id"]]

# 순위 형식의 추천 리스트 작성을 위한 학습용 데이터에 존재하는 모든 사용자와
# 모든 영화의 조합
train_all_keys = \
user_movie_matrix.stack(dropna=False).reset_index()[["user_id", "movie_id"]]

# 특징량(feature) 작성.
#   - 사용자별 평갓값의 최솟값, 최댓값, 평균값 및 영화별 최솟값, 최댓값을 특징량 추가.
#   - 학습 데이터에는 테스트 데이터에만 있는 특징량이 없기에 평균 평갓값으로 대체
train_x = train_keys.copy()
test_x = test_keys.copy()
train_all_x = train_all_keys.copy()

# 학습용 데이터에 존재하는 사용자별 평갓값의 최솟값, 최댓값, 평균값 및
# 영화별 평갓값의 최솟값, 최댓값, 평균값을 특징으로 추가한다.
aggregators = ["min", "max", "mean"]
user_features = dataset.train.groupby("movie_id").rating.agg(
                aggregators).to_dict()
movie_features = dataset.train.groupby("movie_id").rating.agg(
                 aggregators).to_dict()
for agg in aggregators:
    train_x[f"u_{agg}"] = train_x["user_id"].map(user_features[agg])
    test_x[f"u_{agg}"] = test_x["user_id"].map(user_features[agg])
    train_all_x[f"u_{agg}"] = train_all_x["user_id"].map(user_features[agg])
    train_x[f"m_{agg}"] = train_x["movie_id"].map(movie_features[agg])
    test_x[f"m_{agg}"] = test_x["movie_id"].map(movie_features[agg])
    train_all_x[f"m_{agg}"] = train_all_x["movie_id"].map(movie_features[agg])

    # 테스트용 데이터네만 존재하는 사용자와 영화의 특징량을 
    # 학습용 데이터 전체의 평균 평갓값으로 채운다.
    average_rating = train_y.mean()
    test_x.fillna(average_rating, inplace=True)

# 영화가 특정 장르에 속하는지 boolean 특징량 추가.
movie_genres = dataset.item_content[["movie_id", "genre"]]
genres = set(itertools.chain(*movie_genre.genre))
for genre in genres:
    movie_genres[f"is_{genre}"] = movie_genres.genre.apply(lambda x:
                                  genre in x)
movie_genres.drop("genre", axis=1, inplace=True)
train_x = train_x.merge(movie_genre, on="movie_id")
test_x = test_x.merge(movie_genres, on="movie_id")
train_all_x = train_all_x.merge(movie_genres, on="movie_id")

# 특징량을 사훃아지 않는 정보 삭제
train_x = train_x.drop(columns=["user_id", "movie_id"])
test_x = test_x.drop(columns=["user_id", "movie_id"])
train_all_x = train_all_x.drop(columns["user_id", "movie_id"])

# Random Forest를 사용한 학습
reg = RFR(n_jobs=-1, random_state=0)
reg.fit(train_x.values, train_y)

# 테스트용 데이터 안의 사용자의 영화의 조합에 대해 평갓값을 예측한다.
test_pred = reg.predict(test_x.values)

movie_rating_predict = test_keys.copy()
movie_rating_predict["rating_pred"] = test_pred

# 학습용 데이터에 존재하는 모든 사용자와 모든 영화의 조합에 대해
# 평갓값을 예측한다.
train_all_pred = reg.predict(train_all_x.value)

pred_train_all = train_all_keys.copy()
pred_train_all["rating_pred"] = train_all_pred
pred_matrix = pred_train_all.pivot(index="user_Id", columns="movie_id",
              values="rating_pred")

# 사용자가 학습용 데이터 안에서 평가하지 않은 영화 중
# 예측 평갓값이 높은 순으로 10편의 영화를 순위 형식의 추천 리스트로 만든다.
pred_user2items = defaultdict(list)
user_evlauated_movies = dataset.train.groupby("user_id").agg({
                        "movie_id": list})["movie_id"].to_dict()
for user_id in dataset.train.user_id.unique():
    movie_indexes = np.argsort(-pred_matrix.loc[user_id, :]).values
    for movie_index in movie_indexes:
        movie_id = user_movie_matrix.columns[movie_index]
        if moive_id not in user_evaluated_movies[user_id]:
            pred_user2items[user_id].ap pend(movie_id)
        if len(pred_user2items[user_id]) == 10:
            break

# - 무작위 추천보다 RMSE 결과는 좋다.
# - Precision@K와 Recall@K는 비슷하다.
#   → 평가 수는 적지만 평가가 높은 아이템에 의해 추천 정확도가 떨어짐
#   → 평가 수가 일정한 임곗값을 설정하여 편향을 보정할 수 있다.
# - 실제 추천에서는 다양한 특징량(feature)을 고려한다.
```

<br><br>

<h1>8. 행렬 분해</h1>
<ul>
  <li>
    <strong>모델 기반형 협조 필터링</strong> 방법인 <strong>행렬 분해</strong>에 대해 다룬다.
  </li>
    <ul>
      <li>
        메모리 기반 협조 필터링보다 복잡하지만 추천 성능은 일반적으로 좋다 알려져 있다.
      </li>
    </ul>
  <li>
    추천 시스템에서 행렬 분해는 넓은 의미에서 평갓값 행렬을 저차원의 <strong>사용자 인자 행렬</strong>과 <strong>아이템 인자 행렬</strong>로 분해하는 것을 의미한다.
  </li>
    <ul>
      <li>
        사용자와 아이템을 100 차원 정도의 <strong>저차원 벡터로 표현</strong>하고 <strong>벡터의 내적값을 사용자와 아이템의 상성</strong>으로 한다.
      </li>
    </ul>
  <li>
    SVD, NMF, MF, IMF, BPR 그리고 FM 행렬 분해 방법을 순서대로 설명한다.
  </li>
  <li>
    행렬 분해를 실무에서 사용할 때에는 <strong>'결손값 취급'</strong>과 <strong>'평갓값이 명시적인가 암묵적인가'</strong>라는 관점이 매우 중요하다.
  </li>
  <li>
    <strong>차원을 축소</strong>한 뒤 각 영화와의 상성은 <strong>벡터의 내적</strong>으로 계산한다.
  </li>
</ul>

```python
# 1. 평갓값이 명시적인 경우의 행렬 분해
user_movie_matrix = movielens.train.pivot(index="user_id",
                    columns="movie_id", values="ratings")
user_movie_matrix

# 모든 사용자가 모든 아이템을 평가하지 않기에
# 기본적으로 사용자 X 모든 아이템 행렬은 희소 행렬이다.
user_num = len(user_movie_matrix.index)
item_num = len(user_moive_matrix.columns)
non_null_num = user_num * itemnum_num - user_movie_matrix.isnull().sum().sum()
non_null_ratio = non_null_num / (user_num * item_num)

print(f"사용자 수={user_num}, 아이템 수={user_num},
      밀도={non_null_ratio:.2f}")
  
# 사용자 수: 1000 / 아이템 수: 6674 / 밀도: 0.03
```

<br>

<h2>8-2. 특잇값 분해</h2>

$$ R=PSQ^T=\hat P \hat Q^T $$

<p>
행렬 분해는 “거대한 <strong>평점 행렬 𝑅</strong>을 저차원 공간의 <strong>사용자 행렬 𝑃</strong>와 <strong>아이템 행렬 Q</strong>로 나눠, <strong>내적을 통해 평점을 근사</strong>하는 과정”이다.

각 사용자 행렬 𝑃, S 그리고 아이템 행렬 Q는 다음 목적 함수를 최소화 해서 얻을 수 있다.

하단의 목적 함수는 실제 평점 행렬 𝑅 을 <strong>두 잠재 행렬의 곱으로 근사</strong>하면서 오차(Frobenius norm)를 최소화하는 최적화 문제다.
</p>

$$ ||R-PSQ^T||^2_{Fro}=||R- \hat P \hat Q^T||^2_{Fro}$$

<p>
위 식을 Frobenius norm(오차)으로 정의하여 최적화 할 수 있다.
</p>

$$ ||A||^2_{Fro}=\sum_{i, j}A^2_{i,j} $$

```python
# 1. 견손값에는 0 또는 평균값을 대입하고, 
# 특잇값 분해(Singular Value Decomposition, SVD)로 차원 축소
user_movie_matrix.fillna(0)

# scipy 라이브러리를 사용한 특잇값 분해
import scipy
import numpy as np

# 평갓값을 사용자 X 영화 행렬로 변환한다. 결손값은 평균으로 채운다.
user_movie_matrix = movielens.train.pivot(index="user_id",
                    columns="movie_id", values="rating")
user_id2index = dict(zip(user_movie_matrix.index,
                range(len(user_movie_matrix.index))))
movie_id2index = dict(zip(user_movie_matrix.columns,
                 range(len(user_movie_matrix.columns))))
matrix = user_movie_matrix.fillna(
         movielens.train.rating.mean()).to_numpy()

# 인자 수 k로 특잇값 분해
P, S, Qt = scipy.sparse.linalg.svds(matrix, k=5)

# 예측 평갓값 행렬
pred_matrix = np.dot(np.dot(P, np.diag(S)), Qt)

print(f"P: {P.shape}, S: {S.shape}, Qt: {Qt.shape}, pred_matrix:
      {pred_matrix.shape}")

# P: (1000, 5) / S: (5,)  Qt: (5, 6673), pred_matrix: (1000, 6673)

# 위의 결과를 활용해 추천 수행.
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import scipy
import numpy as np
np.random.seed(0)

class SVDRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 결손값을 채우는 방법
        fillna_with_zero = kwargs.get("fillna_with_zero", True)
            factors = kwargs.get("factors", 5)
        # 평갓값을 사용자 X 아이템 행렬로 변환, 평갓값 또는 0으로 채운다.
        user_movie_matrix = dataset.train.pivot(index=="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))
        if fillna_with_zero:
            matrix = user_movie_matrix.fillna(0).to_numpy()
        else:
            matrix = user_movie_matrix. \
            fillna(dataset.train.rating.mean()).to_numpy()
        
        # 인자 수 k로 특잇값 분해를 수행
        P, S, Qt = scipy.sparse.linalg.svds(matrix, k=factors)
        
        # 예측 평갓값 행렬
        pred_matrix = np.dot(np.dot(P, np.diag(S)), Qt)
        
        # 학습용에 나오지 않는 사용자나 영화의 예측 평갓값은 평균값으로 한다.
        average_score = dataset.train.rating.mean()
        movie_rating_predict = dataset.test.copy()
        pred_result = []
        for i, row in dataset.test.iterrows():
            user_id = row["user_id"]
            if user_id not in user_id2index or \
                row["movie_id"] not in movie_id2index:
                pred_results.append(average_score)
                continue
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            pred_score = pred_matrix[user_index, movie_index]
            pred_result.append(pred_score)
        movie_rating_predict["rating_pred"] = pred_results

        # 각 사용자에 대한 추천 영화는 해당 사용자가 아직 평가하지 않은 영화 중에서
        # 예측값이 높은 순으로 한다.
        pred_user2items = defaultdict(list)
        user_evaluated_movies = dataset.train.groupby("user_id").agg({
                                    "movie_id": list})["movie_id"].to_dict()
        for user_id in dataset.train.user_id.unique():
            if user_id not in user_id2index:
                continue
            user_index = user_id2index[row["user_id"]]
            movie_indexes = np.argsort(-pred_matrix[user_index, :])
            for movie_index in movie_indexes:
                movie_id = user_movie_matrix.columns[movie_index]
                if movie_id not in user_evaluated_movies[user_id]:
                    pred_user2items[user_id].append(movie_id)
                if len(pred_user2items[user_id]) == 10:
                    break
        return RecommenResult(movie_rating_predict.rating_pred,
        pred_user2items)

if __name__ == "__main__":
    SVDRecommender().run_sample()

# RMSE=3.335, Precision@K=0.009, Recall@K=0.027

# - 0으로 결손값을 채우면 모두 가장 낮은 평가를 한 것이기 예측 성능이 나빠진다.
# - 대신 평균값으로 결측치를 채우는 방법을 사용할 수도 있다.
# - 잠재 인자 수를 튜닝할 수도 있다.
#   - 잠재 인자 수가 클수록 원래 행렬을 복구할 때 유리해 성능은 좋으나 overfitting 문제가 
#     생길 수 있다.
# - 일반적으로 행렬 전체를 사용할 경우는 scipy.sparse를 통해 희소 행렬 형식으로 0외의 값을 
#   갖는 셀 정보만 유지하는 경우가 많다.
```

<br>

<h2>8-3. 비음수 행렬 분해</h2>
<ul>
  <li>
    SVD는 행렬을 직교 벡터의 선형 결합으로 분해하기에 벡터 방향성(부호)을 고려하며, 그 결과 <strong>음수 값</strong>이 나올 수 있다.
  </li>
  <li>
    반면, <strong>NMF(Non-negative Matrix Factorization)</strong>는 모든 구성 요소를 <strong>비음수로 제한</strong>하여 분해하기에 각 행렬의 성분이 <strong>해석 가능한 비음수값(가중치)</strong>로 표현된다.
  </li>
    <ul>
      <li>
        최적화 과정에서 해를 찾을 때 <strong>음수해가 채택되는 것을 방지</strong>한다.
      </li>
    </ul>
  <li>
    단, 마찬가지로 <strong>결손값을 0</strong>으로 채우기에 추천 성능이 좋지는 못하다.
  </li>
  <li>
    최적화 식은 SVD와 동일하며 <strong>음수 제약</strong>만 걸린다는 차이가 있다.
  </li>
</ul>

$$
\min_{W, H} \; ||R - W H||_F^2 
\quad \text{subject to} \quad W \ge 0, \; H \ge 0
$$

```python
from util.models import RecommendResult, Dataset
from src.base_recommender import BaseRecommender
from collections import defaultdict
import numpy as np
from sklearn.decomposition import NMF
np.random.seed(0)

class NMFRecommender(BaseRecommender):
    def recommend(self, dataset: Dataset, **kwargs) -> RecommendResult:
        # 결손값을 채우는 방법
        fillna_with_zero = kwargs.get("fillna_with_zero", True)
        factors = kwargs.get("factors", 5)
        # 평갓값을 사용자 X 영화의 행렬로 변환한다. 결손값은 평균값 또는 0으로 차운다.
        user_movie_matrix = dataset.train.pivot(index="user_id",
                            columns="movie_id", values="rating")
        user_id2index = dict(zip(user_movie_matrix.index,
                        range(len(user_movie_matrix.index))))
        movie_id2index = dict(zip(user_movie_matrix.columns,
                         range(len(user_movie_matrix.columns))))
        if fillna_with_zero:
            matrix = user_movie_matrix.fillna(0).to_numpy()
        else:
            matrix = user_movie_matrix. \
                fillna(dataset.train.rating.mean().to_numpy())
        nmf = NMF(n_components=factors)
        nmf.fit(matrix)
        P = nmf.fit_transform(matrix)
        Q = nmf.components_
        # 예측 평갓값 행렬
        average_scroe = dataset.train.rating.mean()
        movie_rating_predict = dataset.test.copy()
        pred_result = []
        for i, row in dataset.test.iterrows():
            user_id = row["user_id"]
            if user_id not in user_id2index or \
            row["movie_id"] not in movie_iddex:
                pred_results.append(average_score)
                continue
            user_index = user_id2index[row["user_id"]]
            movie_index = movie_id2index[row["movie_id"]]
            pred_score = pred_matrix[user_index, movie_index]
            pred_results.append(pred_score)
        movie_rating_predict["rating_pred"] = pred_results
        # 각 사용자에 대한 추천 영화는 그 사용자가 아직 평가하지 않은 영화 중에서 
        # 예측값이 높은 순으로 한다.
        pred_user2items = defaultdict(list)
        user_evaluated_movies = dataset.train.groupby("user_id").agg([
                                "movie_id": list])["movie_id"].to_dict()
        for user_id in dataset.train.user_in.unique():
            if user_id not in user_id2index:
                continue
            user_index = user_id2index[row["user_id"]]
            movie_indexes = np.argsort(-pred_matrix[user_index, :])
            for movie_index in movie_indexes:
                movie_id = user_movie_matrix.columns[movie_index]
                if movie_id not in user_evaluated_movies[user_id]:
                    pred_user2items[user_id].append(movie_id)
                if len(pred_user2item[user_id]) == 10:
                    break
        return RecommendResult(movie_rating_predict.rating_pred, pred_user2items)

if __name__ == "__main__":
    NMFRecommender().run_sample()
```

<br>

<h2>8-4. 명시적인 평갓값에 대한 행렬 분해</h2>
