<ul>
  <li>
    Airflow 파이프라인 구성 설정
  </li>
  <li>
    중간 출력 데이터의 구조화
  </li>
  <li>
    멱등성 기반의 태스크 개발
  </li>
  <li>
    여러 유사한 변환을 처리하기 위해 하나의 오퍼레이터 구현
  </li>
</ul>

<br>

<h1>1. 데이터에 대한 이해</h1>
<h2>1-1. Yellow Cab 파일 공유</h2>
<ul>
  <li>
    http://localhost:8081에서 Yellow Cab 데이터를 확인할 수 있다.
  </li>
</ul>

<br>

<h2>1-2. Citi Bike REST API</h2>
<ul>
  <li>
    http://localhost:8082에서 Citi Bike 데이터를 확인할 수 있다.
  </li>
</ul>

<br>

<h2>1-3. 접근 계획 결정</h2>
<ul>
  <li>
    어떻게 데이터를 분석할 것인지에 대해 다룬다.
  </li>
</ul>

<br><br>

<h1>2. 데이터 추출</h1>

```python
# 1. 15분 마다 실행되는 DAG
#     - Yellow Cab 데이터는 115분 간격으로 제공된다.
import airflow.utils.dates
from airflow.models import DAG

dag=DAG(
    dag_id="nyc_dag",
    # 15분 마다 수행.
    schedule_interval="*/15 * * * *",
    start_date=airflow.utils.dates.days_ago(1),
    catchup=False
)
```

<br>

<h2>2-1. Citi Bike 데이터 다운로드하기</h2>

```python
# 1. Citi Bike REST API에서 MiniO로 데이터 다운로드.
#     - SimpleHttpOperator로 HTTP 응답을 받을 수는 있지만 저장은 불가하기에 
#       PythonOperator 사용.
import json
import requests
# Airflow Connection 정보를 가져올 때 사용
from airflow.hooks.base import BaseHook
# DAG 정의용 클래스
from airflow.models import DAG
# Python 함수를 Task로 실행할 때 사용
from airflow.operators.python import PythonOperator
# S3 또는 MinIO 업로드용 Hook
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
# API Basic 인증용
from requests.auth import HTTPBasicAuth

# ---- 실제로 API를 호출하고 S3(MinIO)에 업로드하는 함수 ----
def _download_citi_bike_data(ts_nodash, **_):
    # Airflow Connection에서 'citibike' 연결 정보 불러오기
    # (Admin → Connections에서 citibike 설정 필요)
    citibile_conn=BaseHook.get_connection(conn_id="citibike")

    # Connection 정보로 API URL 구성
    # 예: http://api.citibikenyc.com:80/recent/minute/15
    url=f"http://{citibike_conn.host}:{citibike_conn.port}/recent/minute/15"
    
    # REST API 요청 (필요 시 BasicAuth 사용)
    response = requests.get(
        url, auth=HTTPBasicAuth(citibike_conn.login, citibike_conn.password)
    )

    # 응답(JSON) 데이터를 Python dict로 변환
    data=response.json()

    # S3Hook 생성 (Airflow에 미리 설정된 's3' connection 사용)
    # MinIO도 S3 API 호환이므로 동일 Hook으로 업로드 가능
    s3_hook = S3Hook(aws_conn_id="s3")

    # JSON 데이터를 문자열로 직렬화 후 MinIO 버킷에 업로드
    # key 경로는 Airflow의 실행 시각(ts_nodash)을 포함하여 버전 관리
    s3_hook.load_string(
        # 업로드할 데이터 (문자열)
        string_data=json.dumps(data),
        # S3/MinIO 내 경로
        key=f"raw/citibike/{ts_nodash}.json",
        # 업로드할 버킷명
        bucket_name="datalake"
    )

download_citi_bike_data=PythonOperator(
    task_id="download_citi_bike_data",
    python_callable=_download_citi_bike_data,
    dag=dag
)
```

<br>

<h2>2-2. Yellow Cab 데이터 다운로드</h2>
<ul>
  <li>
    Citi Bike 데이터는 CSV 이지만 Yellow Cab 데이터는 JSON이라는 차이가 있다.
  </li>
</ul>

```python
# 1. Yellow Cab 파일 공유에서 MinIO 스토리지로 데이터 다운로드.
#    - 파일 목록을 먼저 요청하여 받은 뒤, 각 파일을 순회하며 다운로드 및 업로드 수행
def _download_taxi_data():
    # Airflow에서 미리 등록된 'taxi' Connection 불러오기
    # (ex. host = taxi-data-server.local, port는 필요 시 추가)
    taxi_conn=BaseHook.get_connection(conn_id="taxi")
    # MinIO(S3 호환 스토리지)와 연결할 S3Hook 생성
    s3_hook=S3Hook(aws_conn_id="s3")

    # Yellow Cab 파일 서버의 기본 URL 생성
    # ex) http://taxi-data-server.local
    url=f"http://{taxi_conn.host}"

    # 1️⃣ 파일 목록 요청 (예: JSON 형태로 파일 이름 리스트 반환)
    response=requests.get(url)
    files=response.json()
    
    for filename in [f["name"] for in files]:
        reponse=requests.get(f"{url}/{filename}")
        s3_key=f"raw/taxi/{filename}"
        try:
            # 파일 내용을 MinIO 버킷('datalake')에 업로드
            # load_string()은 문자열 데이터를 바로 업로드할 때 사용
            s3_hook.load_string(
                # 파일 내용 (문자열 형태)
                string_data=reponse.text,
                # S3 내 파일 경로
                key=s3_key,
                # 업로드할 버킷 이름
                bucket_name="datalake",
            )
        print(f"Uploaded {s3_key} to MinIO.")

    # 멱등성(idempotency) 보장을 위해 이미 존재하는 파일일 경우 ValueError 예외를 처리
    except ValueError:
        print(f"File {s3_key} already exists.")

# Airflow의 PythonOperator로 Task 등록
# DAG 실행 시 _download_taxi_data 함수가 실행됨
download_taxi_data=PythonOperator(
    task_id="download_taxi_data",
    python_callable=_download_taxi_data,
    dag=dag,
)
```

<br><br>

<h1>3. 데이터에 유사한 변환 적용</h1>

```python
# 1. 모든 Pandas DataFrame 태스크에 대한 단일 오퍼레이터.
import logging

# 모든 Airflow Operator의 기본 클래스
from airflow.models import BaseOperator
 # __init__ 메서드에서 기본값 설정 시 사용하는 데코레이터
from airflow.utils.decorators import apply_defaults

class PandasOperator(BaseOperator):
    # Airflow 템플릿 렌더링 대상 필드 지정 (Jinja 템플릿 등을 사용할 때 렌더링 가능한 필
    # 드)
    template_field=(
        "_input_callable_kwargs",
        "_transfrom_callable_kwargs",
        "_output_callable_kwargs",
    )

    @apply_defaults
    def __init__(
        self,
        # 데이터를 읽어오는 함수 (예: CSV 읽기, DB 조회 등)
        input_callable,
        # 데이터를 저장하는 함수 (예: S3 업로드, DB 적재 등)
        ouptut_callable,
        # 중간 변환 함수 (선택적)
        transform_callable=None,
        # 입력 함수에 전달할 인자(dict)
        input_callable_kwargs=None,
        # 변환 함수에 전달할 인자(dict)
        transform_callable_kwargs=None,
        # 출력 함수에 전달할 인자(dict)
        output_callable_kwargs=None,
        # BaseOperator의 기본 인자들 (task_id, dag 등)
        **kwargs
    ):
        super().__init__(**kwargs)

        # 입력 단계: DataFrame을 읽기 위한 함수와 인자 저장
        self._input_callable=input_callable
        self._input_callable_kwargs=input_callable_kwargs or {}

        # 변환 단계: 변환 함수 및 관련 인자 저장 (옵션)
        self._transform_callable=transform_callable
        self._transform_callable_kwargs=transform_callable_kwargs or {}

        # 출력 단계: 결과 DataFrame을 저장하는 함수 및 인자 저장
        self._output_callable=output_callable
        self._output_callable_kwargs=output_callable_kwargs or {}

    # DAG 실행 시 Airflow가 호출하는 핵심 메서드
    def execute(self, context):
        # 1️⃣ 입력 함수 실행 → DataFrame 읽기
        df = self._input_callable(**self._input_callable_kwargs)
        logging.info("Read DataFrame with shape: %s.", df.shape)

        # 2️⃣ 변환 함수가 정의되어 있으면 DataFrame 변환 수행
        if self._transform_callable:
            df = self._transform_callable(
                df,
                **self._transform_callable_kwargs
            )
            logging.info("DataFrame shape after transform: %s.", df.shape)

        # 3️⃣ 변환된 DataFrame을 출력 함수로 전달 (저장 단계)
        self._output_callable(df, **self._output_callable_kwargs)
```

```python
# 2. 앞서 작성한 코드의 PandasOperator 적용.
process_taxi_data=PandasOperator(
    task_id="process_taxi_data",

    # MinIO에서 원본 데이터 읽기 함수
    input_callable=get_minio_object,
    # 입력 함수에 전달할 인자들
    input_callable_kwrags={
        # CSV 파일 읽기 함수 지정
        "pandas_read_callable": pd.read_csv,
        # 읽을 버킷 이름
        "bucket": "datalake",
        # 이전 태스크의 출력 경로 사용
        "paths": "{{ ti.xcom_pull(task_ids='download_taxi_data') }}",
    },

    # DataFrame 변환(전처리) 함수
    transform_callable=transform_taxi_data,

    # 변환된 데이터를 저장할 함수
    output_callable=write_minio_object,
    # 출력 함수 인자들
    output_callable_kwargs={
        # 저장할 버킷 이름
        "bucket": "datalake",
        # 저장 경로 (실행 시각 포함)
        "path": "processed/taxi/{{ ts_nodash }}.parquet",
        # Parquet 형식으로 저장
        "pandas_write_callable": pd.DataFrame.to_parquet,
        # 저장 시 엔진 자동 선택
        "pandas_write_callable_kwargs": {"engine": "auto"},
    },
    dag=dag
)
```

```python
# 3. MinIO 객체를 읽고 Pandas DataFrames를 반환하는 예제 함수.
def get_minio_object(
    # Pandas의 읽기 함수 
    pandas_read_callable,
    # MinIO 버킷 이름
    bucket,
    # 읽을 파일 경로(문자열 또는 리스트)
    paths,
    # 읽기 함수에 전달할 추가 인자
    pandas_read_callable_kwargs=None,
):
    # Airflow의 's3' Connection 불러오기
    s3_conn=BaseHook.get_connection(conn_id="s3")
    # MinIO 클라이언트 생성
    minio_client = Minio(
        s3_conn.extra_dejson["host"].split("://")[1],
        access_key=s3_conn.extra_dejson["aws_access_key_id"],
        secret_key=s3_conn.extra_dejson["aws_secret_access_key"],
        secure=False,
    )

    # 경로가 문자열이면 리스트로 변환
    if isinstance(paths, str):
        paths=[paths]
        if pandas_read_callable_kwargs is None:
            pandas_read_callable_kwargs={}

    # DataFrame 저장 리스트
    dfs=[]
    for path in paths:
        # MinIO에서 객체 가져오기
        minio_object=minio_client.get_object(
            bucket_name=bucket,
            object_name=path,
        )
        # Pandas로 객체 읽기
        df=pandas_read_callable(
            minio_object,
            **pandas_read_callable_kwargs,
        )
        # 리스트에 추가
        dfs.append(df)
    # 모든 DataFrame 병합 후 반환
    return pd.concat(dfs)
```

```python
# 4. 택시 데이터를 변환하는 예제 함수.
def transform_taxi_data(df):
    # 문자열 형태의 날짜 컬럼을 datetime 형식으로 변환
    df[["pickup_datetime", "dropoff_datetime"]] = df[
        ["pickup_datetime", "dropoff_datetime"]
    ].apply(pd.to_dataframe)

    # 이동 시간(초 단위) 계산: dropoff - pickup
    ☞ df["tripduration"]=(df["dropoff_datetime"] - \
          df["pickup_datetime"]).dt.total_seconds().astype(int)

    # 컬럼 이름 변경 및 불필요한 컬럼 제거
    df=df.rename(
        columns={
            # 승차 시각
            "pickup_datetime": "starttime",
            # 출발지 ID
            "pickup_locationid": "start_location_id",
            # 하차 시각
            "dropoff_datetime": "stoptime",
            # 도착지 ID
            "dropoff_locationid": "end_location_id",
        }
    # 이동 거리 컬럼 제거
    ).drop(columns=["trip_distance"])
    # 변환된 DataFrame 반환
    return df
```

```python
# 4. 변환된 DataFrame을 MinIO 스토리지에 다시 쓰는 함수 예.
def write_minio_object(
    # 저장할 Pandas DataFrame
    df,
    # DataFrame 저장 함수 (예: to_parquet, to_csv 등)
    pandas_write_callable,
    # MinIO 버킷 이름
    bucket,
    # 저장 경로 (S3 object key)
    path,
    # 저장 함수에 전달할 인자(dict)
    pandas_write_callable_kwargs=None
):

    # Airflow의 's3' Connection 불러오기
    s3_conn=BashHook.get_connection(conn_id="s3")
    # MinIO 클라이언트 생성
    minio_client=Minio(
        s3_conn.extra_dejson["host"].split("://")[1],
        access_key=s3_conn.extra_dejson["aws_access_key_id"],
        secret_key=s3_conn.extra_dejson["aws_secret_access_key"],
        secure=False,
    )

    # 데이터를 임시로 담을 메모리 버퍼 생성
    bytes_buffer=io.BytesIO()
    # DataFrame의 저장 메서드 가져오기
    pandas_write_method=getattr(df, pandas_write_callable.__name__)
    # DataFrame을 버퍼에 쓰기
    pandas_write_method(bytes_buffer, **pandas_write_callable_kwargs)

    # 버퍼 크기(바이트 수) 계산
    nbytes=bytes_buffer.tell()
    # 버퍼 포인터를 처음으로 이동 (업로드 준비)
    bytes_buffer.seek(0)

    # MinIO 버킷에 객체 업로드
    minio_client.put_object(
        # 버킷 이름
        bucket_name=bucket,
        # 저장 경로
        object_name=path,
        # 업로드할 데이터 크기
        length=nbytes,
        # 실제 데이터
        data=bytes_buffer,
    )
```