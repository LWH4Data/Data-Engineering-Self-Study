<ul>
  <li>
    스타일 컨벤션을 사용하여 명확하고 이해하기 쉬운 DAG 작성 방법.
  </li>
  <li>
    인증 정보 관리를 위한 일관된 접근방식과 구성 방법의 사용.
  </li>
  <li>
    Factory 함수를 이용한 DAG와 태스크의 반복 생성.
  </li>
  <li>
    멱등성과 결정론적 관점에서 재현 가능한 태스크 설계.
  </li>
  <li>
    DAG에서 처리되는 데이터의 양을 제한하여 데이터를 효율적으로 관리.
  </li>
  <li>
    중간 단계 데이터 셋의 처리 및 저장을 위한 효율적인 접근 방법.
  </li>
  <li>
    리소스 풀을 이용한 동시성 관리.
  </li>
</ul>

<br>

<h1>1. 깔끔한 DAG 작성</h1>
<h2>1-1. 스타일 가이드 사용</h2>
<h3>1-1-1. 스타일 가이드 따르기</h3>
<ul>
  <li>
    깔끔하고 이해하기 쉬운 코드를 작성하는 가장 쉬운 방법은 <strong>일반적인 코딩 스타일</strong>로 코드를 작성하는 것이다.
  </li>
</ul>

```python
# 1. 코드 스타일에 따른 코드 차이.

# PEP8을 지키지 않은 코드
spam( ham[ 1 ], { eggs: 2 } )

i=i+1
submitted += 1

my_list=[
    1, 2, 3,
    4, 5, 6,
    ]

# PEP8을 준수한 예.
#   - 불필요한 공백 제거
#   - 들여쓰기 정정
spam(ham[1], {eggs: 2})

i=i + 1
submitted += 1

my_list=[
    1, 2, 3,
    4, 5, 6
]
```

<h3>1-1-2. 정적 검사기를 이용한 코드 품질 확인</h3>
<ul>
  <li>
    스타일 가이드 외에도 파이썬 커뮤니티에서는 코드의 적절한 코딩 규칙 및 스타일가이드 준수 여부를 확인할 수 있는 많은 <strong>소프트웨어 도구</strong>를 제공한다.
  </li>
</ul>

```bash
# 1. Flake8 설치 및 실행.
pip install flake8
flake8 dags/*.py
```

```bash
# 2. Flake8 출력 예.
python -m flake8 chapter08/dags/
```

<h3>1-1-3. 코드 포매터를 사용하여 공통 포맷 적용</h3>
<ul>
  <li>
    <strong>코드 포매터</strong>를 사용하면 개발자가 코드를 작성한 이후, 코드 포매터로 다시 정해진 규칙에 따라 작성된 코드에 대하여 <strong>일괄적으로 수정</strong>한다.
  </li>
</ul>

```python
# 1. 코드 포매터 적용 전과 후 비교
# Black 적용 전=====================================================
def my_function(
    arg1, arg2,
    arg3):
    """Function to demonstrate black."""
    str_a='abc'
    str_b="def"
    return str_a + \
        str_b

# Black 적용 후=====================================================
def my_function(arg1, arg2, arg3):
    """Function to demonstrate black."""
    str_a="abc",
    str_b="def"
    return str_a + str_b
```

```bash
# 2. Black을 설치하고 실행.
python -m pip install black
python -m black dags/
```

<h3>1-1-4. Airflow 전용의 코드 스타일 규칙</h3>

```python
# 1. DAG 정의를 위한 두 가지 스타일.

# Context 매니저를 사용.
with DAG(...) as dag:
    task1=PythonOperator(...)
    task2=PythonOperator(...)

# Context 매니저 사용 X.
dag=DAG(...)
task1=PythonOperator(..., dag=dag)
task2=PythonOperator(..., dag=dag)
```

```python
# 2. 태스크 종속성 정의를 위한 다양한 스타일.
task1 >> task2
task1 << task2
[task1] >> task2
task1.set_downstream(task2)
task2.set_upstream(task1)
```

```python
# 3. 서로 다른 태스크 종속성 표기법 혼합.
task1 >> task2
task2 << task3
task5.set_upstream(task3)
task3.set_downstream(task4)
```

```python
# 4. 태스크 종속성을 정의하기 위해 일관된 스타일 사용.
task1 >> task2 >> task3 >> [task4, task5]
```

<br>

<h2>1-2. 중앙에서 자격 증명 관리</h2>
<ul>
  <li>
    다양한 시스템과 상호작용 해야하는 DAG의 경우 DB, 컴퓨팅 클러스터, 클라우드 스토리지 등 다양한 유형의 서비스에 대해 자격 증명을 관리해야한다.
  </li>
  <li>
    Airflow 내장 오퍼레이터에 대해서 자격 증명을 연결 저장소에 저장하는 것이 가장 쉬운 방법이지만 경우에 따라 접근성을 우선하여 다른 곳에 저장할 수도 있다.
  </li>
    <ul>
      <li>
        이러한 경우 사용자의 코드에서 저장소로부터 연결 세부 사항을 검색하고 검색된 자격 증명을 사용하여 작업할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. Airflow 메타스토어에서 자격 증명 가져오기.
#     - 이 방법은 다른 모든 Airflow 오퍼레이터와 동일한 방법으로 자격 증명을 저장한다.
from airflow.hooks.base_hook import BaseHook

def _fetch_data(conn_id, **context)
    # 주어진 ID를 사용하여 자격 증명을 가져온다.
    credentials=BaseHook.get_connection(conn_id)

fetch_data=PythonOperator(
    task_id="fetch_data",
    op_kwargs={"conn_id": "my_conn_id"},
    dag=dag
)
```

<br>

<h2>1-3. 구성 세부 정보를 일관성 있게 지정하기</h2>
<ul>
  <li>
    자격 증명 외에도 파일 경로 혹은 테이블명 등 DAG의 구성 정보로서 전달해야하는 여러 매개변수를 <strong>Airflow Variables</strong>를 사용해 메타스토어에 구성 정보를 저장할 수 있다.
  </li>
  <li>
    Airflow Variables는 Airflow 메타스토어에 (전역)변수를 저장하기 위한 Airlfow의 기본 기능이다.
  </li>
  <li>
    DAG 간 구성 정보를 공유하는 경우 <strong>DRY(don't repeat yourself)</strong> 원칙에 따 <strong>단일 위치</strong>에 구성 값을 저장하는 것이 좋다.
  </li>
    <ul>
      <li>
        DRY 원칙이란 각 객체들이 자기 자신을 반복하지 않는 것을 의미한다.
      </li>
    </ul>
  <li>
    구성 옵션이 DAG 내에 참조되는 위치에 따라 다른 콘텍스트에서 로드될 수 있다.
  </li>
</ul>

```python
# 1. YAML 파일에서 구성 옵셩 로드
import yaml

with open("config.yaml") as config_file:
    config=yaml.load(config_file)

""" 중략 """

fetch_data=PythonOperator(
    task_id="fetch_data",
    op_kwargs={
        "input_path": config["input_path"],
        "output_path": config["output_path"],
    },

    """ 중략 """

)
```

```python
# 2. Airflow Variables에 구성 옵션 저장.
from airflow.models import Variable

# Airflow의 변수 메커니즘을 사용하여 전역 변수를 호출.
#     - 스케줄러가 DAG 정의를 읽어올 때마다 DB 변수를 호출하기에 전역변수를 활용하는 것은 
#       좋은 방법은 아니다.
input_path=Variable.get("dag1_imput_path")
output_path=Variable.get("dag1_output_path")

fetch_data=PythonOperator(
    task_id="fetch_data",
    op_kwargs={
        "input_path": input_path,
        "output_path": output_path,
    },
    
    """ 중략 """

)
```

```python
# 3. DAG 정의에 구성 옵션 로드(비효율적)
import yaml

# 전역범위에서 구성 정보는 스케줄러에 로드된다.
with open("config.yaml") as config_file:
    config=yaml.load(config_file)

fetch_data=PythonOperator(...)

# - confing.yaml 파일이 Airflow 웹 서버나 스케줄러를 실행하는 호스트의 로컬 파일 시스템에
#   서 로드된다.
#   → 두 호스트 모두 구성 파일(config.yaml)에 대한 접근 권한이 있어야 함을 의미한다.
```

```python
# 4. 태스크 내에서 구성 옵션 로드(보다 효율적)
import yaml

def _fetch_data(config_path, **context):
    with open(config_path) as config_file:
        # 태스크 영역에서 구성 내역(config)이 워커에 로드된다.
        config=yaml.load(config_file)
    
    """ 중략 """

fetch_data=PythonOperator(
    op_kwargs={"config_path": "config.yaml"},
    
    """ 중략 """
)

# - 구성 정보는 Airflow 워커의 콘텍스트에서 호출된다.
# - 완전히 다른 환경에서 작동할 경우 잘못된 결과가 발생하거나 태스크가 실패할 수 있다.
```

<br>

<h2>DAG 구성 시 연산 부분 배제</h2>
<ul>
  <li>
    Airflow DAG는 <strong>파이썬</strong>으로 작성되기에 유연하지만 Airflow가 DAG의 동작 정보를 알기 위해서는 <strong>파이썬 DAG 파일을 실행</strong>해야 한다는 단점이 있다.
  </li>
    <ul>
      <li>
        또한 DAG에 대한 변경 사항을 적용하려면 Airflow가 <strong>정기적으로 DAG 파일을 다시 읽고</strong> 변경 사항을 내부 상태에 <strong>동기화</strong> 해야한다.
      </li>
      <li>
        DAG 파일을 <strong>로드</strong>하는 데 <strong>오랜 시간</strong>이 걸리는 경우 문제가 발생할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. DAG 정의에서 계산 수행(비효율적)
""" 중략 """

task1=PythonOperator(...)
# 긴 시간 동안 진행되는 계산이 DAG 구문 분석 시점마다 수행.
my_value=do_some_long_computation()
task2=PythonOperator(op_kwargs={"my_value": my_value})

""" 중략 """

# - Airflow DAG 파일이 로드될 때마다 do_some_computation을 실행하여 계산이 완료될 때까
#   지 전체 DAG 구문 분석 프로세스가 중단된다.
```

```python
# 2. 태스크 내에서 계산 수행(보다 효율적)

def _my_not_so_efficient_task(value, ...):
    """ 중략 """

PythonOperator(
    task_id="my_not_so_efficient_task",
    """ 중략 """
    op_kwargs={
        # 값은 DAG가 구문 분석될 때마다 계산된다.
        "value": calc_expensive_value()
    }
)

def _my_more_efficient_task(...):
    # 연산 태스크를 태스크로 옮겨놓으면 해당 태스크가 실행될 때만 값이 계산된다.
    value=calc_expensive_value()
    """ 중략 """

PythonOperator(
    task_id="my_more_efficient_task",
    python_callable=_my_more_efficient_task,
    """ 중략 """
)

# - 필요할 때만 자격 증명을 가져오는 훅 또는 오퍼레이터를 작성한다.
```

```python
# 3. DAG 정의에 대해 메타스토어에서 자격 증명 가져오기(비효율적)
from airflow.hooks.base_hook import BaseHook

# DAG가 구문 분석이 수행될 때마다 DB에 해당 내용을 확인한다.
api_config = BaseHook.get_connection("my_api_conn")
api_key=api_config.login
api_secret=api_config.password

task1=PythonOperator(
    op_kwargs={"api_key": api_key, "api_secret": api_secret},
    """ 중략 """
)
""" 중략 """
```

```python
# 4. 태스크 내에서 자격 증명 가져오기(보다 효율적)
from airflow.hooks.base_hook import BaseHook

# 태스크가 실행될 때만 DB에 해당 내용을 확인한다.
def _task1(conn_id, **context):
    api_config=BaseHook.get_connection(conn_id)
    api_key=api_config.login
    api_secret=api_config.password
    """ 중략 """

task1=PythonOperator(op_kwargs={"conn_id": "my_api_conn"})
```

<br>

<h2>1-5. factory 함수를 사용한 공통 패턴 생성</h2>
<ul>
  <li>
    <strong>하나의 DAG</strong>를 조금씩 바꿔서 여러 개의 DAG를 만들어야 할 때, 매번 비슷한 코드를 복사해서 쓰는 대신 <strong>공통된 구조</strong>를 자동으로 만들어주는 <strong>팩토리 함수(factory function)</strong>를 만들어두면 훨씬 효율적이다.
  </li>
  <li>
    하나의 DAG 파일에서 여러 DAG를 생성할 때에는 혼란스러워 질 수 있기에 지양해야한다.
  </li>
  <li>
    태스크 또는 DAG의 factory 함수는 <strong>구성 파일</strong> 또는 <strong>다른 형태의 외부 구성과 결합</strong>할 때 편리하게 사용할 수 있다.
  </li>
</ul>

```python
# 1. factory 함수용 태스크 생성 함수 정의
import os
from airflow.operators.bash import BashOperator

def generate_tasks(dataset_name, raw_dir, processed_dir, preprocess_script,
                   output_dir, dag):
    # 경로 템플릿 정의 (Airflow의 Jinja 템플릿 변수 사용)
    raw_path = os.path.join(raw_dir, dataset_name, \
    "{{ ds_nodash }}.json")
    processed_path = os.path.join(processed_dir, dataset_name, \
    "{{ ds_nodash }}.json")
    output_path = os.path.join(output_dir, dataset_name, \
    "{{ ds_nodash }}.json")

    # 1️⃣ 원본 데이터 가져오기
    fetch_task = BashOperator(
        task_id=f"fetch_{dataset_name}",
        bash_command=(
            f"echo 'curl http://example.com/{dataset_name}.json > {raw_path}'"
        ),
        dag=dag,
    )

    # 2️⃣ 전처리 수행
    preprocess_task = BashOperator(
        task_id=f"preprocess_{dataset_name}",
        bash_command=f"echo '{preprocess_script} {raw_path} {processed_path}'",
        dag=dag,
    )

    # 3️⃣ 결과 파일 내보내기
    export_task = BashOperator(
        task_id=f"export_{dataset_name}",
        bash_command=f"echo 'cp {processed_path} {output_path}'",
        dag=dag,
    )

    # 태스크 의존 관계 설정
    fetch_task >> preprocess_task >> export_task

    # 첫 번째 태스크와 마지막 태스크를 반환 (다른 DAG에서 연결할 수 있도록)
    return fetch_task, export_task
```

```python
# 2. factory 함수를 적용해 여러 데이터셋용 태스크 세트 생성
import airflow.utils.dates
from airflow import DAG

with DAG(
    dag_id="01_task_factory",
    start_date=airflow.utils.dates.days_ago(5),
    schedule_interval="@daily",
) as dag:

    # 여러 데이터셋(sales, customers)에 대해 동일한 구조의 태스크 세트를 생성
    for dataset in ["sales", "customers"]:
        # - generate_tasks() 함수를 호출하여 데이터셋별 태스크 집합(fetch 
        #   → preprocess → export) 생성
        # - 각 데이터셋마다 다른 구성값(preprocess_script 등)을 전달해 재사용성 극대화
        generate_tasks(
            dataset_name=dataset,
            raw_dir="/data/raw",
            processed_dir="/data/processed",
            output_dir="/data/output",
            preprocess_script=f"preprocess_{dataset}.py",
            # DAG 인스턴스를 전달하여 생성된 태스크들이 이 DAG에 포함되도록 함
            dag=dag,
        )
```

```python
# 3. factory 함수를 사용해 여러 개의 DAG 자체를 자동 생성
def generate_dag(dataset_name, raw_dir, processed_dir, preprocess_script):
    # 데이터셋 이름별로 독립적인 DAG를 생성
    with DAG(
        # 데이터셋마다 다른 DAG 이름
        dag_id=f"02_dag_factory_{ dataset_name }",
        start_date=airflow.utils.dates.days_ago(5),
        schedule_interval="@daily",
    ) as dag:
        # (예시) 데이터 파일 경로 정의
        raw_file_path=...

        # 1️⃣ 데이터 가져오기 태스크
        fetch_task=BaseOperator(...)

        # 2️⃣ 데이터 전처리 태스크
        preprocess_task=BashOperator(...)

        # 태스크 간 의존 관계 설정
        fetch_task >> preprocess_task

    # 완성된 DAG 객체 반환 → Airflow가 이를 DAG로 등록하여 스케줄링 가능
    return dag
```

```plaintext
4. 2 번과 3 번의 차이는 다음과 같다.

(2번) 하나의 DAG 안에 여러 태스크 세트 생성
 └── 01_task_factory
       ├─ sales_fetch → sales_preprocess → sales_export
       └─ customers_fetch → customers_preprocess → customers_export

(3번) 여러 DAG 자체를 자동 생성
 ├─ 02_dag_factory_sales
 │     └─ fetch → preprocess
 └─ 02_dag_factory_customers
       └─ fetch → preprocess
```

```python
# 5. DAG factory 함수 적용.
""" 중략 """
# factory 함수를 사용하여 DAG 만들기.
#   - generate_dag가 factory function의 역할을 한다.
dag=generate_dag(
    dataset_name="sales",
    raw_data="/data/raw",
    processed_dir="/data/processed",
    preprocess_script="preprocess_sales.py",
)
```

```python
# 6. factory 함수로 여러 DAG 생성.
for dataset in ["sales", "customers"]:
    # 다른 구성으로 여러 DAG 생성.
    #     - 각 DAG가 서로 덮어쓰지 않도록 전역 네임 스페이스에서 고유한 이름을 할당한다.
    globals()[f"02_dag_factory_{ dataset }"]=generate_dag(
        dataset_name=dataset,
        raw_dir="/data/raw",
        processed_dir="/data/processed",
        preprocess_script=f"preprocess_{ dataset }.py",
    )
```

<br>

<h2>1-6. 태스크 그룹을 사용하여 관련된 태스크들의 그룹 만들기</h2>
<ul>
  <li>
    여러 DAG가 얽힌 복잡한 DAG구조는 Airflow2의 <strong>태스크 그룹(task group)</strong>을 통해 그룹화하여 관리할 수 있다.
  </li>
    <ul>
      <li>
        태스크 그룹을 더 작은 그룹으로 <strong>그룹화하여(시각적으로)</strong> DAG 구조를 보다 쉽게 관리하고 이해할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. TaskGroup을 사용하여 태스크를 시각적으로 그룹화
""" 중략 """

# 여러 데이터셋(sales, customers)에 대해 동일한 처리 파이프라인을 구성
for dataset in ["sales", "customers"]:

    # 각 데이터셋마다 별도의 TaskGroup을 생성
    # - Airflow UI에서 태스크들을 접고 펼칠 수 있는 '그룹'으로 보여줌
    # - tooltip은 UI에서 마우스를 올렸을 때 표시되는 설명
    with TaskGroup(dataset, tooltip=f"Tasks for processing { dataset }"):

        # generate_tasks()는 fetch → preprocess → export 단계의 태스크 세트를 생성하
        # 는 함수
        #     - 동일한 로직을 여러 데이터셋에 재사용
        #     - TaskGroup 내부에 정의되므로, 해당 데이터셋 그룹에 속한 태스크로 표시됨
        generate_tasks(
            # 데이터셋 이름
            dataset_name=dataset,
            # 원본 데이터 디렉토리 경로
            raw_dir="/data/raw",
            # 전처리된 데이터 디렉토리 경로
            processed_dir="/data/processed",
            # 최종 결과 출력 경로
            output_dir="/data/output",
            # 데이터셋별 전처리 스크립트
            preprocess_scrip=f"preprocess_{ dataset }.py"
            # 생성된 태스크를 포함할 DAG 인스턴스
            dag=dag
        )
```

<br>

<h2>1-7. 대규모 수정을 위한 새로운 DAG 생성</h2>
<ul>
  <li>
    DAG에 큰 변경이 발생하면 이전과 같이 실행되지 않는 경우가 있는데 이때에는 DAG를 수정하기 전 DAG를 복사해 두고 사용한다.
  </li>
  <li>
    Airflow 3.0 이상부터는 <strong>DAG의 버전관리 기능</strong>이 추가되어 복사할 필요가 없어졌다.
  </li>
</ul>

<br><br>

<h1>2. 재현 가능한 태스크 설계</h1>
<h2>2-1. 태스크는 항상 멱등성을 가져야 합니다</h2>
<ul>
  <li>
    Airflow 태스크는 항상 <strong>멱등성(idempotent)</strong>을 가져야한다. 즉, 동일한 태스크를 <strong>여러 번</strong> 다시 실행하더라도 그 <strong>결과는 항상 동일</strong>해야 한다.
  </li>
  <li>
    태스크가 다시 실행될 때 앞서 출력된 데이터를 덮어 쓰도록 설정하여 멱등성을 강제할 수 있으나 부작용을 신중히 고려해야 한다.
  </li>
</ul>

<br>

<h2>2-2. 태스크 결과는 결정적이어야 합니다</h2>
<ul>
  <li>
    태스크는 <strong>동일한 입력값</strong>에 항상 <strong>같은 결과</strong>를 내야한다. 즉, 태스크는 <strong>결정적(deterministic)</strong>일 때만 재현할 수 있다.
  </li>
  <li>
    비결정적인 경우는 다음과 같은 경우가 있다.
  </li>
    <ul>
      <li>
        <strong>함수 내 데이터</strong> 또는 데이터 구조의 <strong>암시적 순서</strong>에 의존하는 경우.
      </li>
      <li>
        함수 내에서 임의 값, 전역 변수, 디스크에 저장된 외부 데이터 등을 포함하여 <strong>외부 상태</strong>를 사용하는 경우.
      </li>
      <li>
        결과에 대해 명시적인 순서를 지정하지 않고 <strong>병렬</strong>로 데이터 처리를 수행하는 경우.
      </li>
      <li>
        다중 스레드 코드 내에서의 경쟁적 실행.
      </li>
      <li>
        부적절한 예외 처리
      </li>
    </ul>
  <li>
    일반적으로 함수 내에서 발생할 수 있는 비결정성의 원인을 고려하면 비결정적 문제를 피할 수 있다.
  </li>
</ul>

<br>

<h2>2-3. 함수형 패러다임을 사용하여 태스크를 설계합니다</h2>
<ul>
  <li>
    <strong>함수형 프로그래밍</strong>이란 기본적으로 계산을 처리하는 컴퓨터 프로그램을 수학적 함수의 응용으로 구현하되, <strong>상태 변경</strong> 및 <strong>변경 가능한 데이터</strong>를 피하는 방식으로 함수를 만드는 것이다.
  </li>
  <li>
    함수형 프로그래밍 언어의 함수는 일반적으로 순수한 상태를 유지한다. 즉, 결과를 반환하지만 어떤 부작용도 포함하면 안된다.
  </li>
  <li>
    함수형 프로그래밍언어는 함수의 결과가 주어진 입력에 대해 항상 동일하며 따라서 <strong>순수 함수(pure function)</strong>는 일반적으로 멱등적이고 결정적이다.
  </li>
  <li>
    이런 함수형 프로그래밍 언어를 Airflow 함수에 사용할 수 있다.
  </li>
</ul>

<br><br>

<h1>3. 효율적인 데이터 처리</h1>
<h2>3-1. 데이터의 처리량 제한하기</h2>
<ul>
  <li>
    데이터를 효율적으로 처리하는 가장 좋은 방법은 원하는 결과를 얻는 데 필요한 <strong>최소한의 데이터</strong>로 처리를 제한하는 것이다.
  </li>
    <ul>
      <li>
        필요한 데이터셋의 경우에도 사용되지 않는 <strong>행과 열을 제거</strong>한다.
      </li>
    </ul>
  <li>
    <strong>필터링</strong>과 <strong>집계 단계</strong>의 수행 순서를 앞으로 배치하여 조인 전에 데이터 세트의 크기를 줄인다.
  </li>
</ul>

<br>

<h2>3-2. 증분 적재 및 처리</h2>
<ul>
  <li>
    증분 처리를 활용한 방법은 데이터를 <strong>파티션으로 분할</strong>하고, 불할된 파티션마다 DAG를 수행하여 <strong>개별적으로 처리</strong>하는 방법이다.
  </li>
    <ul>
      <li>
        개별 처리량은 <strong>해당 파티션의 크기</strong>로 제한할 수 있다.
      </li>
      <li>
        실행 중 오류로 인해 중단되더라도 <strong>실패한 부분만 다시 실행</strong>하면 되기에 효율적이다.
      </li>
    </ul>
</ul>

<br>

<h2>3-3. 중간 단계 데이터 캐싱</h2>
<ul>
  <li>
    태스크에서 중간 단계 데이터를 저장하면 각 태스크를 다른 태스크와 <strong>독립적으로 다시 실행</strong>할 수 있다.
  </li>
    <ul>
      <li>
        단, 대규모 데이터 세트의 여러 중간 버전이 있는 경우 <strong>과도한 양의 스토리지</strong>가 필요할 수 있다는 단점이 있다. 이 경우 제한된 기간의 중간 단계 데이터 세트만 유지하는 절충안을 고려할 수 있다.
      </li>
    </ul>
  <li>
    데이터의 <strong>가장 원시 버전</strong>을 항상 사용 가능하도록 보관하는 것이 좋다. (캐싱과는 다름).
  </li>
    <ul>
      <li>
        이렇게 하면 언제나 데이터를 재처리 할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>3-4. 로컬 파일 시스템에 데이터 저장 방지</h2>
<ul>
  <li>
    로컬에 저장하는 경우 다운스트림 태스크가 접근이 불가하는 경우가 있기 때문에 전체에서 접근 가능한 저장소에 데이터를 저장해야 한다.
  </li>
</ul>

<br>

<h2>3-5. 외부/소스 시스템으로 작업을 이전하기</h2>
<ul>
  <li>
    Airflow는 실제 데이터 처리를 위해 Airflow 워커보다 <strong>오케스트레이션</strong>을 통해 <strong>적합한 앱</strong>으로 처리하도록 하는 것이 좋다.
  </li>
    <ul>
      <li>
        데이터가 커지거나 작업이 복잡해 지면 Airflow보다 적합한 앱이 처리를 하는 것이 효율적이기 때문이다.
      </li>
    </ul>
</ul>

<br><br>

<h1>4. 자원관리</h1>
<h2>4-1. Pool을 이용한 동시성 관리하기</h2>
<ul>
  <li>
    Airflow는 <strong>리소스 풀(resource pool)</strong>을 통해 주어진 리소스에 접근할 수 있는 <strong>태스크의 수</strong>를 제어할 수 있다.
  </li>
    <ul>
      <li>
        리소스 풀의 각 풀은 해당 리소스에 대한 접근 권한을 부여하는 고정된 수의 <strong>슬롯(slot)</strong>을 갖고 있다.
      </li>
      <li>
        개별 태스크는 리소스 풀에 할당되어 해당 태스크가 스케줄 되기 전에 해당 풀에서 <strong>슬롯을 확보</strong>한다.
      </li>
    </ul>
  <li>
    리소스 풀은 Airflow UI의 <strong>'Admin > Pool'</strong> 섹션에서 리소스 풀에 관한 여러 설정을 할 수 있다.
  </li>
  <li>
    풀에 여유 슬롯이 없는 경우 <strong>스케줄러</strong>는 슬롯을 사용할 수 있을 때까지 <strong>태스크의 예약을 연기</strong>한다.
  </li>
</ul>

```python
# 1. 태스크에 특정 자원 풀 할당.
PythonOperator(
    task_id="my_task",
    """ 중략 """
    pool="my_resource_pool"
)
```

<br>

<h2>4-2. SLA 및 경고를 사용하여 장기 실행 작업 탐지</h2>
<ul>
  <li>
    Airflow를 사용하면 <strong>SLA(Service-Level Agreement, 서비스 수준 계약) 메커니즘</strong>을 사용하여 <strong>태스크의 동작을 모니터링</strong> 할 수 있다.
  </li>
    <ul>
      <li>
        태스크 또는 DAG가 SLA를 놓치는 경우(지정된 제한 시간보다 실행하는 데 더 오래 걸리는 경우) Airflow가 경고를 표시한다.
      </li>
    </ul>
  <li>
    SLA 누락은 Airflow 메타스토어에 기록되며 웹 UI의 <strong>'Browse > SLA misses'</strong>에서 확인할 수 있다. (이메일 전송 또한 설정 가능).
  </li>
    <ul>
      <li>
        sla_miss_callback 매개변수를 사용해 핸들러 함수를 DAG에 전달하여 SLA 누락에 대한 사용자 핸들러를 지정할 수도 있다.
      </li>
    </ul>
  <li>
    Airflow SLA가 항상 개별 태스크가 아니라 <strong>DAG의 시작 시간</strong>을 기준으로 정의되기 때문에 <strong>태스크의 종료 시간</strong>과 <strong>DAG의 시작 시간</strong>을 계속 비교해야 한다.
  </li>
</ul>

```python
# 1. DAG의 모든 태스크에 SLA 할당
from datetime import timedelta
default_args={
    "sla": timedelta(hours=2),
    """ 중략 """
}

with DAG(
    dag_id="...",
    """ 중략 """
    default_args=default_args,
) as dag:
    """ 중략 """
```

```python
# 2. SLA 누락에 대한 사용자 지정 콜백
def sla_miss_callback(context):
    send_slack_message("Missed SLA!")
""" 중략 """

with DAG(...
    sla_miss_callback=sla_miss_callback
) as dag:
""" 중략 """
```

```python
# 3. 특정 태스크에 SLA 할당.
PythonOperator(
    """ 중략 """
    sla=timedelta(hours=2)
)
```