<ul>
  <li>
    특정 조건을 센서에 만족하도록 대기하기
  </li>
  <li>
    서로 다른 DAG의 태스크간 의존성 설정하기
  </li>
  <li>
    CLI 및 REST API를 통해 워크플로 실행하기
  </li>
</ul>

<br>

<h1>1. 센서를 사용한 폴링 조건</h1>
<ul>
  <li>
    데이터는 하루 중 언제든 도착할 수 있는 상황을 가정한다.
  </li>
  <li>
    Airflow 오퍼레이터의 특수 타입(서브 클래스)인 <strong>센서(sensor)</strong>를 사용하면, 데이터 도착 여부 등 <strong>특정 이벤트를 감지</strong>하여 DAG 실행을 제어하는 트리거를 구현할 수 있다.
  </li>
    <ul>
      <li>
        특정 조건이 true인지 지속적으로 확인한다. true인 경우 실행을 하고, <strong>false</strong>인 경우 센서의 상태가 <strong>true가 될 때까지</strong> 또는 <strong>타임아웃</strong>될 때까지 계속 확인한다.
      </li>
    </ul>
  <li>
    sensor가 확인을 하는 주기는 <strong>60초</strong>가 기본값이다.
  </li>
    <ul>
      <li>
        sensor는 대략 1분에 한 번씩 이벤트를 <strong>포크(poke)</strong> 한다.
      </li>
      <li>
        <strong>Poking</strong>: <strong>sensor를 실행</strong>하고 <strong>상태를 확인</strong>하기 위해 Airflow에서 사용하는 명칭이다.
      </li>
    </ul>
  <li>
    
  </li>
</ul>

```python
# 1. sensor 예
from airflow.sensors.filesystem import FileSensor

wail_for_supermarket_1=FileSensor(
    task_id="wait_for_supermarket_1",
    # 파일이 있는 경우가 true가 된다.
    #   - 파일이 없다면 기본값인 60초 마다 재확인을 한다.
    #   - 태스크 로그에서 센서의 출력 내용을 확인할 수 있다.
    filepath="/data/supermarget1/data.csv",
)
```

<br>

<h2>1-1. 사용자 지정 조건 폴링</h2>
<ul>
  <li>
    <strong>FileSensor</strong>는 주로 와일드 카드를 사용하여 <strong>일치하는 파일명</strong>이 있는지 확인하는 방법을 사용한다.
  </li>
    <ul>
      <li>
        단, 이 경우 예상치 못하게 일치하는 다른 파일이 들어오면 의도하지 않게 DAG가 수행되어 버린다는 문제가 있다.
      </li> 
    </ul>
  <li>
    위와 같은 문제는 FileSensor에 글로빙을 활용하여 해결할 수도 있지만 이보다는 <strong>PythonSensor</strong>로 <strong>사용자가 지정한 조건</strong>을 확인하는 방법이 더 효율적이다.
  </li>
  <li>
    PythonSensor는 callable을 지원하지만 callable의 반환 값은 <strong>조건</strong>에 따라 <strong>true 혹은 false</strong>를 반환하도록 제한된다.
  </li>
</ul>

```python
# 1. PythonSensor를 사용하여 사용자 지정 조건 구현.
from pathlib import Path
from airflow.sensors.python import PythonSensor

# 1-1. 특정 마트(supermarket) 경로에서 조건을 확인하는 함수 정의
def _wait_for_supermarket(supermarket_id):
    supermarket_path=Path("/data/" + supermarket_id)
    # data-*.csv 파일들이 존재하는지 확인
    data_files=supermarget_path.glob("data-*.csv")
    # "_SUCCESS" 파일이 존재하는지 확인
    success_file=supermarket_path / "_SUCCESS"
    # 두 조건 모두 만족해야 True 반환 → PythonSensor 성공
    #     - data_files가 존재해야 함
    #     - _SUCCESS 파일이 존재해야 함
    return data_files and success_file.exists()

# 1-2. PythonSensor 오퍼레이터 정의
wait_for_supermarket_1=PythonSensor(
    task_id="wait_for_supermarket_1",
    # 호출할 사용자 정의 함수
    python_callable=_wait_for_supermarket,
    # 함수에 전달할 인자 (마트 ID)
    op_kwargs=("supermarket_id": "supermarket1"),
    # DAG에 연결
    dag=dag,
)
```

<br>

<h2>1-2. 원활하지 않는 흐름의 센서 처리</h2>
<ul>
  <li>
    Sensor는 <strong>timeout 인수</strong>를 사용하여 <strong>최대 실행 허용 시간</strong>을 관리한다.
  </li>
    <ul>
      <li>
        기본값은 7일 이며 7일 후에는 fail로 처리된다.
      </li>
    </ul>
  <li>
    그러나 Sensor가 <strong>계속해서 조건을 확인</strong>하는 특성상, DAG가 매일 실행되면 <strong>새로운 Sensor 태스크 인스턴스가 추가</strong>되어 실행 중인 <strong>태스크 수가 점점 늘어나는 문제</strong>가 발생할 수 있다.
  </li>
    <ul>
      <li>
        조건 확인을 위해 계속 실행하는 것을 <strong>폴링(polling)</strong>이라 한다.
      </li>
      <li>
        이러한 문제는 Airflow가 <strong>실행 태스크 수를 제한</strong>하여 해결할 수 있다.
      </li>
      <li>
        DAG에서는 <strong>Concurrency 인수</strong>를 사용하여 최대 태스크 수를 제한한다.
      </li>
    </ul>
  <li>
    센서의 <strong>폴링</strong>으로 인해 실행 중인 <strong>태스크가 누적</strong>되고, <strong>새로운 태스크가 계속 추가</strong>되면서 최대 동시 실행 수를 초과해 <strong>차단(blocking)</strong>되는 현상을 <strong>센서 데드록(sensor deadlock)</strong>이라 한다.
  </li>
  <li>
    센서 데드록 문제는 sensor 클래스의 mode 인수를 통해 해결할 수 있다.
  </li>
    <ul>
      <li>
        <strong>poke</strong>
      </li>
        <ul>
          <li>
            최대 태스크 수에 도달하면 <strong>새로운 태스크가 차단</strong>된다.
          </li>
          <li>
            새로운 태스크가 차단되지만 <strong>태스크 슬롯은 여전히 차지</strong>한다.
          </li>
        </ul>
      <li>
        <strong>reschedule</strong>
      </li>
        <ul>
          <li>
            포크 동작을 실행할 때만 슬롯을 차지하며 <strong>대기 시간 동안은 슬롯을 차지하지 않는다</strong>.
          </li>
        </ul>
    </ul>
  <li>
    <strong>동시 태스크의 수</strong>는 Airflow <strong>전역 설정 옵션</strong>으로 제어할 수 있다.
  </li>
</ul>

```python
# 1. Concurrency를 사용하여 최대 태스크 수 제한.
Dag=DAG(
    Dag_id="couponing_app",
    Start_date=datetime(2019, 1, 1),
    Schedule_interval="0 0 * * *",
    # 최대 50 개의 태스크 허용.
    Concurrency=50,
)
```

<br><br>

<h1>2. 다른 DAG를 트리거하기</h1>
<ul>
  <li>
    큰 DAG을 <strong>여러 개의 작은 DAG</strong>으로 분할하면, 앞선 태스크 완료를 기다리지 않고 <strong>병렬적</strong>으로 일부 워크플로를 처리할 수 있다.
  </li>
    <ul>
      <li>
        단일 DAG에 많은 태스크를 넣지 않아도 되고, 한 DAG가 다른 DAG를 여러 번 호출해 유연하게 실행할 수 있다는 장점이 있다.
      </li>
    </ul>
  <li>
    <strong>트리 뷰</strong>의 두 가지 상세 내역을 통해 트리거가 되었는지 확인할 수 있다.
  </li>
    <ul>
      <li>
        DAG가 수행되고 해당 태스크 인스턴스에 <strong>검은색 테두리</strong>가 표시된다.
      </li>
      <li>
        각 DAG 실행은 <strong>run_id 필드</strong>가 있으며 다음 중 하나로 실행된다.
      </li>
        <ul>
          <li>
            <strong>schedule__</strong>: <strong>스케줄</strong>되어 DAG가 실행이 시작되었음을 나타낸다.
          </li>
          <li>
            <strong>backfill__</strong>: <strong>백필 태스크</strong>에 의해 DAG가 실행이 시작되었음을 나타낸다.
          </li>
          <li>
            <strong>manual__</strong>: <strong>수동</strong>으로 DAG 실행이 시작되었음을 나타낸다.
          </li>
        </ul>
    </ul>
</ul>

```python
# 1. TriggerDagRunOperator를 사용하여 다른 DAG 실행하기.
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

# 1-1. 첫 번째 DAG 정의 (데이터 적재용)
dag1=DAG(
    dag_id="ingest_supermarket_data",
    # DAG 시작일 (3일 전부터)
    start_date=airflow.utils.dates.days_ago(3),
    # 매일 16시에 실행 (cron 표현식)
    schedule_interval="0 16 * * *",
)

# supermarket_id의 수에 맞추어 반복된다.
for supermarket_id in range(1, 5):
    """ 중략: 데이터 적재 태스크 정의 부분이 들어갈 수 있다 """

    trigger_create_metrics_dag=TriggerDagRunOperator(
        task_id=f"trigger_create_metrics_dag_supermarket_{supermarket_id}",
        # 트리거할 DAG의 ID (아래 정의된 dag2)
        #   - TriggerDagRunOperator의 trigger_dag_id의 값은 트리거할 DAG의 dag_id와 일치해야 한다.
        trigger_dag_id="create_metrics",
         # 현재 DAG(dag1)에 속하도록 지정
        dag=dag1,
    )

# 1-3. 두 번째 DAG 정의 (지표 생성용)
dag2=DAG(
    # TriggerDagRunOperator의 trigger_dag_id와 일치한다.
    dag_id="create_metrics",
    start_date=airflow.utils.dates.days_ago(3),
    # 스케줄 없음 (외부 Trigger로만 실행됨)
    schedule_interval=None,
)

""" 중략: dag2 내부의 실제 태스크(지표 계산 등) 정의 """
```

<br>

<h2>2-1. TriggerDagRunOperator로 백필 작업</h2>
<ul>
  <li>
    TriggerDagRunOperator를 DAG에서 삭제해도 설정된 DAG가 사라지는 것이 아니라, <strong>독립적으로 새로운 DAG Run</strong>이 생성된다.
  </li>
    <ul>
      <li>
        이는 Airflow가 <strong>과거 DAG 실행과 태스크 실행 내역</strong>을 저장하고 있어, 단순히 <strong>의존성</strong>만 끊겼을 뿐 대상 DAG는 여전히 <strong>독립적으로 수행</strong>되기 때문이다.
      </li>
    </ul>
</ul>

<br>

<h2>2-2. 다른 DAG의 상태를 폴링하기</h2>
<ul>
  <li>
    DAG을 여러 개로 <strong>분할</strong>하고 <strong>TriggerDagRunOperator로 연결</strong>하면, 복잡한 워크플로를 관리하기 쉬워지고, 실행 유연성이 생기며, 리소스와 안정성이 개선된다.
  </li>
    <ul>
      <li>
        하나의 DAG의 복잡한 <strong>의존관계</strong>를 작고 독립적으로 만들 수 있다.
      </li>
      <li>
        특정 DAG만을 <strong>독립적</strong>으로 실행할 수 있어 <strong>재사용성</strong>이 높아진다.
      </li>
      <li>
        여러 개의 DAG로 분할되기에 <strong>병목현상을 방지</strong>할 수 있다.
      </li>
    </ul>
  <li>
    Airflow는 단일 DAG의 의존성은 관리하지만 분리된 <strong>여러 DAG의 의존성</strong>의 관리는 <strong>ExternalTaskSensor</strong>를 적용할 수 있다.
  </li>
    <ul>
      <li>
        ExternalTaskSensor는 다른 DAG의 태스크를 지정하여 <strong>해당 태스크의 상태를 확인</strong>하는 <strong>프록시 역할</strong>을 한다.
      </li>
    </ul>
</ul>