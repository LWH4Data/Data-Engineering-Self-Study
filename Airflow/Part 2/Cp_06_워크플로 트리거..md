<ul>
  <li>
    특정 조건을 센서에 만족하도록 대기하기
  </li>
  <li>
    서로 다른 DAG의 태스크간 의존성 설정하기
  </li>
  <li>
    CLI 및 REST API를 통해 워크플로 실행하기
  </li>
</ul>

<br>

<h1>1. 센서를 사용한 폴링 조건</h1>
<ul>
  <li>
    데이터는 하루 중 언제든 도착할 수 있는 상황을 가정한다.
  </li>
  <li>
    Airflow 오퍼레이터의 특수 타입(서브 클래스)인 <strong>센서(sensor)</strong>를 사용하면, 데이터 도착 여부 등 <strong>특정 이벤트를 감지</strong>하여 DAG 실행을 제어하는 트리거를 구현할 수 있다.
  </li>
    <ul>
      <li>
        특정 조건이 true인지 지속적으로 확인한다. true인 경우 실행을 하고, <strong>false</strong>인 경우 센서의 상태가 <strong>true가 될 때까지</strong> 또는 <strong>타임아웃</strong>될 때까지 계속 확인한다.
      </li>
    </ul>
  <li>
    sensor가 확인을 하는 주기는 <strong>60초</strong>가 기본값이다.
  </li>
    <ul>
      <li>
        sensor는 대략 1분에 한 번씩 이벤트를 <strong>포크(poke)</strong> 한다.
      </li>
      <li>
        <strong>Poking</strong>: <strong>sensor를 실행</strong>하고 <strong>상태를 확인</strong>하기 위해 Airflow에서 사용하는 명칭이다.
      </li>
    </ul>
  <li>
    
  </li>
</ul>

```python
# 1. sensor 예
from airflow.sensors.filesystem import FileSensor

wail_for_supermarket_1=FileSensor(
    task_id="wait_for_supermarket_1",
    # 파일이 있는 경우가 true가 된다.
    #   - 파일이 없다면 기본값인 60초 마다 재확인을 한다.
    #   - 태스크 로그에서 센서의 출력 내용을 확인할 수 있다.
    filepath="/data/supermarget1/data.csv",
)
```

<br>

<h2>1-1. 사용자 지정 조건 폴링</h2>
<ul>
  <li>
    <strong>FileSensor</strong>는 주로 와일드 카드를 사용하여 <strong>일치하는 파일명</strong>이 있는지 확인하는 방법을 사용한다.
  </li>
    <ul>
      <li>
        단, 이 경우 예상치 못하게 일치하는 다른 파일이 들어오면 의도하지 않게 DAG가 수행되어 버린다는 문제가 있다.
      </li> 
    </ul>
  <li>
    위와 같은 문제는 FileSensor에 글로빙을 활용하여 해결할 수도 있지만 이보다는 <strong>PythonSensor</strong>로 <strong>사용자가 지정한 조건</strong>을 확인하는 방법이 더 효율적이다.
  </li>
  <li>
    PythonSensor는 callable을 지원하지만 callable의 반환 값은 <strong>조건</strong>에 따라 <strong>true 혹은 false</strong>를 반환하도록 제한된다.
  </li>
</ul>

```python
# 1. PythonSensor를 사용하여 사용자 지정 조건 구현.
from pathlib import Path
from airflow.sensors.python import PythonSensor

# 1-1. 특정 마트(supermarket) 경로에서 조건을 확인하는 함수 정의
def _wait_for_supermarket(supermarket_id):
    supermarket_path=Path("/data/" + supermarket_id)
    # data-*.csv 파일들이 존재하는지 확인
    data_files=supermarget_path.glob("data-*.csv")
    # "_SUCCESS" 파일이 존재하는지 확인
    success_file=supermarket_path / "_SUCCESS"
    # 두 조건 모두 만족해야 True 반환 → PythonSensor 성공
    #     - data_files가 존재해야 함
    #     - _SUCCESS 파일이 존재해야 함
    return data_files and success_file.exists()

# 1-2. PythonSensor 오퍼레이터 정의
wait_for_supermarket_1=PythonSensor(
    task_id="wait_for_supermarket_1",
    # 호출할 사용자 정의 함수
    python_callable=_wait_for_supermarket,
    # 함수에 전달할 인자 (마트 ID)
    op_kwargs=("supermarket_id": "supermarket1"),
    # DAG에 연결
    dag=dag,
)
```

<br>

<h2>1-2. 원활하지 않는 흐름의 센서 처리</h2>
<ul>
  <li>
    Sensor는 <strong>timeout 인수</strong>를 사용하여 <strong>최대 실행 허용 시간</strong>을 관리한다.
  </li>
    <ul>
      <li>
        기본값은 7일 이며 7일 후에는 fail로 처리된다.
      </li>
    </ul>
  <li>
    그러나 Sensor가 <strong>계속해서 조건을 확인</strong>하는 특성상, DAG가 매일 실행되면 <strong>새로운 Sensor 태스크 인스턴스가 추가</strong>되어 실행 중인 <strong>태스크 수가 점점 늘어나는 문제</strong>가 발생할 수 있다.
  </li>
    <ul>
      <li>
        조건 확인을 위해 계속 실행하는 것을 <strong>폴링(polling)</strong>이라 한다.
      </li>
      <li>
        이러한 문제는 Airflow가 <strong>실행 태스크 수를 제한</strong>하여 해결할 수 있다.
      </li>
      <li>
        DAG에서는 <strong>Concurrency 인수</strong>를 사용하여 최대 태스크 수를 제한한다.
      </li>
    </ul>
  <li>
    센서의 <strong>폴링</strong>으로 인해 실행 중인 <strong>태스크가 누적</strong>되고, <strong>새로운 태스크가 계속 추가</strong>되면서 최대 동시 실행 수를 초과해 <strong>차단(blocking)</strong>되는 현상을 <strong>센서 데드록(sensor deadlock)</strong>이라 한다.
  </li>
  <li>
    센서 데드록 문제는 sensor 클래스의 mode 인수를 통해 해결할 수 있다.
  </li>
    <ul>
      <li>
        <strong>poke</strong>
      </li>
        <ul>
          <li>
            최대 태스크 수에 도달하면 <strong>새로운 태스크가 차단</strong>된다.
          </li>
          <li>
            새로운 태스크가 차단되지만 <strong>태스크 슬롯은 여전히 차지</strong>한다.
          </li>
        </ul>
      <li>
        <strong>reschedule</strong>
      </li>
        <ul>
          <li>
            포크 동작을 실행할 때만 슬롯을 차지하며 <strong>대기 시간 동안은 슬롯을 차지하지 않는다</strong>.
          </li>
        </ul>
    </ul>
  <li>
    <strong>동시 태스크의 수</strong>는 Airflow <strong>전역 설정 옵션</strong>으로 제어할 수 있다.
  </li>
</ul>

```python
# 1. Concurrency를 사용하여 최대 태스크 수 제한.
Dag=DAG(
    Dag_id="couponing_app",
    Start_date=datetime(2019, 1, 1),
    Schedule_interval="0 0 * * *",
    # 최대 50 개의 태스크 허용.
    Concurrency=50,
)
```

<br><br>

<h1>2. 다른 DAG를 트리거하기</h1>
<ul>
  <li>
    큰 DAG을 <strong>여러 개의 작은 DAG</strong>으로 분할하면, 앞선 태스크 완료를 기다리지 않고 <strong>병렬적</strong>으로 일부 워크플로를 처리할 수 있다.
  </li>
    <ul>
      <li>
        단일 DAG에 많은 태스크를 넣지 않아도 되고, 한 DAG가 다른 DAG를 여러 번 호출해 유연하게 실행할 수 있다는 장점이 있다.
      </li>
    </ul>
  <li>
    <strong>트리 뷰</strong>의 두 가지 상세 내역을 통해 트리거가 되었는지 확인할 수 있다.
  </li>
    <ul>
      <li>
        DAG가 수행되고 해당 태스크 인스턴스에 <strong>검은색 테두리</strong>가 표시된다.
      </li>
      <li>
        각 DAG 실행은 <strong>run_id 필드</strong>가 있으며 다음 중 하나로 실행된다.
      </li>
        <ul>
          <li>
            <strong>schedule__</strong>: <strong>스케줄</strong>되어 DAG가 실행이 시작되었음을 나타낸다.
          </li>
          <li>
            <strong>backfill__</strong>: <strong>백필 태스크</strong>에 의해 DAG가 실행이 시작되었음을 나타낸다.
          </li>
          <li>
            <strong>manual__</strong>: <strong>수동</strong>으로 DAG 실행이 시작되었음을 나타낸다.
          </li>
        </ul>
    </ul>
</ul>

```python
# 1. TriggerDagRunOperator를 사용하여 다른 DAG 실행하기.
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator

# 1-1. 첫 번째 DAG 정의 (데이터 적재용)
dag1=DAG(
    dag_id="ingest_supermarket_data",
    # DAG 시작일 (3일 전부터)
    start_date=airflow.utils.dates.days_ago(3),
    # 매일 16시에 실행 (cron 표현식)
    schedule_interval="0 16 * * *",
)

# supermarket_id의 수에 맞추어 반복된다.
for supermarket_id in range(1, 5):
    """ 중략: 데이터 적재 태스크 정의 부분이 들어갈 수 있다 """

    trigger_create_metrics_dag=TriggerDagRunOperator(
        task_id=f"trigger_create_metrics_dag_supermarket_{supermarket_id}",
        # 트리거할 DAG의 ID (아래 정의된 dag2)
        #   - TriggerDagRunOperator의 trigger_dag_id의 값은 트리거할 DAG의 dag_id와 일치해야 한다.
        trigger_dag_id="create_metrics",
         # 현재 DAG(dag1)에 속하도록 지정
        dag=dag1,
    )

# 1-3. 두 번째 DAG 정의 (지표 생성용)
dag2=DAG(
    # TriggerDagRunOperator의 trigger_dag_id와 일치한다.
    dag_id="create_metrics",
    start_date=airflow.utils.dates.days_ago(3),
    # 스케줄 없음 (외부 Trigger로만 실행됨)
    schedule_interval=None,
)

""" 중략: dag2 내부의 실제 태스크(지표 계산 등) 정의 """
```

<br>

<h2>2-1. TriggerDagRunOperator로 백필 작업</h2>
<ul>
  <li>
    TriggerDagRunOperator를 DAG에서 삭제해도 설정된 DAG가 사라지는 것이 아니라, <strong>독립적으로 새로운 DAG Run</strong>이 생성된다.
  </li>
    <ul>
      <li>
        이는 Airflow가 <strong>과거 DAG 실행과 태스크 실행 내역</strong>을 저장하고 있어, 단순히 <strong>의존성</strong>만 끊겼을 뿐 대상 DAG는 여전히 <strong>독립적으로 수행</strong>되기 때문이다.
      </li>
    </ul>
</ul>

<br>

<h2>2-2. 다른 DAG의 상태를 폴링하기</h2>
<ul>
  <li>
    DAG을 여러 개로 <strong>분할</strong>하고 <strong>TriggerDagRunOperator로 연결</strong>하면, 복잡한 워크플로를 관리하기 쉬워지고, 실행 유연성이 생기며, 리소스와 안정성이 개선된다.
  </li>
    <ul>
      <li>
        하나의 DAG의 복잡한 <strong>의존관계</strong>를 작고 독립적으로 만들 수 있다.
      </li>
      <li>
        특정 DAG만을 <strong>독립적</strong>으로 실행할 수 있어 <strong>재사용성</strong>이 높아진다.
      </li>
      <li>
        여러 개의 DAG로 분할되기에 <strong>병목현상을 방지</strong>할 수 있다.
      </li>
    </ul>
  <li>
    Airflow는 단일 DAG의 의존성은 관리하지만 분리된 <strong>여러 DAG의 의존성</strong>의 관리는 <strong>ExternalTaskSensor</strong>를 적용할 수 있다.
  </li>
    <ul>
      <li>
        ExternalTaskSensor는 다른 DAG의 태스크를 지정하여 <strong>해당 태스크의 상태를 확인</strong>하는 <strong>프록시 역할</strong>을 한다.
      </li>
    </ul>
  <li>
    DAG 사이에 특정한 <strong>이벤트</strong>가 없는 경우 <strong>DAG의 상태</strong>를 확인하는데 어려움이 존재한다.
  </li>
    <ul>
      <li>
        <strong>ExternalTaskSensor</strong>는 기본적으로 <strong>동일 날짜</strong>를 활용하거나 <strong>오프셋(offset)</strong>을 확인한다.
      </li>
      <li>
        동일 날짜나 오프셋을 활용하면 <strong>날짜 단위 데이터 처리의 정합성</strong>을 보장하고, <strong>데이터 지연</strong>이나 <strong>시차</strong>가 있는 워크플로에도 적용이 가능하다.
      </li>
      <li>
        동일 날짜는 exexution_date를 통해, 오프셋은 execution_delta인수로 설정한다.
      </li>
    </ul>
  <li>
    두 DAG의 <strong>실행 주기</strong>가 다른 경우 ExternalTaskSensor의 <strong>execution_date_fn</strong> 인수를 통해 <strong>timedelta</strong>를 직접 반환하는 것이 아니라, 여러 <strong>datetime</strong>을 반환하고 이를 이용해 오프셋(시간 차이)을 계산할 수 있다.
  </li>
    <ul>
      <li>
        즉, execution_date_fn은 datetime 목록을 반환하며, 이때 각 datetime 간의 차이를 통해 필요한 timedelta를 계산해 활용할 수 있다.
      </li>
      <li>
        이렇게 계산된 timedelta는 오프셋(시간 차이) 목록처럼 동작하여 여러 태스크 간 시간 차이를 고려할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. ExternalTaskSensor 사용 예
import airflow.utils.dates
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task import ExternalTaskSensor

# 첫 번째 DAG 정의 (데이터 적재 및 처리)
dag1 = DAG(
    dag_id="ingest_supermarket_data", 
    schedule_interval="0 16 * * *", ...
    )
# 두 번째 DAG 정의
dag2 = DAG(schedule_interval="0 16 * * *", ...)

# dag1의 태스크 정의.
DummyOperator(task_id="copy_to_raw", dag=dag1) >> DummyOperator(task_id="process_supermarket", dag=dag1)

# 두 번째 DAG에서 ExternalTaskSensor 정의.
wait = ExternalTaskSensor(
    task_id="wait_for_process_supermarket",
    # 모니터링할 DAG
    external_dag_id="ingest_supermarket_data",
    # 모니터링할 태스크
    external_task_id="process_supermarket",
    dag=dag2,
)

# 두 번째 DAG에서 후속 태스크 정의
reprot = DummyOperator(task_id="report", dag=dag2)

# 의존성 설정.
wait >> report
```

```python
# 2. ExternalTaskSensor에 execution_delta를 이용하면 오프셋(시간 차이)을 적용할 수 있다
from airflow import DAG
from airflow.operators.dummy import DummyOperator
from airflow.sensors.external_task import ExternalTaskSensor

# DAG1: 매일 16시에 실행
dag1 = DAG(dag_id="dag1", schedule_interval="0 16 * * *")
# DAG2: 매일 20시에 실행
dag2 = DAG(dag_id="dag2", schedule_interval="0 20 * * *")

# DAG1 안의 태스크 (ETL 작업)
DummyOperator(task_id="etl", dag=dag1)

# DAG2에서 ExternalTaskSensor 정의
ExternalTaskSensor(
    task_id="wait_for_etl",
    # 모니터링할 DAG = dag1
    external_dag_id="dag1",
    # 모니터링할 태스크 = dag1의 etl
    external_task_id="etl",
    # DAG1 실행(16시)과 DAG2 실행(20시) 사이의 차이 = 4시간
    # 즉 DAG2는 자신 실행일보다 4시간 앞선 DAG1(etl) 실행을 참조
    execution_delta=datetime.timedelta(hours=4),
    dag=dag2,
)
```

<br><br>

<h1>3. REST/CLI를 이용해 워크플로 시작하기</h1>
<ul>
  <li>
    <strong>REST API 및 CLI</strong>를 통해 하나의 DAG에서 다른 DAG를 트리거하는 방법을 배운다.
  </li>
  <li>
    <strong>REST/CLI</strong>는 단순히 DAG 간 트리거뿐 아니라, <strong>외부 서비스</strong>에서 Airflow 워크플로를 시작하는 공식적인 방법이기도 하다. (CI/CD에서 트리거 등).
  </li>
</ul>

```bash
# 1. CLI로 DAG 트리거하기.
#   - 실행 날짜가 현재 날짜 및 시간으로 설정된 dag1을 트리거한다.
#   - 수동 또는 Airflow 외부에서 트리거되었다는 것을 나타내기 위해 "manual__" 접두사가 붙
#     는다.
airflow dags trigger dag1
```

```bash
# 2. 추가로 DAG 트리거하기.
#   - --conf 옵션을 통해 supermarket_id가 1인 dag1을 수행하도록 한다.
airflow dags trigger -c '{"supermarket_id": 1}' dag1
airflow dags trigger --conf '{"supermarket_id": 1}' dag1
```

```python
# 3. DAG 실행 시 conf 설정값 출력하기 예제
#     - DAG 실행 시 전달된 conf(JSON)를 출력하는 코드
import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator

# 1. DAG 정의
dag=DAG(
    dag_id="print_dag_run_conf",
    # DAG 시작일 (3일 전)
    start_date=airflow.utils.dates.days_ago(3),
    # 스케줄 없음 (수동 실행만 가능)
    schedule_interval=None,
)

# 2. conf 값을 출력하는 Python 함수
def print_conf(**context):
    # DAG 실행 시 --conf 옵션으로 전달된 JSON을 출력
    #     - 예: airflow dags trigger -c '{"supermarket_id": 1}' 
    #           print_dag_run_conf
    print(context["dag_run"].conf)

# 3. PythonOperator로 태스크 정의
process=PythonOperator(
    task_id="process",
    python_callable=print_conf,
    dag=dag,
)
```

```bash
# 3. CLI로 Airflow 인스턴스테 HTTP 접근으로 REST API를 사용할 수도 있지만 권장하지 않는
#    다.

curl \
# username과 password를 노출하는 것은 바람직하지 않다.
-u admin:admin \
-X POST \
"http://localhost:8080/api/v1/dags/print_dag_run_conf/dagRuns" \
-H "Content-Type: application/json" \
-d '{"conf": {}}'
```