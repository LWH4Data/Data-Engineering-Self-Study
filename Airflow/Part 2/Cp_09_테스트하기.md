<ul>
  <li>
    CI/CD 파이프라인에서 Airflow 테스트하기
  </li>
  <li>
    pytest로 테스트하기 위한 프로젝트 구성하기
  </li>
  <li>
    템플릿을 적용한 테스트 태스크를 위한 DAG 실행 시뮬레이션하기
  </li>
  <li>
    목업(mocking)으로 외부 시스템 조작하기
  </li>
  <li>
    컨테이너를 사용하여 외부 시스템에서 동작 테스트하기
  </li>
</ul>

<br><br>

<h1>1. 테스트 시작하기</h1>
<ul>
  <li>
    테스트의 순서는 <strong>단위 테스트 → 통합 테스트 → 승인 테스트(비즈니스)</strong>로 진행된다. 도서에서는 단위/통합 테스트만 다룬다.
  </li>
  <li>
    실습에서는 <strong>pytest</strong>를 사용한다.
  </li>
</ul>

<ul>

<h2>1-1. 모든 DAG에 대한 무결성 테스트</h2>
<ul>
  <li>
    Airflow 관점에서 테스트를 위한 첫 번째 단계는 일반적으로 <strong>DAG 무결성 테스트(integrity)</strong>이다.
  </li>
  <li>
    test 영역은 <strong>디렉터리 최상단</strong>에 dags와 함께 위치시킨다. 따라서 root/tests, root/dags, root/mypackage 세 개가 존재한다.
  </li>
    <ul>
      <li>
        일반적으로 테스트 파일에는 <strong>test_ 접두사</strong>가 붙는다.
      </li>
    </ul>
  <li>
    Pytest 프레임워크는 지정된 디렉터리(기본적으로 현재 작업 디렉터리)를 재귀적으로 스캔하여 
    <strong>test_*.py</strong> 또는 <strong>*_test.py</strong> 형태의 파일을 자동으로 찾는다.
  </li>
    <ul>
      <li>
        각 테스트 파일 안에서는 <strong>test_</strong>로 시작하는 함수나, <strong>test</strong>로 시작하는 클래스와 그 안의 
        <strong>test_</strong> 함수들이 자동으로 인식된다.
      </li>
      <li>
        패키지 구조와 상관없이 테스트를 찾기 때문에 <strong>__init__.py</strong> 파일을 만들 필요가 없다.
      </li>
    </ul>
</ul>

```python
# 1. 무결성이 발생하는 예
t1 = DummyOperator(task_id="t1", dag=dag)
t2 = DummyOperator(task_id="t2", dag=dag)
t3 = DummyOperator(task_id="t3", dag=dag)

# DAG가 순환된기 때문에 무결성이 깨진다.
t1 >> t2 >> t3 >> t1
```

```bash
# 2. 무결성 확인을 위해 pytest 설치.
pip install pytest
```

```python
# 3. tests/dags/test_dag_integrity.py 파일 생성.
import glob
import importlib.util
import os

import pytest
from airflow.models import DAG

# DAG 파일 경로 패턴 설정 (tests/dags 기준으로 상위 디렉터리의 dags 폴더 전체)
DAG_PATH = os.path.join(
    os.path.dirname(__file__), "..", "..", "dags/**/*.py"
)

# glob을 이용해 dags 디렉토리 밑 모든 .py 파일 목록 수집
DAG_FILES = glob.glob(DAG_PATH, recursive=True)

@pytest.mark.parametrize("dag_file", DAG_FILES)
def test_dag_integrity(dag_file):
    # 파일 이름에서 확장자 제거 → 모듈 이름으로 사용
    module_name, _ = os.path.splitext(dag_file)
    # 모듈 경로 생성
    module_path = os.path.join(DAG_PATH, dag_file)
    # 모듈을 파일 경로에서 직접 import할 수 있도록 spec 객체 생성
    mod_spec = importlib.util.spec_from_file_location(module_name, module_path)

    # spec으로부터 실제 모듈 객체 생성
    module = importlib.util.module_from_spec(mode_spec)
    mod_spec.loader.exec_module(module)

    # 모듈 내에서 DAG 객체만 추출
    dag_objects = [var for var in vars(module).values() if isinstance \
    (var, DAG)]

    # 최소 하나의 DAG이 정의되어 있어야 함
    assert dag_objects

    # DAG 객체들에 대해 무결성 테스트 (사이클 여부 검사)
    for dag in dag_objects:
        dag.test_cycle()
```

```python
# 4. 경로를 주어 무결성 검사 수행.
pytest tests/

# 출력의
#   - 최상단: 어떤 테스트가 실패했는지 출력된다.
#   - 최하단: 왜 실패했는지 출력된다.
```

<br>

<h2>1-2. CI/CD 파이프라인 설정하기</h2>
<ul>
  <li>
    CI/CD 파이프라인은 한 마디로 코드 저장소의 <strong>코드가 변경</strong>될 때 <strong>사전 정의된 스크립트를 실행</strong>하는 시스템이다.
  </li>
    <ul>
      <li>
        <strong>지속적 통합(CI)</strong>: 변경된 코드가 <strong>코드 코딩 표준</strong>과 <strong>테스트 조건</strong>을 준수하는지 확인하고 검증한다.
      </li>
      <li>
        <strong>지속적 배포(CD)</strong>: 사람의 간섭 없이 <strong>완전히 자동화된 코드</strong>를 프로덕션 시스템에 <strong>자동으로 배포</strong>하는 것을 의미한다.
      </li>
    </ul>
  <li>
    대부분의 CI/CD 시스템은 파이프라인이 정의된 <strong>YAML 구성 파일</strong>로 시작한다.
  </li>
  <li>
    파이프라인 정의는 일반적으로 프로젝트에서 <strong>가장 상단 루트</strong>에 있다.
  </li>
</ul>

```yaml
# 1. Airflow 프로젝트용 GitHub Action 파이프라인 예

# GitHub Actions 워크플로 이름
name: python static checks and tests

# 워크플로 실행 조건: push 이벤트가 발생할 때마다 실행
on: [push]

jobs:
  # Job 이름
  testing:
    # 실행 환경: Ubuntu 18.04 VM에서 동작
    runs-on: ubuntu-18.04
    steps:
      # 현재 저장소 코드 체크아웃
      - uses: actions/checkout@v1

      # Python 환경 설정 (Python 3.6.9, 64비트)
      - name: Setup Python
        uses: actions/setup-python@v1
        with:
          python-version: 3.6.9
          architecture: x64
      
      # 정적 코드 검사 도구 Flake8 설치 및 실행
      - name: Install Flake8
        run: pip install flake8
      - name: Run Flake8
        run: flake8
      
      # Pylint 설치 및 실행
      - name: Install Pylint
        run: pip install pylint
      - name: Run Pylint
        run: find . -name "*.py" | xargs black --check

      # 프로젝트 의존성 설치 (Airflow + Pytest)
      - name: Install dependencies
        run: pip install apache-airflow pytest
      
      # DAG 무결성 테스트 실행 (pytest로 tests/ 디렉토리 안 테스트 실행)
      - name: Test DAG integrity
        run: pytest tests/
```

<br>

<h2>1-3. 단위 테스트 작성하기</h2>

```python
# 1. 실습에 필요한 Operator 구현

#   - 주어진 두 날짜 사이에 상위 N개의 인기 영화를 반환하는 Operator
class MovielensPopularityOperator(BaseOperator):
    def __init__(
        self,
        conn_id,
        start_date,
        end_date,
        min_ratings=4,
        top_n=5,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self._conn_id = conn_id
        self._start_date = start_date
        self._end_date = end_date
        self._min_ratings = min_ratings
        self._top_n = top_n

    def execute(self, context):
        # Hook을 이용해 Movielens DB 연결 (with문으로 자동 close 보장)
        with MovielensHook(self._conn_id) as hook:
            # 지정된 날짜 범위의 모든 평점 데이터 가져오기
            ratings = hook.get_ratings(
                start_date=self._start_date,
                end_date=self._end_date,
            )
            
            # 각 영화별 평점 누적 합계 및 평가 횟수 저장
            rating_sums = defaultdict(Counter)
            for rating in ratings:
                rating_sums[rating["movieId"]].update(
                    # 평점 횟수 +1
                    count=1,
                    # 평점 값 누적
                    rating=rating["rating"]
                )
            
            # 각 영화별 (평균 평점, 평가 개수) 계산
            averages = {
                movie_id: (
                    # 평균 평점
                    rating_counter["rating"] / rating_counter["count"],
                    # 평가 개수
                    rating_counter["count"]
                )
                for movie_id, rating_counter in rating_sums.items()
                # 최소 평가 개수 이상인 영화만 포함
                if rating_counter["count"] >= self._min_ratings
            }

            # 평균 평점을 기준으로 내림차순 정렬 후 상위 N개 반환
            return sorted(
                averages.items(),
                # (평균, 개수) 튜플 기준 정렬
                key=lambda X: x[1],
                reverse=True
            )[:self._top_n]
```

<br>

<h2>1-4. Pytest 프로젝트 구성하기</h2>
<ul>
  <li>
    Airflow <strong>시스템 외부</strong>와 <strong>단위 테스트 내부</strong>에서 오퍼레이터를 <strong>단독으로 실행</strong>하기 위해 pytest 컴포넌트 구성.
  </li>
  <li>
    테스트는 <strong>격리된 환경</strong>에서 진행되기에 특정 테스트의 결과가 다른 테스트에 영향을 줄 수 없다. 이때 테스트 사이에 발생하는 정보를 전달할 때에는 <strong>목업(mocking)</strong>을 사용한다.
  </li>
    <ul>
      <li>
        목업은 특정 작업이나 객체를 모조로 만드는 것이다. 즉, 임의의 값을 전달해속이거나 모조한다.
      </li>
      <li>
        목업을 사용하기 위해 <strong>pytest-mock 패키지</strong>를 다운 받고, <strong>mocker라는 인수</strong>를 전달한다.
      </li>
    </ul>
  <li>
    python에서 목업의 구현하는 위치는 <strong>호출되는 위치</strong>에서 구성을 해야한다.
  </li>
</ul>

```python
# 1. BashOperator를 테스트하는 예제 테스트 함수.
def test_example():
    task = BashOperator(
        test_id="test",
        bash_command="echo 'hello!'",
        xcom_push=True,
    )
    result = task.execute(context={})
    # assert: 조건이 반드시 참이어야 한다는 디버깅/검증용 구문.
    assert result == "hello!"
```

```python
# 2. MovielensPopularityOperator를 테스트하는 함수의 예
def test_movielenspopularityoperator():
    task = MovielensPopularityOperator(
        task_id="test_id",
        start_date="2015-01-01",
        end_date="2015-01-03",
        top_n=5,
    )

    result = task.execute(context={})
    assert len(result) == 5

# conn_id가 없어 실패한다.
```

```bash
# 3. 목업을 위한 패키지 다운로드
pip install pytest-mock
```

```python
# 4. 테스트에서 객체를 목업하기.
def test_movielenspopularityoperator(mocker):
    # - mocker.patch.object 를 사용해 MovielensHook의 get_connection 메서드를 
    #   가짜로 대체
    # - 실제 DB에 연결하지 않고, 지정된 Connection 객체를 반환하도록 함
    mocker.path.object(
        MovielensHook,
        # 대체할 메서드 이름
        "get_connection",
        # 이 메서드가 호출되면 반환할 가짜 객체
        return_value=Connection(
            conn_id="test",
            login="airflow",
            password="airflow",
        ),
    )

    # MovielensPopularityOperator 인스턴스 생성
    task = MovielensPopularityOperator(
        task_id="test_id",
        conn_id="test",
        start_date="2015-01-01",
        end_date="2015-01-03",
        top_n=5,
    )

    # Operator 실행 (context는 여기서 None 전달)
    result = task.execute(context=None)

    # 결과 검증: 반환된 인기 영화 리스트의 길이가 5인지 확인
    assert len(result) == 5
```

```python
# 5. 목업 함수의 동작 검증하기

# MovielensHook.get_connection 메서드를 mock 처리
# 실제 DB에 연결하지 않고, 호출 기록만 추적할 수 있도록 함
mock_get = mocker.patch.object(
    MovielensHook,
    "get_connection",
    # 호출되면 반환할 가짜 Connection 객체
    return_value=Connection(...),
)

# 테스트할 Operator 생성 (conn_id="testconn" 전달)
task = MovielensPopularityOperator(..., conn_id="testconn")

# Operator 실행 → 내부적으로 MovielensHook.get_connection을 호출하게 됨
task.execute(...)

# 검증 1: get_connection이 정확히 1번 호출됐는지 확인
assert mock_get.call_count == 1

# 검증 2: 호출 시 인자로 "testconn"이 전달됐는지 확인
mock_get.assert_called_with("testconn")
```

```python
# 6. 목업을 호출되는 위치에서 구현. 즉, import 위치. (정의되는 위치 X).
# Mock은 클래스나 함수가 정의된 곳이 아니라, 호출되는 모듈의 import 경로를 따라 patch해야 한다.
from airflowbook.operators.movielens_operator import (
    # 테스트 대상 Operator
    MovielendsPopularityOperator,
    # Operator 내부에서 사용되는 Hook
    MovielensHook,
)

def test_movielenspopularityoperator(mocker):
    # MovielensHook.get_connection 메서드를 patch
    # - 실제 DB 연결 대신 Connector(...)라는 가짜 객체를 반환
    # - 중요한 점: MovielensHook이 정의된 모듈이 아니라,
    #   Operator가 import 해서 사용하는 모듈 경로 기준으로 patch 해야 함
    mock_get = mocker.patch.object(
        MovielensHook,
        "get_connection",
        # 가짜 연결 객체
        return_value=Connector(...),
    )

    # 테스트할 Operator 인스턴스 생성
    task = MovielensPopularityOperator(...)
```

<br>

<h2>1-5. 디스크의 파일로 테스트하기</h2>
<ul>
  <li>
    JSON 리스트를 갖는 파일을 읽고 CSV 형식으로 쓰는 오퍼레이터 실습.
  </li>
</ul>

```python
# 1. 오퍼레이터 생성.

# JSON 파일을 읽어서 CSV 파일로 변환하는 사용자 정의 Operator
class JsonToCsvOperator(BaseOperator):
    def __init__(self, input_path, output_path, **kwargs):
        # BaseOperator의 기본 파라미터(task_id, retries 등) 초기화
        super().__init__(**kwargs)
        # 입력 JSON 파일 경로
        self._input_path = input_path
        # 출력 CSV 파일 경로
        self._output_path = output_path

    def execute(self, context):
        # 1) JSON 파일 열기 → Python 객체로 로드
        with open(self._input_path, "r") as json_file:
            data = json.load(json_file)
        
        # 2) JSON 데이터에 등장하는 모든 key를 추출 → CSV 컬럼명으로 사용
        columns = {key for row in data for key in row.keys()}

        # 3) CSV 파일 열기 (쓰기 모드)
        with open(self._output_path, mode="w") as csv_file:
            # CSV writer 생성
            writer = csv.DictWriter(csv_file, feildnames=columns)
            # CSV 헤더 작성
            writer.writeheader()
            # JSON 데이터를 한 줄씩 CSV 행으로 기록
            writer.writerows(data)
```

```python
# 2. 임시 경로를 사용하여 테스트하기.

import csv
import json
from pathlib import Path

from airflowbook.operator.json_to_csv_operator import JsonToCsvOperator

# pytest에서 제공하는 tmp_path → 테스트 실행마다 임시 디렉토리를 제공
# 이 경로를 사용하면 테스트 후 자동 정리되므로 안전하게 파일 테스트 가능
def test_json_to_csv_operator(tmp_path: Path):

    # 입력 JSON, 출력 CSV 파일 경로를 임시 디렉토리에 생성
    input_path = tmp_path / "input.json"
    output_path = tmp_path / "output.csv"

    # 테스트용 JSON 데이터 준비
    input_data = [
        {"name": "bob", "age": "41", "sex": "M"},
        {"name": "alice", "age": "24", "sex": "F"},
        {"name": "carol", "age": "60", "sex": "F"}
    ]

    # JSON 파일 쓰기
    with open(input_path, "w") as f:
        f.write(json.dumps(input_data))

    # JsonToCsvOperator 인스턴스 생성
    operator = JsonToCsvOperator(
        task_id="test",
        input_path=input_path,
        output_path=output_path,
    )

    # Operator 실행 → JSON → CSV 변환 수행
    operator.execute(context={})

    # 변환된 CSV 파일 읽기
    with open(output_path, "r") as f:
        # CSV를 dict 형태로 읽음
        reader = csv.DictReader(f)
        # 행 데이터를 딕셔너리 리스트로 변환
        result = [dict(row) for row in reader]

    # 변환 결과가 원래 JSON 데이터와 동일한지 검증
    assert result == input_data
```

<br><br>

<h1>2. 테스트에서 DAG 및 태스크 콘텍스트로 작업하기</h1>
<ul>
  <li>
    Airflow가 실제 시스템에서 태스크를 실행하여 <strong>태스크 인스턴스 콘텍스트를 생성</strong>하고, <strong>모든 변수를 템플릿</strong>으로 만드는 등 보다 현실적인 시나리오 실습을 진행한다.
  </li>
    <ul>
      <li>
        앞선 예제는 인스턴스 콘텍스트가 비어있는 일부분 테스트 였다.
      </li>
    </ul>
  <li>
    템플릿 변수를 통해 제공되는 두 날짜 사이에 영화 평점을 가져오는 오퍼레이터를 구현했다고 가정한다.
  </li>
  <li>
    연결 작업 증명 조회 시 모든 DB 호출에 대해 목업 환경을 구성하기보다 Airflow가 테스트를 실행하는 동안 쿼리할 수 있는 <strong>실제 메타스토어</strong>를 실행하는 것이 합리적이다.
  </li>
</ul>

```python
# 1. 템플릿 변수를 사용하는 오퍼레이터 예제
class MovielensDownloadOperator(BaseOperator):
    # Airflow의 템플릿 필드 지정
    # - 여기 적힌 속성들은 {{ ds }}, {{ execution_date }} 같은 Jinja 템플릿 변
    #   수를 사용할 수 있음
    template_fields = ("_start_date", "_end_date", "_output_path")

    def __init__(
        self,
        conn_id,
        start_date,
        end_date,
        output_path,
        **kwargs,
    ):
        # 부모 클래스 초기화 (task_id, owner, retries 같은 공통 파라미터 처리)
        super().__init__(**kwargs)
        self._conn_id = conn_id
        self._start_date = start_date
        self._end_date = end_date
        self._output_path = output_path

    def execute(self, context):
        # Hook을 이용해 Movielens에 연결 → 평점 데이터 가져오기
        with MovielensHook(self._conn_id) as hook:
            ratings = hook.get_ratings(
                start_date=self._start_date,
                end_date=self._end_date,
            )

        # 가져온 데이터를 JSON 파일로 저장
        with open(self._output_path, "w") as f:
            f.write(json.dumps(ratings))
```

```python
# 2. 테스트를 위한 기본 인수를 갖는 DAG
#   - 현재 커스텀 오퍼레이터는 단순히 execute()만 호출하는 게 아니라,
#     태스크 인스턴스(TaskInstance) context와 함께 실행되어야 한다.
#   - 그래서 테스트 시에는 operator.execute() 대신 operator.run()을 호출해야 하
#     는데, run() 메서드는 DAG와 TaskInstance 객체가 있어야 실행 가능하다.
#   - 따라서 테스트 코드 안에서도 DAG 객체를 정의해주어야 한다.
dag = DAG(
    # DAG ID (고유 식별자)
    "test_dag",
    default_args={
        "owner": "airflow",
        # DAG 시작 날짜 (과거 날짜여야 스케줄러가 실행 가능)
        "start_date": datetime.datetime(2019, 1, 1),
    },
    # DAG 실행 주기 (매일 1회 실행)
    schedule_interval="@daily"
)
```

```python
# 3. 태스크를 정의하고 실행
#   - 템플릿 변수를 할당하기 위해 DAG 안에서 테스트를 수행해야 한다.
#   - Operator.execute()는 context 없이 직접 호출할 수 있지만,
#     run()은 DAG + TaskInstance context가 필요하다.
def test_movielens_operator(tmp_path, mocker):
    mocker.patch.object(
        # - MovielensHook.get_connection을 mock 처리
        # - 실제 DB 연결이 아니라, 지정된 Connection 객체를 반환하도록 대체
        MovielensHook,
        "get_connection",
        return_value=Connection(
            conn_id="test", login="airflow", password="airflow"
        ),
    )

    # - 테스트용 DAG 정의
    # - run() 메서드 실행을 위해 DAG 객체가 반드시 필요하다.
    dag = DAG(
        "test_dag",
        default_args={
            "owner": "airflow",
            "start_date": datetime.datetime(2019, 1, 1),
        },
        schedule_interval="@daily",
    )

    # - MovielensDownloadOperator 태스크 정의
    # - 템플릿 변수(prev_ds, ds)를 활용하여 동적으로 날짜가 치환됨
    task = MovielensDownloadOperator(
        task_id="test",
        # DAG Connection ID
        conn_id="testconn",
        # 이전 실행일
        start_date="{{ prev_ds }}",
        # 현재 실행일
        end_date="{{ ds }}",
        # 실행일 기준으로 파일명 생성
        output_path=str(tmp_path / "{{ ds }}.json"),
        dag=dag
    )

    # - 태스크 실행
    # - run() 메서드는 DAG, TaskInstance, context를 모두 준비해서 Operator 실행
    task.run(
        # 실행 시작일
        start_date=dag.default_args["start_date"],
        # 실행 종료일
        end_date=dag.default._args["start_date"],
    )
```

```python
# 4. 테스트 전체에서 DAG를 재사용하기 위한 pytest 픽처스 예.
import datetime
import pytest
from airflow.models import DAG

# pytest의 fixture 정의
# - test 함수에서 매개변수로 'test_dag'를 쓰면 아래 함수가 먼저 실행되어 DAG 객체를 
#    리턴해 준다.
@pytest.fixture
def test_dag():
    # 테스트 전용 DAG 객체 생성 및 반환
    return DAG(
        "test_dag",
        default_args={
            "owner": "airflow",
            "start_date": datetime.datetime(2019, 1, 1),
        },
        schedule_interval="@daily",
    )
```

```python
# 5. 테스트에 픽처스를 포함하여 필요한 객체 생성.
def test_movielens_operator(tmp_path, mocker, test_dag):
    # - MovielensHook.get_connection을 mock 처리
    #   → 실제 DB/API 연결 대신 Connection(...) 객체를 반환하도록 설정
    mocker.patch.object(
        MovielensHook,
        "get_connection",
        return_value=Connection(
            conn_id="test",
            login="airflow",
            password="airflow",
        ),
    )

    # - MovielensDownloadOperator 태스크 생성
    #    - Jinja 템플릿 변수를 start_date/end_date/output_path에 포함시킴
    #    - 실행 시점에서 {{ ds }}, {{ prev_ds }} 값으로 치환됨
    #    - dag=test_dag : fixture로 제공된 DAG 객체에 태스크를 소속시킴
    task = MovielensDownloadOperator(
        task_id="test",
        conn_id="testconn",
        start_date="{{ prev_ds }}",
        end_date="{{ ds }}",
        output_path=str(tmp_path / "{{ ds }}.json"),
        dag=test_dag,
    )

    # - 태스크 실행
    #     - run()은 DAG/TaskInstance context가 필요하므로 start_date, 
    #       end_date 지정
    task.run(
        start_date=dag.default_args["start_date"],
        end_date=dag.default_args["start_date"],
    )
```

<br>

<h2>2-1. 외부 시스템 작업</h2>
<ul>
  <li>
    MovieLens 평점을 읽고 결과를 Postgres DB에 저장하는 MovielensToPostgresoeprator와 같은 오퍼레이터를 실습한다.
  </li>
</ul>

```python
# 1. PostgreSQL DB에 연결하는 오퍼레이터 예제.
# PostgreSQL 연결용 Hook
from airflow.hooks.postgres_hook import PostgresHook
# 모든 Operator의 기본 클래스
from airflow.models import BaseOperator

# Movielens 데이터 소스용 Hook
from airflowbook.hooks.movielens_hook import MovielensHook

class MovielensToPostgresOperator(BaseOperator):
    # Airflow에서 템플릿 변수로 사용할 수 있는 필드 정의
    # → {{ ds }}, {{ prev_ds }} 등의 Jinja 템플릿이 여기에 자동 치환됨
    template_fields = ("_start_date", "_end_date", "_insert_query")

    def __init__(self,
        movielens_conn_id,  # Movielens 데이터베이스/API 연결 ID
        start_date,         # 조회 시작일 (템플릿 가능)
        end_date,           # 조회 종료일 (템플릿 가능)
        postgres_conn_id,   # PostgreSQL 연결 ID
        insert_query,       # INSERT SQL 템플릿
        **kwargs,           # BaseOperator 공통 인자 (task_id, retries 등)
    ):  

        # 부모 클래스 초기화 (task_id, owner, retries 등의 속성 등록)
        super().__init__(**kwargs)
        # 인스턴스 변수 저장
        self._movielens_conn_id = movielens_conn_id
        self._start_date = start_date
        self._end_date = end_date
        self._postgres_conn_id = postgres_conn_id
        self._insert_query = insert_query

    def execute(self, context):
        # - MovielensHook을 이용해 데이터 소스 연결
        # - 지정된 기간(start_date~end_date)의 평점 데이터를 가져옴
        with MovielensHook(self._movielens_conn_id) as movielens_hook:
            ratings = list(movielens_hook.get_ratings(
                start_date=self._start_date,
                end_date=self._end_date),
            )
        
        # - PostgreSQL 연결을 위한 Hook 생성
        postgres_hook = PostgresHook(
            postgres_conn_id=self._postgres_conn_id
        )

        # - 각 rating 레코드를 SQL INSERT 문으로 변환
        #    - rating은 dict 형태이므로 key 기준으로 정렬 후 value만 추출
        #    - INSERT 쿼리 템플릿에 값 목록을 format으로 삽입
        insert_queries = [
            self._insert_query.format(
                ",".join([str(_[1]) for _ in sorted(rating.items())])
            ) for rating in ratings
        ]

        # - 변환된 INSERT 쿼리들을 PostgreSQL에 실행
        postgres_hook.run(insert_queries)
```

```python
# 2. Docker의 DB로 저장하는 실습.
#   - Docker를 사용하기 위해 환경 설정.
from pytest_docker_tools import fetch

# fetch 함수는 docker pull을 트리거하고 이미지를 반환한다.
#     - fetch 함수는 pytest 픽스처 이기에 직접 호출은 불가하고 테슽트에 매개변수로 
#       전달된다.
postgres_image = fetch(repository="postgres:11.1-alpine")
```

```python
# 3. 테스트 시에 docker 이미지를 출력하여 확인.
from pytest_docker_tools import fetch

postgres_image = fetch(repository="postgres:11.1-alpine")

def test_call_fixture(postgres_image):
    print(postgres_image.id)
```

```python
# 4. 이전에 얻은 이미지 ID를 사용하여 postgres 컨테이너 구성 및 시작.
from pytest_docker_tools import container

postgres_container = container(
    image="{postgres_image.id}"
    ports={"5432/tcp": None}
)

def test_call_fixture(postgres_container):
    print(
        f"Running Postgres container named {postgres_container.name}"
        f"on port {postgres_container.ports['5432/tcp'][0]}."
    )
```

```python
# 4. Postgres 컨테이너 초기화.

# 1) 테스트용 Postgres Docker 이미지 불러오기
#    - repository="postgres:11.1-alpine" 이미지를 로컬에 pull 받음
#    - fetch() 함수는 지정된 이미지를 가져오고, 결과로 image 객체를 반환
postgres_image = fetch(repository="postgres:11.1-alpine")

# 2) Postgres 컨테이너 생성
postgres = container(
    # 사용할 이미지 ID 지정 (위에서 fetch한 postgres_image의 ID)
    image="{postgres_image.id}",

    # 3) 환경 변수 설정
    #    - POSTGRES_USER, POSTGRES_PASSWORD: 컨테이너 초기 사용자 정보
    environment={
        "POSTGRES_USER": "testuser",
        "POSTGRES_PASSWORD": "testpass",
    },

     # 4) 포트 매핑
    #    - "5432/tcp": None → 호스트에서 임의의 사용 가능한 포트를 자동 할당
    ports={"5432/tcp": None},
    
    # 5) SQL 초기화 스크립트 마운트
    #    - tests 디렉토리 내의 postgres-init.sql 파일을 컨테이너 내부 경로에 연결
    #    - Postgres 컨테이너는 /docker-entrypoint-initdb.d/ 아래의 SQL 파일을 자동 실행
    volumes={
        os.path.join(os.path.dirname(__file__), "postgres-init.sql"): {
            "bind": "/docker-entrypoint-initdb.d/postgres-init.sql"
        }
    },
)
```

```sql
-- 5. 테스트 데이터베이스 스키마 초기화.
SET SCHEMA 'public';
CREATE TABLE movielens(
    movieId integer,
    rating float,
    ratingTimestamp integer,
    userId integer,
    scrapeTime timestamp
);
```

```python
# 6. 외부 시스템 테스트를 위해 도커 컨테이너를 사용하여 테스트 구성 완료.
import os

import pytest
from airflow.models import Connection
from pytest_docker_tools import fetch, container

from airflowbook.operators.movielens_operator import (
    MovielensHook,
    MOvielensToPostgresOperator,
    PostgresHook,
)

# 1) PostgreSQL Docker 이미지 및 컨테이너 설정
postgres_image = fetch(repository="postgres:11.1-alpine")
postgres=container(
    image="{postgres_image.id}",
    environment={
        "POSTGRES_USER": "testuser",
        "POSTGRES_PASSWORD": "testpass",
    },
    # 테스트 시 자동 포트 할당
    ports={"5432/tcp": None},
    volumes={
        os.path.join(os.path.dirname(__file__), "postgres-init.sql"): {
            "bind": "/docker-entrypoint-initdb.d/postgres-init.sql"
        }
    },
)

def test_movielens_to_postgres_operator(mocker, test_dag, postgres):
    # 2) MovielensHook 연결 Mock 처리 (실제 외부 API 호출 방지)
    mocker.patch.object(
        MovielensHook,
        "get_connection",
        return_value=Connection(
            conn_id="test",
            login="airflow",
            password="airflow",
        ),
    )

    # 3) PostgresHook 연결 Mock 처리 (도커 컨테이너 내부 DB로 연결)
    mocker.patch.object(
        PostgresHook,
        "get_connection",
        return_value=Connection(
            conn_id="postgres",
            conn_type="postgres",
            host="localhost",
            login="testuser",
            password="testpass",
            # 실제 매핑된 포트 사용
            port=postgres.ports["5432/tcp"][0],
        ),
    )

    # 4) Movielens → Postgres 오퍼레이터 생성
    task = MovielensToPostgresOperator(
        task_id="test",
        movielens_conn_id="movielens_id",
        start_date="{{ prev_ds }}",
        end_date="{{ ds }}",
        postgres_conn_id="postgres_id",
        insert_query=(
            "INSERT INTO movielens"
            "(movieId, rating, ratingTimestamp, userId, scrapeTime)"
            "VALUES ({0}, '{{ macros.datetime.now() }}')"
        ),
        dag=test_dag,
    )

    # 5) PostgresHook 인스턴스 생성 후 초기 상태 확인
    pg_hook = PostgresHook()

    row_count = pg_hook.get_first("SELECT COUNT(*) FROM movielens")[0]
    assert row_count == 0

    # 6) 오퍼레이터 실행 (Movielens → Postgres 적재)
    task.run(
        start_date=test_dag.default_args["start_date"],
        end_date=test_dag.default_args["start_date"],
    )

    # 7) 실행 후 데이터 적재 여부 확인
    row_count = pg_hook.get_first("SELECT COUNT(*) FROM movielens")[0]
    assert row_count > 0
```

<br><br>

<h1>3. 개발을 위해 테스트 사용하기</h1>
<ul>
  <li>
    테스트는 코드가 정상적인 동작을 하는지 확인하느 것 뿐만 아니라 <strong>운영 시스템</strong>을 사용하지 않고도 작은 코드를 실행할 수 있기에 유용하다.
  </li>
    <ul>
      <li>
        예를 들어 mocking을 하기 때문에 PyCharm 디버거 등을 사용하여 코드를 한 줄씩 확인해 볼 수도 있다.
      </li>
    </ul>
</ul>

<br>

<h2>3-1. DAG 완료 테스트하기</h2>
<ul>
  <li>
    DAG의 모든 오퍼레이터가 예상한 대로 작동하는 것을 확인하는 것은 개발 환경상 어렵다. 몇 가지 대안을 바로 뒤에서 알아본다.
  </li>
</ul>

<br><br>

<h1>4. Whirl을 이용한 프로덕션 환경 에뮬레이션</h1>
<ul>
  <li>
    <strong>Whirl</strong>은 <strong>도커 컨테이너</strong>에서 운영 환경의 모든 구성 요소를 시뮬레이션하고, <strong>도커 컴포즈</strong>로 모든 구성 요소를 관리하는 것이다.
  </li>
    <ul>
      <li>
        단, 모든 이미지가 제공되는 것은 아니기에 몇 가지 환경에서는 제한될 수 있다.
      </li>
    </ul>
</ul>

<br><br>

<h1>5. DTAP 환경 세팅하기</h1>
<ul>
  <li>
    Whirl과 docker를 이용한 방식은 보안의 이유로 제한되는 경우가 존재한다. 이떄에는 <strong>DTAP</strong>를 제안할 수 있다.
  </li>
    <ul>
      <li>
        DTAP란 development, test, acceptance 그리고 production을 의미한다.
      </li>
    </ul>
  <li>
    Airflow 프로젝트 배경에서는 GitHub 저장소에서 개발 환경 > 개발 브랜치, 프로덕션 환경 > 프로젝션/메인 등 <strong>하나의 전용 브랜치</strong>를 만드는 것을 권장한다.
  </li>
</ul>