<ul>
  <li>
    Airflow 오퍼레이터로 Airflow 외부의 시스템에서 태스크를 수행하기.
  </li>
  <li>
    외부 시스템에 특화된 오퍼레이터 적용하기.
  </li>
  <li>
    Airflow에서 오퍼레이터를 구현하여 A-to-B 작업을 수행하기.
  </li>
  <li>
    외부 시스템에 연결하는 태스크 테스트하기.
  </li>
</ul>

<br>

<h1>1. 클라우드 서비스에 연결하기</h1>
<ul>
  <li>
    오늘날 대부분의 소프트웨어는 <strong>클라우드 서비스</strong>에서 실행되며 클라우드 공급자가 제공하는 <strong>API</strong>를 통해 제어할 수 있다.
  </li>
  <li>
    Airflow가 프로그래머에게 제공하는 인터페이스는 <strong>Operator</strong>이다.
  </li>
    <ul>
      <li>
        Operator는 내부적으로 클라우드의 SDK를 사용해 요청며 프로그래머에게는 클라우드 SDK를 감싼 레이어를 제공한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-1. 추가 의존성 패키지 설치하기</h2>
<ul>
  <li>
    apache-airflow 파이썬 패키지는 몇 가지 필수 오퍼레이터가 포함되지만 클라우드 서비스 연결을 위한 컴포넌트는 없기에 다운 받아야 한다.
  </li>
    <ul>
      <li>
        <strong>AWS</strong>: pip install apache-airflow-providers-amazon
      </li>
      <li>
        <strong>GCP</strong>: pip install apache-airflow-providers-google
      </li>
      <li>
        <strong>Azure</strong>: pip install apache-airflow-providers-azure
      </li>
    </ul>
  <li>
    클라우드 뿐만 아니라 PostgresOperator 등을 실행하기 위한 외부 서비스에도 패키지 설치는 필요하다.
  </li>
</ul>

```python
# 1. AWS에서 태스크 수행을 위한 오퍼레이터 살펴보기.
#   - S3CopyObjectOperator: 특정 버킷의 오브젝트를 다른 곳으로 복사한다.
#   - AWS의 boto3 클라이언트에 대해 자세히 알 필요 없이 S3의 오브젝트르 다른 위치에 복사하
#     는 간단한 방법을 제공한다.
from airflow.providers.amazon.aws.operators.s3_copy_object import S3CopyObjectOperator

S3CopyObjectOperator(
    task_id="...",
    # 복사할 버킷 이름
    source_bucket_name="datebucket",
    # 복사할 오브젝트 이름
    source_bucket_key="/data/{{ ds }}.json",
    # 복사될 버킷 이름
    dest_bucket_name="backupbucket",
    # 대상 오브젝트 이름
    dest_bucket_key="/data/{{ ds }}-backup.json",
)
```

<br>

<h2>1-2. 머신러닝 모델 개발하기</h2>
<ul>
  <li>
    모델에는 온라인과 오프라인 두 부분이 있다.
  </li>
    <ul>
      <li>
        <strong>온라인</strong>: <strong>모델을 로드</strong>하고 이전에 학습하지 않닸던 데이터를 숫자로 분류한다.
      </li>
      <li>
        <strong>오프라인</strong>: 손으로 쓴 숫자의 큰 데이터 세트를 가져와 <strong>학습</strong>시킨다.
      </li>
    </ul>
  <li>
    Airflow는 일반적으로 오프라인 작업에 적합하다.
  </li>
  <li>
    외부 서비스를 사용할 때, Airflow 내부의 복잡성 때문보다 파이프라인에 <strong>다양한 컴포넌트를 정확하게 통합</strong>하는 과정에서 복잡해지는 경우가 많다.
  </li>
</ul>

```python
# 1. 손글씨 숫자 분류 모델을 학습하고 배포하기 위한 DAG
import gzip
import io
import pickle

import airflow.utils.dates
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.s3_copy_object import S3CopyObjectOperator
from airflow.providers.amazon.aws.operators.sagemaker_endpoint import (
    SageMakerEndpointOperator,
)
from airflow.providers.amazon.aws.operators.sagemaker_training import (
    SageMakerTrainingOperator,
)
from sagemaker.amazon.common import write_numpy_to_dense_tensor

dag = DAG(
    dag_id="chapter7_aws_handwritten_digits_classifier",
    schedule_interval=None,
    start_date=airflow.utils.dates.days_ago(3),
)

# 두 S3 버킷 간에 데이터 복사하기
#     - S3CopyObjectOperator는 소스와 타겟 버킷 이름, 오브젝트 이름을 주면 선택한 오브젝
#       트를 대신 복사해 준다.
download_mnist_data = S3CopyObjectOperator(
    task_id="download_mnist_data",
    source_bucket_name="sagemaker-sample-data-eu-west-1",
    source_bucket_key="algorithms/kmeans/mnist/mnist.pkl.gz",
    dest_bucket_name="[your-bucket]",
    dest_bucket_key="mnist.pkl.gz",
    dag=dag,
)

def _extract_mnist_data():
    s3hook = S3Hook()

    # 메모리로 S3 데이터 세트 다운로드.
    mnist_buffer = io.BytesIO()
    mnist_obj = s3hook.get_key(
        bucket_name="[your-bucket]",
        key="mnist.pkl.gz",
    )
    mnist_obj.download_fileobj(mnist_buffer)

    # gzip 파일의 압축을 풀고 데이터 세트를 추출, 변환 후 S3로 다시 데이터를 업로드.
    mnist_buffer.seek(0)
    with gzip.GzipFile(fileobj=mnist_buffer, mode="rb") as f:
        train_set, _, _ = pickle.loads(f.read(), encoding="latin1")
        output_buffer = io.BytesIO()
        write_numpy_to_dense_tensor(
            file=output_buffer,
            array=train_set[0],
            labels=train_set[1],
        )
        output_buffer.seek(0)
        s3hook.load_file_obj(
            output_buffer,
            key="mnist_data",
            bucket_name="[your-bucket]",
            replace=True,
        )

extract_mnist_data = PythonOperator(
    task_id="extract_mnist_data",
    python_callable=_extract_mnist_data,
    dag=dag,
)

sagemaker_train_model = SageMakerTrainingOperator(
    task_id="sagemaker_train_model",
    config={
        "TrainingJobName": (
            "mnistclassifier-"
            "{{ execution_date.strftime('%Y%m-%d-%H-%M-%S') }}"
        ),
        "AlgorithmSpecification": {
            "TrainingImage": "438346466558.dkr.ecr.eu-west-1.amazonaws.com/kmeans:1",
            "TrainingInputMode": "File",
        },
        "HyperParameters": {"k": "10", "feature_dim": "784"},
        "InputDataConfig": [
            {
                "ChannelName": "train",
                "DataSource": {
                    "S3DataSource": {
                        "S3DataType": "S3Prefix",
                        "S3Uri": "s3://[your-bucket]/mnist_data",
                        "S3DataDistributionType": "FullyReplicated",
                    }
                },
            }
        ],
        "OutputDataConfig": {
            "S3OutputPath": "s3://[your-bucket]/mnistclassifier-output"
        },
        "ResourceConfig": {
            "InstanceType": "ml.c4.xlarge",
            "InstanceCount": 1,
            "VolumeSizeInGB": 10,
        },
        "RoleArn": (
            "arn:aws:iam::297623009465:role/service-role/"
            "AmazonSageMaker-ExecutionRole-20180905T153196"
        ),
        "StoppingCondition": {"MaxRuntimeInSeconds": 24 * 60 * 60},
    },
    wait_for_completion=True,
    print_log=True,
    check_interval=10,
    dag=dag,
)

sagemaker_deploy_model = SageMakerEndpointOperator(
    task_id="sagemaker_deploy_model",
    wait_for_completion=True,
    config={
        "Model": {
            "ModelName": (
                "mnistclassifier-"
                "{{ execution_date.strftime('%Y%m-%d-%H-%M-%S') }}"
            ),
            # 모델 아티팩트 위치 예: "PrimaryContainer": {"Image": "...", "ModelDataUrl": "s3://.../output/model.tar.gz"}
            # 필요 시 위 필드를 추가해야 합니다.
        },
        "ExecutionRoleArn": (
            "arn:aws:iam::297623009465:role/service-role/"
            "AmazonSageMaker-ExecutionRole-20180905T153196"
        ),
        "EndpointConfig": {
            "EndpointConfigName": (
                "mnistclassifier-"
                "{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}"
            ),
            "ProductionVariants": [
                {
                    "InitialInstanceCount": 1,
                    "InstanceType": "ml.t2.medium",
                    "ModelName": "mnistclassifier",
                    "VariantName": "AllTraffic",
                }
            ],
        },
        "Endpoint": {
            "EndpointConfigName": (
                "mnistclassifier-"
                "{{ execution_date.strftime('%Y-%m-%d-%H-%M-%S') }}"
            ),
            "EndpointName": "mnistclassifier",
        },
    },
    dag=dag,
)

download_mnist_data >> extract_mnist_data >> sagemaker_train_model >> sagemaker_deploy_model
```

<br>

<h2>1-3. 외부 시스템을 사용하여 개발하기</h2>

```bash
# 1. 로컬에서 AWS 오퍼레이터를 테스트하기 위해 설정하기.
# Add secrets in ~/.aws/credentials:
#     [myaws]
#     aws_access_key_id=AKIAEXAMPLE123456789
#     aws_secret_access_key=supersecretaccesskeydonotshare!123456789

# boto3 클라이언트 인증을 위해 AWS_PROFILE 환경 변수 설정.
export AWS_PROFILE=myaws
export AWS_DEFAULT_REGION=eu-west-1
# Airflow의 로그 등을 저장하는 위치 환경 변수 설정.
#     - 디렉터리 내의 /dags 디렉터리 검색.
export AIRFLOW_HOME=[your project dir]
# 로컬 Airflow 메타스토어 초기화
#     - 로그 저장 목적.
airflow db init
# 단일 태스크 실행.
airflow tasks test chatper7_aws_handwritten_digits_classifier \
download_mnist_data 2020-01-01

# download_mnist_data 태스크크 실행하고 로그를 표시한다.
```

```python
# 2. SageMaker KMeans 모델을 위해 MNIST 데이터를 RecordIO 형식으로 변환
import gzip
import io
import pickle

from airflow.operators.python import PythonOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from sagemaker.amazon.common import write_numpy_to_dense_tensor

def _extract_mnist_data():
    # S3와 통신하기 위해 S3Hook을 초기화
    s3hook = S3Hook()

    # S3의 데이터 세트를 메모리에 다운로드
    mnist_buffer = io.BytesIO()
    # B메모리 내 바이너리 스트림으로 데이터 다운로드
    mnist_obj = s3hook.get_key(
        bucket_name="your-bucket",
        key="mnist.pkl.gz",
    )
    mnist_obj.download_fileobj(mnist_buffer)

    # gzip 파일의 압축을 풀고 데이터 세트를 추출, 변환 후 S3로 다시 데이터를 업로드
    mnist_buffer.seek(0)
    # 압축 해제 및 피클링 해제
    with gzip.GzipFile(fileobj=mnist_buffer, mode="rb") as f:
        train_set, _, _ = pickle.loads(f.read(), encoding="latin1")
        output_buffer = io.BytesIO()
        # Numpy 배열을 RecordIO 레코드 포맷으로 변환
        write_numpy_to_dense_tensor(
            file=output_buffer,
            array=train_set[0],
            labels=train_set[1],
        )
        output_buffer.seek(0)
        # S3에 결과 업로드
        s3hook.load_file_obj(
            output_buffer,
            key="mnist_data",
            bucket_name="your-bucket",
            replace=True,
        )

extract_mnist_data = PythonOperator(
    task_id="extract_mnist_data",
    python_callable=_extract_mnist_data,
    dag=dag,
)
```

```python
# 3. AWS SageMaker 모델 학습
sagemaker_train_model = SageMakerTrainOperator(
    task_id="sagemaker_train_model",
    config={
        # TrainingJobName은 AWS 계정과 region 내에서 고유해야 한다.
        "TrainingJobName": (
            # execution_Date로 템플릿을 지정하여 고유하게 만들면서 멱등성을 유지.
            # (날짜 시간이 붙어 이름이 중복되지 않기 때문).
            "mnistclassifier-"
            "{{ execution_date.strftime('%Y%m-%d-%H-%M-%S') }}"
        ),
    },
    # AWS는 작업의 완료 여부를 알 수 없기에 작업 완료까지 기다리도록 설정하는 것이 좋다.
    wait_for_completion=True,
    print_log=True,
    check_interval=10,
    dag=dag,
)

# 학습이 완료되고 배포된다.
```

```python
# 4. AWS Chalice를 사용한느 사용자용 API 예
import json
from io import BytesIO

import boto3
import numpy as np
from PIL import image
from chalice import Chalice, Response
from sagemaker.amazon.common import numpy_to_record_serializer

app = Chalice(app_name="number-classifier")

@app.route("/", methods=["POST"], content_types=["image/jpeg"])
def predict():
    """
    이 엔드포인트에 jpeg 포맷의 이미지를 제공한다.
    이 이미지는 학습 이미지와 사이즈가 동일해야 한다. (28 X 28).
    """
    # 입력 이미지를 회색조(grayscale) numpy 배열로 변환
    img = Image.open(BytesIO(app.current_request.raw_body)).convert("L")
    img_arr = np.array(img, dtype=np.float32)
    runtime = boto3.Session().client(
        service_name="sagemaker-runtime",
        ContentType="application/x-recordio-protobuf",
        Body=numpy_to_record_serializer()(img_arr.flatten()),
    )
    # Airflow DAG에서 배포한 SageMaker의 엔드포인트를 호출
    response = runtime.invoke_endpoint(
        EndpointName="mnistclassifier",
        ContentType="application/x-recorio-protobuf",
        Body="numpy_to_record_serializer()(img_arr.flatten())
    )
    # SageMaker 응답은 바이트로 반환된다.
    result = json.loads(response["Body"].read().decode("utf-8"))
    return Response(
        result, status_code=200, headers={"Content-Type": "application/json"},
    )
```

```bash
# 5. 엔드포인트에 JPEG 이미지 입력
curl --request POST \
     --url http://localhost:8080/ \
     --header 'content-type: image/jpeg' \
     --data-binary @'/path/to/image.jpeg'
```

<br><br>

<h1>2. 시스템 간 데이터 이동하기</h1>
<ul>
  <li>
    Airflow로 오케스트레이션하는 방법을 배운다.
  </li>
  <li>
    Airflow는 작업을 시작하고 관리하며 작업이 올바른 순서로 완료될 수 있도록 한다. 만약 그렇지 못한 경우 파이프라인은 실패한다.
  </li>
  <li>
    https://github.com/K9Ns/data-pipelines-with-apache-airflow/tree/main/chapter07 에는 도커를 포함하여 전체 실습 코드가 있다.
  </li>
  <li>
    Airflow 오케스트레이션에서 어려운 부분은 다양한 작업의 조각들을 올바르게 구성되도록 서로 마추어 주는 것이다. (데이터 포맷 및 자료 형식 등).
  </li>
</ul>

<br>

<h2>2-1. PostgresToS3Operator 구현하기</h2>

```python
# 1. MongoToS3Operator 구현
def execute(self, context):
    # S3Hook 인스턴스 생성 (self.s3_conn_id를 이용해 연결)
    s3_conn = S3Hook(self.s3_conn_id)

    # MongoHook는 인스턴스화하고 데이터를 쿼리하는 데 사용한다.
    results = MongoHook(self.mongo_conn_id).find(
        mongo_collection=self.mongo_collection,
        query=self.mongo_query,
        mongo_db=self.mongo_db
    )

    # 결과를 반환한다.
    docs_str = self._stringify(self.transform(results))

    # S3로 데이터 적재
    # 변환 결과를 가져오기 위해 S3Hook에서 load_string()을 호출한다.
    s3_conn.load_string(
        string_data=docs_str,
        key=self.s3_key,
        bucket_name=self.s3_bucket,
        replace=self.replace
    )

# 해당 오퍼레이터는 모든 결과를 메모리에 보관하기에 쿼리 결과 크기에 주의 해야한다.
# MongoDB → Airflow의 오퍼레이터 메모리 → AWS S3.
```

```python
# 2. S3ToSFTPOperator 구현
def execute(self, context):
    # ssh 인스턴스화
    ssh_hook = SSHHook(ssh_conn_id=self.sftp_conn_id)
    # s3 인스턴스화
    s3_hook = S3Hook(self.s3_conn_id)

    s3_client = s3_hook.get_conn()
    sftp_client = ssh_hook.get_conn().open_sftp()

    # S3에서 내려받은 파일을 임시 저장한 뒤, SFTP 서버로 업로드
    with NamedTemporaryFile("w") as f:
        # 메모리에 쌓아두지 않고 곧바로 /tmp/data.json에 기록
        #     - 디스크 용량 고려
        s3_client.download_file(self.s3_bucket, self.s3_key, f.name)
        sftp_client.put(f.name, self.sftp_path)
```

```python
# 3. PostgresToS3Operator 구현.
def execute(self, context):
    # 인수 설정을 주의해야 한다.
    #   - 인수는 서브클래스로서 사용한 여러 메서드를 상속받기 때문이다.
    postgres_hook = PostgresHook(postgres_conn_id=self._postgres_conn_id)
    s3_hook = S3Hook(aws_conn_id=self._s3_conn_id)

    # PostgreSQL DB에서 레코드 가져오기.
    results = postgres_hook.get_records(self._query)

    # S3 오브젝트에 레코드 업로드.
    s3_hook.load_string(
        string_data=str(results),
        bucket_name=self._s3_bucket,
        key=self._s3_key,
    )
```

```python
# 4. Postgres 쿼리 결과를 CSV로 메모리 내 변환 및 S3에 업로드
#   - Postgres 쿼리 결과가 튜플 type이기에 다른 곳에서 쓰기 위해 CSV로 변환한다.
def execute(self, context):
    postgres_hook = PostgresHook(postgres_conn_id=self._postgres_conn_id)
    s3_hook = S3Hook(aws_conn_id=self._s3_conn_id)

    results = postgres_hook.get_records(self.query)

    # 문자열 버퍼 생성 → 데이터 작성 → 바이너리로 변환
    #     - 버퍼는 메모리에 있으며 처리 후 파일 시스템에 남지 않아 관리가 편하다.
    #     - 단, Postgres 쿼리 출력값은 크기에 메모리를 고려해야 한다.
    data_buffer = io.StringIO()
    csv_writer = csv.writer(data_buffer, lineterminator=os.linesep)
    csv_writer.writerows(results)
    data_buffer_binary = io.BytesIO(data_buffer.getvalue().encode())
    s3_hook.load_file_obj(
        # 바이너리 모드에서는 파일과 같은 오브젝트가 필요하다.
        file_obj=data_buffer_binary,
        bucket_name=self._s3_bucket,
        key=self._s3_key,
        # 파일이 이미 있는 경우 기존 파일을 대체하여 멱등성을 보장한다.
        replace=True,
    )
```

```python
# 5. PostgresToS3Operator 실행하기
download_from_posgres = PostgresToS3Operator(
    task_id="download_from_postgres",
    postgres_conn_id="inside_airbnb",
    query="SELECT * FROM listings WHERE download_date={{ ds }}",
    s3_conn_id="s3",
    s3_bucket="inside-airbnb",
    s3_key="listing-{{ ds }}.csv",
    dag=dag
)
```

<br>

<h2>2-2. 큰 작업을 외부에서 수행하기</h2>
<ul>
  <li>
    Airflow의 역할에 대해서는 논쟁이 많지만, Airflow가 구동 중인 시스템의 모든 리소스를 소모하는 <strong>큰 작업</strong>은 직접 실행하기보다는 <strong>외부 시스템</strong>에서 수행하고 Airflow는 오케스트레이션만 담당하는 편이 낫다.
  </li>
  <li>
    <strong>Spark</strong>는 다음과 같은 방법으로 Airflow에서 작업 수행이 가능하다.
  </li>
    <ul>
      <li>
        <strong>SparkSubmitOperator</strong>
      </li>
        <ul>
          <li>
            Airflow 컨테이너 안에 <strong>spark-submit CLI</strong>가 설치돼 있다면, <strong>Spark Standalone / YARN / K8s</strong> 등에 직접 Spark 잡을 제출할 수 있는 <strong>가장 표준적인 방법</strong>.
          </li>
        </ul>
      <li>
        <strong>SSHOperator</strong>
      </li>
        <ul>
          <li>
            Spark이 설치된 원격 서버(마스터 노드)에 <strong>SSH로 접속해 spark-submit 명령</strong>을 실행하는 방식. Airflow에 <strong>Spark CLI가 없을 때</strong> 혹은 <strong>원격에서 실행</strong>하고 싶을 때 주로 사용.
          </li>
        </ul>
      <li>
        SimpleHTTPOperator
      </li>
        <ul>
          <li>
            <strong>Spark REST API</strong>(예: Spark Standalone 모드의 REST 서버, Livy, 또는 API 게이트웨이)를 호출해서 Spark 잡을 제출하는 방식. <strong>HTTP 요청</strong>으로 Spark 잡을 실행/모니터링할 수 있음.
          </li>
        </ul>
    </ul>
  <li>
    Airflow에서 특정 오퍼레이터와 작업하기 위해서 가장 중요한 것은 <strong>관련 문서</strong>를 잘 읽고 <strong>어떤 인수</strong>를 제공해야 하는지 파악하는 것이다.
  </li>
  <li>
    Airflow는 <strong>도커 컨테이너 시작</strong>, <strong>로그 가져오기</strong> 및 필요한 경우 <strong>삭제</strong>를 관리한다. 핵심은 <strong>멱등성 보장</strong>과 <strong>불필요한 잔여물</strong>을 남기지 않는 것이다.
  <li>
</ul>

```python
# 1. DockerOperator로 도커 컨테이너 실행
#     - Docker 데몬(Docker desktop 등)이 떠 있다면 Airflow가 컨테이너를 띄울 수 있다.
crunch_numbers = DockerOperator(
    task_id="crunch_numbers",
    image="airflowbook/numbercruncher",
    api_version="auto",
    # 완료 후 컨테이너 삭제
    auto_remove=True,
    docker_url="unix://var/run/docker.sock",
    # http://localhost를 통해 호스트 컴퓨터의 다른 서비스에 연결하려면 호스트 네트워크 모
    # 드를 사용하여 호스트 네트워크 네임 스페이스를 공유해야 한다.
    network_mode="host",
    environment={
        # host로 초기화된 네트워크 모드를 통해 localhost로 접속.
        "S3_ENDPOINT": "localhost:9000",
        "S3_ACCESS_KEY": "[insert access key]",
        "S3_SECRET_KEY": "[insert secret key]",
    },
    dag=dag,
)
```