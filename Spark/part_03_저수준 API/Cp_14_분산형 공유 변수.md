```bash
docker run --rm -it \
  -v /mnt/c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/opt/spark-data \
  apache/spark:3.5.2 \
  /opt/spark/bin/pyspark \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.xerial:sqlite-jdbc:3.45.3.0
```

<ul>
  <li>
    Spark 저수준 API에는 RDD 외에도 <strong>분산형 공유 변수</strong>가 있다.
  </li>
  <li>
    분산형 공유 변수에는 <strong>broadcast 변수</strong>와 <strong>accumulator</strong> 두 개의 타입이 있다.
  </li>
    <ul>
      <li>
        클러스터에서 실행할 때 <strong>특별한 속성을 갖는 사용자 정의 함수</strong>에서 변수들을 사용할 수 있다.
      </li>
    </ul>
  <li>
    <strong>Accumulator</strong>를 사용하면 <strong>모든 태스크의 데이터</strong>를 <strong>공유 결과</strong>에 추가할 수 있다.
  </li>
  <li>
    <strong>Broadcast</strong>를 사용하면 <strong>모든 워커 노드에 큰 값</strong>을 저장하기에 <strong>재전송이 없는 많은 Spark 액션</strong>에서 <strong>재사용</strong>할 수 있다.
  </li>
</ul>

<br><br>

<h1>브로드캐스트 변수</h1>
<ul>
  <li>
    <strong>Broadcast 변수</strong>는 <strong>변하지 않는 값(불변성 값)</strong>을 클로저(closure) 함수에 변수로 캡슐화하지 않고 <strong>클러스터에서 효율적으로 공유</strong>하는 방법을 제공한다.
  </li>
  <li>
    <strong>기존의 방식</strong>은 태스크에서 드라이버 노드의 변수를 사용할 때 <strong>closure 함수 내부</strong>에서 <strong>단순히 참조</strong>하는 방법을 사용한다.
  </li>
    <ul>
      <li>
        위와 같은 기존 방식은 워커 노드에서 <strong>closure 함수에서 변수를 사용</strong>할 때 <strong>여러 번(태스크당) 역직렬화</strong>가 발생하기 때문에 비효율적이다.
      </li>
      <li>
        여러 Spark action과 job에서 동일한 변수를 사용하면 <strong>job을 실행할 때마다</strong> 워커로 <strong>큰 변수를 재전송</strong>하게 된다.
      </li>
    </ul>
  <li>
    <strong>Broadcast 변수</strong>는 모든 태스크마다 직렬화를 하지 않고 <strong>클러스터의 모든 머신에 캐시</strong>하는 <strong>불변성 공유 변수</strong>이다.
  </li>
    <ul>
      <li>
        익스큐터 메모리 크기에 맞는 <strong>조회용 테이블(broadcast 변수)</strong>을 전달하고 함수에서 사용한다.
      </li>
    </ul>
  <li>
    Broadcast 변수를 사용한 방식과 closure에 담아 전달하는 방식의 차이는 <strong>broadcast가 더 효율적</strong>이라는 것이다.
  </li>
    <ul>
      <li>
        작은 데이터는 작은 클러스터에서 실행 시 차이가 거의 없지만, <strong>큰 데이터</strong>는 broadcast 변수(예: 딕셔너리)로 사용하면 데이터를 <strong>한 번만 직렬화·전송</strong>하여 Executor에 캐시하므로 성능상 이점이 있다
      </li>
      <li>
        RDD 영역에서 broadcast 변수를 사용하고 UDF 혹은 Dataset에서도 사용할 수 있으며 동일한 효과를 얻을 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. 실습에 필요한 데이터 생성.

# 문자열을 정의하고 공백 기준으로 나누어 리스트로 초기화
my_collection = "Spark The Definitive Guide : Big Data Processing Made Simple"\
    .split(" ")

# 리스트를 RDD로 변환하고 2개의 파티션으로 분할
words = spark.sparkContext.parallelize(my_collection, 2)
```

```python
# 2. 구조체를 통해 사용할 메모리를 단어 목록과 추가.
supplementalData = {"Spark":1000, "Definitive":200,
                    "Big":-300, "Simple":100}
```

```python
# 3. 2번에서 초기화한 구초체를 참조하여 broadcast
#   - 해당 값은 불변성이다.
#   - action을 실행할 때 클러스터의 모든 노드에 지연 처리 방식으로 복제된다.
suppBroadcast = spark.sparkContext.broadcast(supplementalData)
```

```python
# 4. value 메서드를 통해 broadcast된 supplementalData 참조
#   - value 메서드는 직렬화된 함수에서 broadcast된 데이터를 직렬화하지 않아도 접근할 수 
#     있다.
#   - Spark는 broadcast를 통해 데이터를 전송한다.
#     → 직렬화와 역직렬화에 대한 부하기 크게 줄어든다.
suppBroadcast.value
```

```python
# 5. Broadcast된 데이터를 사용해 RDD를 변환.
#   - map 연산 처리 과정에 따라 key-value 쌍 데이터를 생성한다.
#   - 값이 빈 경우 0으로 치환한다.
words.map(lambda word: (word, suppBroadcast.value.get(word, 0)))\
    .sortBy(lambda wordPair: wordPair[1])\
    .collect()
```

<br><br>

<h1>2. 어큐뮬레이터</h1>
<ul>
  <li>
    <strong>Accumulator</strong>는 Spark의 두 번째 공유 변수 타입이며 <strong>transformation 내부의 다양한 값을 갱신</strong>하는데 사용한다.
  </li>
  <li>
    Accumulator는 Spark 클러스에서 <strong>row 단위</strong>로 <strong>안전하게 값을 갱신</strong>할 수 있는 <strong>변경 가능한 변수</strong>를 제공하며 <strong>디버깅용</strong>이나 <strong>저수준 집계 생성용</strong>으로 사용할 수 있다.
  </li>
  <li>
    Accumulator는 <strong>가환성</strong>과 <strong>결합성</strong>을 갖는 연산을 통해서만 더할 수 있는 변수이다. 따라서 <strong>병렬 처리 과정</strong>에 효율적으로 사용할 수 있다.
  </li>
    <ul>
      <li>
        <strong>가환성</strong>: 연산의 순서를 바꾸어도 결과가 변하지 않는 성질.
      </li>
      <li>
        <strong>결합성</strong>: 연산을 묶는 방식(괄호 위치)을 바꾸어도 결과가 변하지 않는 성질.
      </li>
      <li>
        따라서 <strong>카운터나 합계</strong>를 구하는 용도로 사용할 수 있다.
      </li>
    </ul>
  <li>
    Spark는 기본적으로 <strong>수치형 accumulator</strong>를 지원하며 <strong>사용자 정의 accumulator</strong>를 만들어 사용할 수도 있다.
  </li>
  <li>
    Accumulator의 값은 <strong>action</strong>을 처리하는 과정에서만 갱신된다.
  </li>
    <ul>
      <li>
        Spark는 각 테스크에서 accumulator를 <strong>한 번만 갱신</strong>하도록 제어한다. 따라서 <strong>재시작한 태스크</strong>에서는 accumulator 값을 <strong>갱신할 수 없다</strong>.
      </li>
      <li>
        transformation에서 태스크나 잡 스테이지를 재처리하는 경우 각 태스크의 <strong>갱신 작업이 두 번이상</strong> 적용될 수 있다.
      </li>
    </ul>
  <li>
    Accumulator는 Spark의 <strong>지연 연산 모델</strong>에 영향을 주지 않는다.
  </li>
    <ul>
      <li>
        Accumulator가 RDD 처리 중에 갱신되면 <strong>RDD 연산이 수행된 시점(action)</strong>에 딱 한 번만 값을 갱신한다.
      </li>
      <li>
        따라서 map 함수 같은 <strong>지연 처리 형태의 transformation</strong>에서 <strong>accumulator 갱신 작업</strong>을 수행하는 경우 <strong>실제 실행 전</strong>까지 accumulator가 갱신되지 않는다.
      </li>
    </ul>
  <li>
    Accumulator의 <strong>이름</strong>은 선택적으로 지정할 수 있다. <strong>이름이 지정된(named) accumulator</strong>의 결과는 <strong>Spark UI</strong>에 표시된다. (이름이 지정되지 않은 경우는 X).
  </li>
  <li>
    <strong>Spark UI</strong>에서 <strong>익스큐터 단위</strong>로 <strong>accumulator 값</strong>을 확인할 수 있다.
  </li>
</ul>

<br>

<h2>2-1. 기본 예제</h2>

```python
# 1. Dataset API를 사용하여 항공운항 데이터셋 사용자 정의 집계를 수행.
# 필요한 데이터 불러오기
flights = spark.read\
    .parquet("/opt/spark-data/data/flight-data/parquet/2010-summary.parquet")
```

```python
# 2. 출발지나 도착지가 중국인 항공편의 수를 구하는 accumulator 생성.
# (SQL로도 처리할 수 있지만 실습을 위해 accumulator 사용).
accChina = spark.sparkContext.accumulator(0)
```

```scala
// 3. Accumulator는 이름을 지정할 수 있다.
//    - Scala/Java에서 new LongAccumulator로 생성하면 register()로 수동 등록해야 한
//      다.
//    - sparkContext.longAccumulator("이름")으로 생성하면 자동 등록된다.
//    - PySpark에서는 Accumulator 생성 시 자동으로 SparkContext에 등록된다.
//      SparkContext에 등록한다.
val accChina = new LongAccumulator
val accChina2 = spark.sparkContext.longAccumulator("China")

spark.sparkContext.register(accChina, "China")
```

```python
# 4. 함수를 통해 accumulator 값을 더하는 방법을 정의
def accChinaFunc(flight_row):
    
    # 각 속성(컬럼)을 받는다.
    destination = flight_row["DEST_COUNTRY_NAME"]
    origin = flight_row["ORIGIN_COUNTRY_NAME"]

    # 도착지가 China인 경우 처리 (더해준다).
    if destination == "China":
        accChina.add(flight_row["count"])
    
    # 출발지가 China인 경우 처리 (더해준다).
    if origin == "China":
        accChina.add(flight_row["count"])
```

```python
# 5. foreach 메서드를 통해 각 row를 돌며 함수 적용.
#   - Spark는 action에서만 accumulator 실행을 보장하며 foreach 메서드가 action의 역할
#     을 한다.
#   - foreach 메서든 DataFrame의 각 row 마다 함수를 한 번씩 적용해 accumulator 값을 증
#     가시킨다.
flights.foreach(lambda flight_row: accChinaFunc(flight_row))
```

```python
# 6. Spark UI가 아닌 프로그래밍 방식으로 조회. (value 속성 사용).
accChina.value
```

<br>

<h2>2-2. 사용자 정의 어큐뮬레이터</h2>
<ul>
  <li>
    Accumulator를 직접 정의하려면 <strong>AccumulatorV2 클래스</strong>를 상속 받아야한다.
  </li>
  <li>
    <strong>python</strong>의 경우 <strong>AccumulatorParam</strong>을 상속받아 사용자 정의 accumulator를 생성할 수 있으며 방법은 아래와 동일하다.
  </li>
</ul>

```scala
// 1. accumulator에 짝수값만 더하는 사용자 정의 accumulator 구현.
import scala.collection.mutable.ArrayBuffer
import org.apache.spark.util.AccumulatorV2

val arr = ArrayBuffer[BigInt]()

class EvenAccumulator extends AccmulatorV2[BigInt, BigInt] {
  private var num: BigInt = 0
  def reset(): Unit = {
    this.num = 0
  }
  def add(intValue: BigInt): Unit = {
    if (intValue % 2 == 0) {
      this.num += intValue
    }
  }
  def merge(other: AccumulatorV2[BigInt, BigInt]): Unit = {
    this.num += other.value
  }
  def value():BigInt = {
    this.num
  }
  def copy(): AccumulatorV2[BigInt, BigInt] = {
    new EvenAccumulator
  }
  def isZero():Boolean = {
    this.num == 0
  }
}

val acc = new EvenAccumulator
val newAcc = sc.register(acc, "evenAcc")

acc.value
flights.foreach(flight_row => acc.add(flight_row.count))
acc.value
```