<h1>PySpark</h1>
<ul>
  <li>
    Spark 2.2 버전에서는 pip 명령을 사용해 PySpark를 설치할 수 잇다.
  </li>
</ul>

<br>

<h2>1-1. Pyspark의 근본적인 차이점</h2>
<ul>
  <li>
    구조적 API를 사용하는 경우 성능적으로 차이가 없으나 사용자 정의 함수를 하용한 경우에는 성능에 영향이 있을 수 있다.
  </li>
  <li>
    즉, 저수준 API를 사용할 경우 유연성을 얻을 수 있지만 파이썬 데이터를 Spark와 JVM에서 이해할 수 있도록 변환하고 역변환하는 과정에서 큰 비용이 생긴다.
  </li>
</ul>

<br>

<h2>1-2. Pandas 통합하기</h2>
<ul>
  <li>
    PySpark는 여러 프로그래밍 모델을 함께 사용할 수 있다는 장점이 있다.
  </li>
</ul>

```python
# 1. Spark DF를 pandas DF로 변환.
import pandas as pd

df = pd.DataFrame({"first":range(200), "second":range(50, 250)})
sparkDF = spark.createDataFrame(df)
newPDF = sparkDF.toPandas()
newPDF.head()
```

<br><br>

<h1>2. R로 스파크 사용하기</h1>
<ul>
  <li>
    R의 경우 대부분의 API가 일반적인 R과 동일하다. 단, MLlib 등은 python 보다 반영이 느리기에 추후 R로 사용할 필요성이 있을 때에만 다시 확인.
  </li>
</ul>