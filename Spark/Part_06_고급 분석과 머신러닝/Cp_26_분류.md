<h1>1. 활용 사례</h1>
<ul>
  <li>
    <strong>신용 리스크 예측</strong>
  </li>
    <ul>
      <li>
        고객에게 대출을 제공할지 여부를 결정.
      </li>
    </ul>
  <li>
    <strong>뉴스기사 분류</strong>
  </li>
    <ul>
      <li>
        뉴스키사의 주제를 예측한다.
      </li>
    </ul>
  <li>
    <strong>사용자 행위 분류</strong>
  </li>
    <ul>
      <li>
        센서에서 데이터를 수집하여 사용자 활동을 예측할 수 있다.
      </li>
    </ul>
</ul>

<br><br>

<h1>2. 분류 유형</h1>
<h2>2-1. 이진 분류</h2>
<ul>
  <li>
    이진 분류에서 예측할 수 있는 레이블의 수는 <strong>오직 두 개</strong>이다.
  </li>
</ul>

<br>

<h2>2-2. 다중 클래스 분류</h2>
<ul>
  <li>
    <strong>다중 클래스 분류(multiclass classification)</strong>이란 <strong>3개 이상</strong>의 레이블 중에서 하나의 레이블을 예측하는 알고리즘이다.
  </li>
</ul>

<br>

<h2>2-3. 다중 레이블 분류</h2>
<ul>
  <li>
    <strong>다중 레이블 분류(multilabel classification)</strong>란 주어진 입력에 대해 <strong>여러 레이블</strong>을 생성할 수 있다.
  </li>
  <li>
    다중 클래스 분류는 여러 label 중 하나로 예측하지만 다중 레이블 분류는 두 개 이상의 label이 예측 결과로 나올 수 이다. (e.g. 책의 장르가 액션인 경우와 액션-판타지인 경우).
  </li>
</ul>

<br><br>

<h1>3. MLlib의 분류 모델</h1>
<ul>
  <li>
    Spark에서는 대표적으로 다음과 같은 분류 모델을 제공한다.
  </li>
    <ul>
      <li>
        로지스틱 회귀
      </li>
      <li>
        의사결정트리
      </li>
      <li>
        랜덤 포레스트
      </li>
      <li>
        그래디언트 부스티드 트리
      </li>
    </ul>
  <li>
    Spark 2.2 기준 <strong>다중 레이블 예측</strong>을 지원하지 않는다. 다중 레이블 모델을 학습 시키려면 레이블당 하나의 모델을 학습시켜 수동으로 조합해야 한다.
  </li>
  <li>
    이번 장에서는 다음과 같은 내용을 배운다.
  </li>
    <ul>
      <li>
        모델의 기본 설명 및 배경
      </li>
      <li>
        모델 설정을 위한 하이퍼파라미터(모델을 초기화할 수 있는 다양한 방법).
      </li>
      <li>
        학습 파라미터(모델 학습에 영향을 미치는 파라미터)
      </li>
      <li>
        예측 파라미터(예측에 영향을 미치는 파라미터)
      </li>
    </ul>
</ul>

<br>

<h2>3-1. 모델 확장성</h2>
<ul>
  <li>
    어떤 모델이 적합한지 판단할 수 있는 모델 확장성 평가표를 제공한다.
  </li>
  <li>
    대부분의 모델들이 학습에 사용되는 <strong>데이터 크기</strong>에 제한이 없는데 이는 <strong>확률적 경사 하강법(stochastic gradient descent)</strong> 혹은 <strong>L-BFGS(Limited memory-BFGS)</strong>를 사용하기 때문이며 특히 방대한 데이터셋에 최적화 되어 있다.
  </li>
</ul>

```python
# 1. 모델을 테스트해 볼 데이터 불러오기.
bInput = spark.read.format("parquet").load("/workspace/Spark-The-Definitive-Guide/data/binary-classification")\
    .selectExpr("features", "cast(label as double) as label")

# 결과 확인
bInput.show(5, truncate=False)
```

<br><br>

<h1>4. 로지스틱 회귀</h1>
<ul>
  <li>
    로지스틱 회귀는 <strong>개별 입력</strong>과 <strong>특정 가중치</strong>를 결합하여 <strong>특정 클래스에 속할 확률</strong>을 얻는 선형 방법론이다.
  </li>
  <li>
    로지스틱 회귀에서 <strong>가중치</strong>는 <strong>특징의 중요성</strong>을 잘 나타내기 때문에 유용하게 사용된다.
  </li>
</ul>

<br>

<h2>4-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>family</strong>
  </li>
    <ul>
      <li>
        해당 모델의 클래스가 다항인지 이항인지를 지정한다.
      </li>
    </ul>
  <li>
    <strong>elasticNetParam</strong>
  </li>
    <ul>
      <li>
        0에서 1사이의 부동소수점수를 지정한다.
      </li>
      <li>
        엘라스틱넷 일반화에 따라 L1 일반화화 L2 일반화의 조합을 지정한다.
      </li>
        <ul>
          <li>
            <strong>L1</strong>: 특정한 특징의 <strong>가중치가 0</strong>이 되기 때문에 모델에서 희소성을 만든다.
          </li>
          <li>
            <strong>L2</strong>: 가중치가 0에 근사하지만 <strong>0은 되지 않는다</strong>.
          </li>
        </ul>
    </ul>
  <li>
    <strong>fitIntercept</strong>
  </li>
    <ul>
      <li>
        true 혹은 false를 지정하여 모델의 입력 및 가중치의 선형 조합에 <strong>추가된 절편</strong>이나 <strong>임의의 수</strong>를 적할시킬지 결정한다.
      </li>
    </ul>
  <li> 
    <strong>regParam</strong>
  </li>
    <ul>
      <li>
        <strong>0 이상</strong>의 값을 지정한다.
      </li>
      <li>
        목적 함수에서 <strong>일반화 항</strong>에 얼마만큼의 가중치를 주는지 결정한다.
      </li>
    </ul>
  <li>
    <strong>standardization</strong>
  </li>
    <ul>
      <li>
        입력을 모델로 전달하기 전 <strong>표준화 여부</strong>를 결정한다. true 혹은 false의 값을 지정한다.
      </li>
    </ul>
</ul>

<br>

<h2>4-2. 학습 파라미터</h2>
<ul>
  <li>
    <strong>maxIter</strong>
  </li>
    <ul>
      <li>
        <strong>총 학습 반복 횟수</strong>이다. 파라미터의 조정에 따라 결과가 크게 달라지지 않기에 조정할 파라미터 중 <strong>후순위</strong>로 다룬다.
      </li>
    </ul>
  <li>
    <strong>tol</strong>
  </li>
    <ul>
      <li>
        파라미터의 변경으로 인해 <strong>가중치</strong>가 충분히 최적화되었음을 나타내는 <strong>입곗값</strong>을 지정하고, <strong>반복을 중지</strong>시킬 수 있다.
      </li>
      <li>
        마찬가지로 후순위에 조정할 파라미터에 해당한다.
      </li>
    </ul>
  <li>
    <strong>weightCol</strong>
  </li>
    <ul>
      <li>
        <strong>특정 row</strong>에 가중치를 부여할 때 사용한다.
      </li>
      <li>
        특정 학습 예제가 <strong>얼마나 중요한지 나타내는 척도</strong>가 있고 <strong>그와 관련한 가중치</strong>를 갖고 있는 경우 유용하게 사용할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>4-3. 예측 파라미터</h2>
<ul>
  <li>
    <strong>threshold</strong>
  </li>
    <ul>
      <li>
        <strong>0에서 1 범위의 Double 타입</strong>의 값을 갖는다.
      </li>
      <li>
        주어진 클래스를 예측하는 <strong>확률 임곗값</strong>이다.
      </li>
    </ul>
  <li>
    <strong>thresholds</strong>
  </li>
    <ul>
      <li>
        <strong>다중 분류</strong>를 할 때 각 클래스에 대한 입곗값 배열을 지정할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>4-4. 실습 예제</h2>

```python
# 1. 로지스틱 회귀 실습.
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression()
print(lr.explainParams())
lrModel = lr.fit(bInput)
```

```python
# 2. 모델 확인.
print(lrModel.coefficients)
print(lrModel.intercept)
```

<br>

<h2>4-5. 모델 요약</h2>
<ul>
  <li>
    Spark 2.2에서는 이진 로직스틱 회귀에 대해서만 모델 요약이 가능하다.
  </li>
  <li>
    이진 로지스틱 회귀 모델 요약에는 ROD, 임곗값 기준 {f 계수, 정밀도(precision), 리콜(recall), 임곗값에 의한 호출, ROC 곡선}을 활용한 정보를 얻을 수 있다.
  </li>
</ul>

```python
# 1. 모델 요약 확인.
summary = lrModel.summary
print(summary.areaUnderROC)
summary.roc.show()
summary.pr.show()
```

```python
# 2. 모델이 최종 결과에 도출하기까지의 속도를 확인.
summary.objectiveHistory
```

<br><br>

<h1>5. 의사결정트리</h1>
<ul>
  <li>
    의사결정트리는 인간이 자주 활용하는 <strong>의사결정 모형</strong>과 매우 유사하기에 가장 친근하고 해석 가능한 모델 중 하나이다.
  </li>
  <li>
    의사결정트리는 모든 입력을 사용하여 이러한 유형의 구조를 만들어 놓고 <strong>예측할 시점에 해당 가지치기(branch)</strong>를 따른다.
  </li>
    <ul>
      <li>
        따라서 추론이나 검토가 쉽고, 데이터 구조에 대한 가정을 거의하지 않기에 유용한 초기 모델이 될 수 있다.
      </li>
    </ul>
  <li>
    함수를 모델링하기 보다 예측 시점이 되었을 때 의사결정할 커다란 트리를 만든다.
  </li>
  <li>
    다중 클래스 분류를 지원하며 최종 결과에서 두 개의 컬럼을 생성하여 <strong>예측 결과</strong> 및 <strong>확률값</strong>을 제공한다.
  </li>
  <li>
    <strong>계산 시간</strong>이 많이 소요된다는 단점이 존재하며 데이터에 대해 극단적으로 과적합될 수 있다.
  </li>
  <li>
    <strong>가지치기</strong>를 통해 모델이 너무 과적합되는 것을 방지할 수 있다.
  </li>
</ul>

<br>

<h2>5-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>macDepth</strong>
  </li>
    <ul>
      <li>
        <strong>과적합</strong>되는 것을 방지하기 위해 <strong>최대 깊이</strong>를 지정하는 것이 좋다.
      </li>
    </ul>
  <li>
    <strong>maxBins</strong>
  </li>
    <ul>
      <li>
        의사결정트리는 연속형 특징을 <strong>범주형 특징으로 변환</strong>하는데 이때 <strong>몇 개의 구간</strong>으로 범주화할지 결정한다.
      </li>
      <li>
        2 이상 데이터 셋의 범주형 변수들이 갖고 있는 범주의 수 이하의 값을 가져야 한다.
      </li>
    </ul>
  <li>
    <strong>impurity</strong>
  </li>
    <ul>
      <li>
        <strong>불순도(impurity)</strong>는 모델이 특정 말단 노드에서 <strong>분할되어야 하는지 여부를 결정</strong>하기 위한 지표를 의미한다.
      </li>
      <li>
        일반적으로 사용되는 두 가지 불순도 측정 기준인 <strong>엔트로피(entropy)</strong>와 <strong>지니(gini)</strong> 중 하나를 설정할 수 있다.
      </li>
    </ul>
  <li>
    <strong>minInfoGain</strong>
  </li>
    <ul>
      <li>
        의사결정트리 분할에 사용할 수 있는 <strong>최소 정보 획득</strong>을 결정한다.
      </li>
      <li>
        <strong>값이 클수록</strong> 과적합을 방지한다.
      </li>
      <li>
        일반적으로 다양한 값을 시뮬레이션하면서 모델을 검토하며 결정한다. 기본값은 0이다.
      </li>
    </ul>
  <li>
    <strong>minInstancePerNode</strong>
  </li>
    <ul>
      <li>
        모델이 학습해야 할 <strong>최소 인스턴스 수</strong>를 지정하여 특정 노드에서 의사결정트리의 생성을 <strong>종료</strong>하도록 한다.
      </li>
      <li>
        maxDepth와 같이 의사결정트리의 <strong>최대 깊이를 제한</strong>하는 또 다른 방법이다.
      </li>
      <li>
        요구사항이 충족될 때까지 <strong>가지치기</strong>를 진행할 수 있으며 기본 값은 1이고, <strong>값이 클수록</strong> 과적합을 방지할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>5-2. 학습 파라미터</h2>
<ul>
  <li>
    checkpointInterval
  </li>
    <ul>
      <li>
        학습 과정 동안 진행되는 모델의 <strong>작업 내용을 저장</strong>하는 방법으로 특정 노드가 다양한 이유로 충돌하는 등의 경우 <strong>이전까지 진행된 작업 내용을 복구</strong>할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>5-3. 예측 파라미터</h2>

```python
# 1. 의사결정트리 분류기 예제
from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier()
print(dt.explainParams())
dtModel = dt.fit(bInput)
```

<br><br>

<h1>6. 랜덤 포레스트와 그래디언트 부스티드 트리</h1>
<ul>
  <li>
    랜덤 포레스트와 그래디언트 부스티드 트리는 의사결정트리의 확장된 알고리즘으로 다양한 서브 데이터셋으로 <strong>여러 개의 트리</strong>를 생성한다.
  </li>
  <li>
    각 트리가 <strong>특정 도메인</strong>에 적합하도록 학습을 하고 최종에서 결합하는 <strong>집단 지성</strong>을 활용한 방법이다.
  </li>
  <li>
    <strong>랜덤 포레스트</strong>는 단순히 많은 트리를 학습시킨 다음 각 트리의 응답을 <strong>평균화</strong>하여 최종 예측을 한다.
  </li>
  <li>
    <strong>그래디언트 부스티드 트리</strong>는 개별 트리를 학습할 때 <strong>별도의 가중치</strong>가 부여된다는 차이가 있다.
  </li>
</ul>

<br>

<h2>6-1. 모델 하이퍼파라미터</h2>
<h3>6-1-1. 랜덤 포레스트에서만 사용되는 하이퍼파라미터</h3>
<ul>
  <li>
    <strong>numTrees</strong>
  </li>
    <ul>
      <li>
        학습시킬 트리 수
      </li>
    </ul>
  <li>
    <strong>featureSubsetStrategy</strong>
  </li>
    <ul>
      <li>
        분할을 할 때 <strong>고려해야 할 특징 수</strong>를 결정한다. 
      </li>
      <li>
        auto, all, sqrt, log2 그리고 n 등 다양한 값이 설정될 수 있다.
      </li>
    </ul>
</ul>

<h3>6-1-2. 그래디언트 부스티드 트리에서만 사용되는 하이퍼파라미터</h3>
<ul>
  <li>
    <strong>lossType</strong>
  </li>
    <ul>
      <li>
        <strong>손실 함수</strong>를 지정하는 하이퍼파라미터이다.
      </li>
    </ul>
  <li>
    <strong>maxIter</strong>
  </li>
    <ul>
      <li>
        총 학습 반복 횟수.
      </li>
      <li>
        우선 순위가 낮은 하이퍼파라미터이며 기본값은 100이다.
      </li>
    </ul>
  <li>
    <strong>stepSize</strong>
  </li>
    <ul>
      <li>
        알고리즘의 <strong>학습률</strong>이다. 값이 클수록 <strong>반복 학습 사이의 변화 크기</strong>가 더 커진다.
      </li>
      <li>
        <strong>우선순위가 높은</strong> 파라미터로 최적화 과정에 도움을 주는 파라미터이며 다양하게 테스트해야한다.
      </li>
    </ul>
</ul>

<br>

<h2>6-2. 학습 파라미터</h2>
<ul>
  <li>
    랜덤 포레스트와 그래디언트 부스티드 트리 모델은 학습 파라미터로 <strong>checkpointInterval</strong> 단 하나만 제공한다.
  </li>
</ul>

<br>

<h2>6-3. 예측 파라미터</h2>
<ul>
  <li>
    의사결정트리와 동일한 예측 파라미터를 갖는다.
  </li>
</ul>

```python
# 1. 랜덤 포레스트 적용 예
from pyspark.ml.classification import RandomForestClassifier

rfClassifier = RandomForestClassifier()
print(rfClassifier.explainParams())
trainedModel = rfClassifier.fit(bInput)
```

```python
# 2. 그래디언트 부스티드 트리 적용 예
from pyspark.ml.classification import GBTClassifier

gbtClassifier = GBTClassifier()
print(gbtClassifier.explainParams())
trainedModel = gbtClassifier.fit(bInput)
```

<br><br>

<h1>7. 나이브 베이즈</h1>
<ul>
  <li>
    <strong>나이브 베이즈(Naive Bayes) 분류기</strong>는 <strong>베이즈 정리</strong>에 기반한 분류기로 모델의 핵심 가정은 <strong>모든 특징이 서로 독립</strong>이라는 점이다.
  </li>
  <li>
    나이브 베이즈 분류기에는 <strong>문서 내 용어의 존재</strong>를 <strong>지시변수(indicator variable)</strong>를 활용하여 나타내는 <strong>다변량 베르누이 모델(multivariate Bernoulli model)</strong>과 <strong>하나의 용어의 총 수</strong>를 변수로 활용하는 <strong>다항 모델(multinomial model)</strong> 두 가지가 있다.
  </li>
  <li>
    모든 입력 특징은 <strong>음수가 아니어야 한다</strong>.
  </li>
</ul>

<br>

<h2>7-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>modelType</strong>
  </li>
    <ul>
      <li>
        <strong>bernoulli</strong>와 <strong>multinomial</strong> 중 선택한다.
      </li>
    </ul>
  <li>
    <strong>weightCol</strong>
  </li>
    <ul>
      <li>
        각 데이터값에 대해 <strong>서로 다른 가중치</strong>를 줄 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>7-2. 학습 파라미터</h2>
<ul>
  <li>
    <strong>smoothing</strong>
  </li>
    <ul>
      <li>
        <strong>가산적 평활화(additive smoothing)</strong>을 사용하여 일반화 작업의 범위를 결정한다.
      </li>
      <li>
        <strong>범주 데이터를 평활화</strong>하고 특정 클래스의 <strong>확률을 변경</strong>하여 학습 데이터에 대한 <strong>과적합을 피할 수 있다</strong>.
      </li>
    </ul>
</ul>

```python
# 1. 나이브 베이즈 적용 예
from pyspark.ml.classification import NaiveBayes

nb = NaiveBayes()
print(nb.explainParams())
trainedModel = nb.fit(bInput.where("label != 0"))
```

<br><br>

<h1>8. 분류와 자동 모델 튜닝을 위한 평가기</h1>
<ul>
  <li>
    평가기는 독립적으로 사용하면 큰 도움이 되지 않지만, <strong>파이프라인</strong>에서 사용하면 변환자의 다양한 파라미터에 대한 <strong>그리드 서치(grid search)를 자동화</strong>할 수 있다.
  </li>
  <li>
    분류 문제에는 <strong>두 개의 평가기</strong>가 있고 <strong>두 개의 컬럼</strong>으로 존재한다.
  </li>
    <ul>
      <li>
        이진 분류의 경우 <strong>BinaryClassificationEvaluator</strong>를 사용하며 areaUnderROC와 areaUnderPR 두 가지 평가지표 최적화를 지원한다.
      </li>
      <li>
        다중 클래스 분류의 경우 <strong>MulticlassClassificationEvaluator</strong>를 사용하며 f1, weightedPrecision, weightedRecall 및 accuracy에 대한 최적화를 지원한다.
      </li>
    </ul>
</ul>

<br><br>

<h1>9. 세부 평가지표</h1>
<ul>
  <li>
    2018년에는 다양한 평가지표가 RDD로만 제공 되었지만 DataFrame도 현재는 제공할 것이다.
  </li>
</ul>

```python
# 1. 평가지표 예
from pyspark.mllib.evaluation import BinaryClassificationMetrics

out = trainedMode.transform(bInput)\
    .select("prediction", "label")\
    .rdd.amp(lambda x: (float(x[0]), float(x[1])))
metrics = BinaryClassificationMetrics(out)

# 평가지표 확인.
print(metrics.areaUnderPR)
print(metrics.areaUnderROC)
```

<br><br>

<h1>10. 일대다 분류기</h1>
<ul>
  <li>
    다중 클래스 분류를 지원하지 않는 MLlib은 이진 분류기만 주어진 상황에서 다중 클래스 분류를 수행하기 위해 <strong>일대다(one-vs-rest) 분류기</strong>를 활용할 수 있다.
  </li>
  <li>
    일대다 분류기는 <strong>하나의 클래스</strong>와 <strong>나머지 클래스</strong> 둘 중 하나로 분류한다. 따라서 최종 예측 클래스는 두 개가 된다.
  </li>
  <li>
    일대다 분류기는 <strong>추정자</strong>로 구현된다. 
  </li>
  <li>
    예측은 각 이진 분류기의 평가를 기반으로 하며 가장 신뢰도가 높은 분류기의 인덱스가 최종 레이블이 된다.
  </li>
</ul>

<br><br>

<h1>11. 다층 퍼셉트론</h1>
<ul>
  <li>
    다층 퍼셉트론(multilayer perceptron, MLP)는 <strong>가능한 수의 계층(그리고 계층의 크기)</strong>을 갖는 신경망에 기초한 분류기이다.
  </li>
</ul>