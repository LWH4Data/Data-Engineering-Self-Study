```Dockerfile
# Dockerfile
FROM apache/spark:3.5.2

USER root
RUN apt-get update && apt-get install -y python3-pip \
    && pip3 install --no-cache-dir numpy pandas

USER spark
```

```bash
MSYS_NO_PATHCONV=1 MSYS2_ARG_CONV_EXCL="*" \
docker run -it --rm \
  -p 4040:4040 \
  -v /c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/workspace/Spark-The-Definitive-Guide \
  spark-with-numpy \
  bash -lc '/opt/spark/bin/pyspark'
```

<ul>
  <li>
    6부에서 다룰 Spark 고급 분석 도구들
  </li>
    <ul>
      <li>
        데이터 전처리: 데이터 정제 및 피처 엔지니어링
      </li>
      <li>
        지도 학습(supervised learning)
      </li>
      <li>
        비지도 학습(unsupervised learning)
      </li>
      <li>
        추천 엔진(recommendation learning)
      </li>
      <li>
        그래프 분석(graph analytics)
      </li>
      <li>
        딥러닝(deep learning)
      </li>
    </ul>
</ul>

<br>

<h1>1. 고급 분석에 대한 짧은 입문서</h1>
<ul>
  <li>
    <strong>고급 분석</strong>이란 <strong>데이터 기반의 인사이트</strong>를 도출하여 <strong>핵심 문제를 해결</strong>하거나 <strong>예측</strong> 또는 <strong>추천</strong>을 하기위한 기술을 의미한다.
  </li>
  <li>
    머신러닝에서 가장 일반적으로 활용되는 작업
  </li>
    <ul>
      <li>
        다양한 특징(feature)을 기반으로 각 샘플에 부여된 <strong>label을 예측</strong>하는 분류/회귀 문제를 포함한 <strong>지도 학습</strong>.
      </li>
      <li>
        <strong>사용자의 과거 행동</strong>에 기반하여 제품을 제안하는 <strong>추천 엔진</strong>.
      </li>
      <li>
        군집 분석, 이상징후 탐색, 토픽 모델링(topic modeling)과 같이 <strong>데이터 구조</strong>를 파악하기 위한 <strong>비지도 학습</strong>.
      </li>
      <li>
        소셜 네트워크상에서 유의미한 <strong>패턴</strong>을 찾는 <strong>그래프 분석</strong>.
      </li>
    </ul>
</ul>

<br>

<h2>1-1. 지도 학습</h2>
<ul>
  <li>
    <strong>지도 학습(supervised learning)</strong>은 레이블을 포함하는 <strong>과거 데이터</strong>를 사용하여 <strong>모델을 학습</strong>시칸 후 데이터 포인트의 다양한 특징을 기반으로 <strong>해당 레이블값을 예측</strong>하는 것이다.
  </li>
  <li>
    일반적으로 <strong>경사 하강법(gradient descent)</strong>과 <strong>반복적 최적화 알고리즘</strong>으로 진행된다.
  </li>
</ul>

<h3>1-1-1. 분류</h3>
<ul>
  <li>
    <strong>분류</strong>란 <strong>범주형 종속변수</strong>를 예측하는 알고리즘을 학습하는 행위이다.
  </li>
  <li>
    <strong>이진 분류(binary classification)</strong>은 <strong>두 그룹 중 어디에</strong> 속하는지를 예측하는 알고리즘이다.
  </li>
  <li>
    <strong>세 가지 이상의 범주</strong>로 분류하는 알고리즘을 <strong>다중 클래스 분류(multiclass classfication)</strong>이라 한다.
  </li>
</ul>

<h3>1-1-2. 회귀</h3>
<ul>
  <li>
    <strong>회귀 붅석</strong>은 <strong>연속형 변수</strong>를 예측한다.
  </li>
</ul>

<br>

<h2>1-2. 추천</h2>
<ul>
  <li>
    <strong>추천 알고리즘</strong>은 <strong>유사성</strong>을 기반으로 특정 사용자와 유사한 사용자가 <strong>선호하는 상품</strong>이나 특정 사용자가 <strong>이미 구매한 상품</strong>과 비슷한 상품을 추천한다.
  </li>
</ul>

<br>

<h2>1-3. 비지도 학습</h2>
<ul>
  <li>
    <strong>비지도 학습</strong>은 주어진 데이터셋에서 <strong>특정 패턴</strong>을 찾거나 <strong>숨겨진 구조적 특징을 발견</strong>하는 행위이다.
  </li>
  <li>
    예측 대상이 되는 <strong>종속변수가 필요없다</strong>는 것이 지도 학습과의 가장 큰 차이이다.
  </li>
</ul>

<br>

<h2>1-4. 그래프 분석</h2>
<ul>
  <li>
    <strong>그래프 분석</strong>은 <strong>객체를 가리키는 정점(vertex)</strong>과 해당 <strong>객체 간의 관계를 나타내는 에지(edge)</strong>를 지정하는 구조에 대한 연구이다.
  </li>
  <li>
    그래프는 <strong>관계에 대한 모든 것</strong>을 의미한다.
  </li>
</ul>

<br>

<h2>1-5. 고급 분석 프로세스</h2>
<ul>
  <li>
    일반적인 머신러닝 수행 단게 
  </li>
    <ul>
      <li>
        분석 주제와 관련된 <strong>데이터를 수집</strong>한다.<br>→ 데이터 파악을 위해 <strong>데이터를 정제</strong>하고 <strong>검토</strong>한다.<br>→ 알고리즘이 데이터를 잘 인식할 수 있도록 <strong>피터 엔지니어링(feature engineering)</strong>을 수행한다.<br>→ 후보 모델을 생성하는 하나 이상의 알고리즘을 학습 시키기 위해 데이터 일부를 <strong>학습 데이터셋</strong>으로 사용한다.<br>→ 샌성된 모델을 <strong>검증 데이터에 적용</strong>하고 성공 기준과 비교하여 최종 평가한다.<br>→ 해당 과정을 통해 비즈니스 문제를 해결한다.
      </li>
    </ul>
</ul>

<h3>1-5-1. 데이터 수집</h3>
<ul>
  <li>
    알고리즘을 학습시키기 위한 데이터를 모으는 과정이다.
  </li>
</ul>

<h3>1-5-2. 데이터 정제</h3>
<ul>
  <li>
    데이터를 정제하고 검토 과정을 일반적으로 <strong>탐색적 데이터 분석(EDA, Exploratory Data Analysis)</strong>라 한다.
  </li>
  <li>
    EDA에서는 읿반적으로 대화형 쿼리 및 시각화 방식 등을 사용하여 데이터의 분포, 상관관계 및 기타 세부 현황을 파악한다.
  </li>
</ul>

<h3>1-5-3. 피처 엔지니어링</h3>
<ul>
  <li>
    머신러닝 알고리즘에 적용 가능한 형식으로 <strong>데이터를 변환</strong>한다.
  </li>
  <li>
    피터 엔지니어링의 프로세스는 <strong>데이터 정규화</strong>, <strong>새로운 변수 추가</strong>, <strong>범주형 변수 조작 및 머신러닝 모델에서 인식할 수 있도록 변환</strong>하는 작업 등을 포함한다.
  </li>
  <li>
    Spark의 MLlib에서는 일반적으로 실젯값의 형태와 관계없이 모든 변수가 <strong>실수(double)형 벡터</strong>로 입력 되어야 한다.
  </li>
</ul>

<h3>1-5-4. 모델 학습</h3>
<ul>
  <li>
    학습 과정의 결과를 <strong>모델(model)</strong>이라 하며 <strong>통찰력</strong>을 얻거나 <strong>미래를 예측</strong>하는 데 사용한다.
  </li>
</ul>

<h3>1-5-5. 모델 튜닝 및 평가</h3>
<ul>
  <li>
    새로운 데이터에 일반화되지 않는 <strong>과적합 현상</strong>이 발생하지 않는지 확인한다.
  </li>
  <li>
    <strong>과적합을 테스트</strong> 하기 위해 사용하는 데이터셋을 보통 <strong>테스트셋(test set)</strong>이라 한다.
  </li>
  <li>
    모델 학습 과정에서 사전 분할된 새로운 데이터셋으로 학습에 영향을 주는 <strong>다양항 하이퍼파라미터를 시험</strong>해보고 <strong>변형된 모델의 적합성</strong>을 테스트한다. 이때 사용되는 데이터 셋을 <strong>검증셋(validation set)</strong>이라 한다.
  </li>
  <li>
    따라서 모델을 생성하기 위해서는 <strong>학습셋</strong>, <strong>검증셋</strong> 그리고 <strong>테스트셋 세 가지의 데이터셋</strong>이 필요하다.
  </li>
</ul>

<h3>1-5-6. 모델 및(또는) 통찰력 활용하기.</h3>
<ul>
  <li>
    모델 학습 및 테스트 과정을 통해 성능이 좋은 모델을 모델링하고 사용할 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 스파크의 고급 분석 툴킷</h1>
<ul>
  <li>
    Spark는 고급 분석을 수행하기 위한 몇 가지 핵심 패키지와 외부 패키지를 제공하며 그 중 기본이 되는 패키지는 <strong>MLlib</strong>으로 <strong>머신러닝 파이프라인 구축을 위한 인터페이스</strong>를 제공한다.
  </li>
</ul>

<br>

<h2>2-1. MLlib이란</h2>
<ul>
  <li>
    <strong>org.apache.spark.ml 패키지</strong>에는 <strong>DataFrame을 사용</strong>할 수 있는 언터페이스가 포함되어 있다.
  </li>
    <ul>
      <li>
        <strong>선행 단계 수행 과정을 표준화</strong>하는 데 도움이 <strong>머신러닝 파이프라인 구축</strong>을 위한 고급 인터페이스 제공.
      </li>
    </ul>
  <li>
    보다 하위 레벨의 패키지인 <strong>org.apache.spark.mllib</strong>은 <strong>Spark의 저수준 RDD API</strong>를 위한 인터페이스를 제공한다.
  </li>
</ul>

<h3>2-1-1. MLlib은 언제 그리고 왜 사용해야 할까</h3>
<ul>
  <li>
    단일 머신의 도구와 MLib은 <strong>상호보완적</strong>이라 할 수 있다.
  </li>
  <li>
    Spark의 확장 기능이 적용되는 두 가지 주요 예
  </li>
    <ul>
      <li>
        <strong>Spark로 데이터를 전처리하고 특징을 생성</strong>하여 대량의 데이터로부터 학습 및 테스트를 위한 <strong>데이터셋 생성에 걸리는 시간을 단축</strong>한 후 단일 머신 기반의 학습 라이브러리에 활용할 수 있다.
      </li>
      <li>
        <strong>입력 데이터나 모델의 크기</strong>가 단일 머신에 올려놓기 어렵운 경우 Spark를 사용해 <strong>분산 처리 기반의 머신러닝</strong>을 수행할 수 있다.
      </li>
    </ul>
  <li>
    단, 학습 및 데이터 처리가 간단해지지만 학습이 완료된 모델을 배포할 때에 <strong>복잡성이 증가</strong>한다는 단점이 있다.
  </li>
  <li>
    <strong>MLlib</strong>은 일반적으로 <strong>모델을 검사</strong>하고 가능하면 <strong>다른 도구로 내보낼 수</strong> 있도록 설계되어 있다.
  </li>
</ul>

<br><br>

<h1>3. 고수준 MLlib의 개념</h1>
<ul>
  <li>
    MLlib에는 <strong>변환자</strong>, <strong>추정자</strong>, <strong>평가기(evaluator)</strong> 그리고 <strong>파이프라인(pipeline)</strong>과 같은 몇 가지 <strong>기본적인 구조적 유형</strong>이 있다.
  </li>
  <li>
    <strong>변환자</strong>
  </li>
    <ul>
      <li>
        변환자는 <strong>원시 데이터</strong>를 <strong>다양한 방식으로 변환</strong>하는 함수이다.
      </li>
      <li>
        새로운 상호작용 변수를 생성, 컬럼을 정규화 혹은 Integer를 모델에 입력하기 위해 Double로 변경 등 주로 <strong>데이터 전처리</strong> 및 <strong>피처 엔지니어링</strong> 과정에 사용된다.
      </li>
    </ul>
  <li>
    <strong>추정자</strong>
  </li>
    <ul>
      <li>
        추정자는 두 가지 의미를 갖는다. 첫 번째는 <strong>데이터를 초기화</strong>하는 일종의 <strong>변환자</strong>를 의미한다.
      </li>
      <li>
        두 번째는 사용자가 데이터로부터 모델을 학습시키기 위해 사용하는 <strong>알고리즘</strong> 또한 추정자라 한다.
      </li>
    </ul>
  <li>
    <strong>평가기</strong>
  </li>
    <ul>
      <li>
        평가기는 주어진 모델의 <strong>성능</strong>이 <strong>지정한 기준</strong>에 따라 어떻게 작동하는지 볼 수 있게 해준다.
      </li>
      <li>
        평가기를 사용하여 테스트한 모델 중 가장 우수한 모델을 사용하여 최종 예측을 할 수 있다.
      </li>
    </ul>
  <li>
    고수준에서 차례로 변환, 추정 및 평가를 하나씩 지정할 수도 있지만 <strong>파이프라인의 단계</strong>로 지정하는 것이 더 간편할 수 있다.
  </li>
</ul>

<h3>3-1. 저수준 데이터 타입</h3>
<ul>
  <li>
    MLlib에서 작업해야 할 수 있는 몇 가지 <strong>저수준 데이터 타입</strong>이 있으며 그중 <strong>Vector</strong>가 가장 일반적인 타입이다.
  </li>
  <li>
    머신러닝 모델에 전달되는 일련의 특징은 <strong>Double 타입</strong>으로 구성된 <strong>벡터 형태</strong>로 전달 되어야 한다. 해당 벡터는 희소하거나 밀도가 높을 수 있다.
  </li>
</ul>

```python
# 1. 수동으로 벡터 생성.
from pyspark.ml.linalg import Vectors

denseVec = Vectors.dense(1.0, 2.0, 3.0)
size = 3
idx = [1, 2]
values = [2.0, 3.0]
sparseVec = Vectors.sparse(size, idx, values)
```

<br><br>

<h1>4. MLlib 실제로 사용하기</h1>

```python
# 1. 실습을 위한 데이터 로딩
df = spark.read.json("/workspace/Spark-The-Definitive-Guide/data/simple-ml")
df.orderBy("value2").show()
```

<br>

<h2>4-1. 변환자를 사용해서 피터 엔지니어링 수행하기</h2>
<ul>
  <li>
    변환자는 여러 방식으로 <strong>현재 컬럼을 조작</strong>하며 주로 모델의 <strong>입력변수</strong>로 사용할 특징을 개발하는 데 초점을 맞춘다.
  </li>
  <li>
    MLlib을 사용할 때 Spark에서 제공하는 머신러닝 알고리즘의 모든 입력변수는 <strong>Double 타입</strong> 혹은 <strong>Vector 타입</strong>으로 구성되어야 한다.
  </li>
    <ul>
      <li>
        예제에서는 <strong>RFormula</strong>를 통해 타입을 변환한다.
      </li>
      <li>
        RFormula는 <strong>R 연산자</strong>의 한정된 부분 집합을 지원한다.
      </li>
        <ul>
          <li>
            <strong>~</strong>: 함수에서 target과 term을 분리
          </li>
          <li>
            <strong>+</strong>: 연결 기호. +0은 절편 제거를 의미한다.
          </li>
          <li>
            <strong>-</strong>: 삭제 기호. -1은 절편 제거를 의미하며 +0과 의미가 같다.
          </li>
          <li>
            <strong>:</strong> : 상호작용(수치형 값이나 이진화된 범주 값에 대한 곱셈).
          </li>
          <li>
            <strong>.</strong> : target/종속을 제외한 모든 컬럼.
          </li>
        </ul>
    </ul>
</ul>

```python
# 1. 데이터 타입 변환.
from pyspark.ml.feature import RFormula

# R의 연산을 사용하기에 모델이 데이터 형식을 알아서 적절한 타입으로 변환한다.
supervised = RFormula(formula="lab ~ . + color:value1 + color:value2")
```

```python
# 2. 어떤 컬럼이 범주형이고, 범주형 컬럼의 고윳값이 무엇인지 결정하기 휘애 fit 메서드를 활
#    용한다.
# - fit 메서드를 통해 실제로 데이터를 변형시키는 데 사용할 수 있는 학습된 버전의 변환자를 
#   반환한다.
fittedRF = supervised.fit(df)
preparedDF = fittedRF.transform(df)
preparedDF.show()

# 요약
# RFormula는 fit 함수가 호출되는 과정에서 데이터를 검사
# → RformulaModel이라는 지정된 수식에 따라 데이터를 변환할 객체를 출력.
# → Spark는 자동으로 번주형 변수를 Double 타입으로 변환.
```

```python
# 3. 학습과 테스트를 위해 데이터를 분할.
train, test = preparedDF.randomSplit([0.7, 0.3])
```

<br>

<h2>4-2. 추정자</h2>
<ul>
  <li>
    <strong>explainParams 메서드</strong>는 MLlib에서 제공하는 모든 알고리즘의 <strong>파라미터</strong>를 확인할 수 있다.
  </li>
  <li>
    Spark는 모든 과정을 <strong>하나의 선언적 파이프라인</strong>으로 지정하여 <strong>자동적으로 모델을 생성하고 평가</strong>할 수 있다.
  </li>
  <li>
    하이퍼파라미터란 모델 아키텍처 및 일반화(regularization)와 같은 <strong>학습 프로세스에 영향을 주는 설정 매개변수</strong>이다.
  </li>
  <li>
    <strong>표준화(standardization)</strong>란 <strong>평균을 기준</strong>으로 관측값들이 <strong>얼마나 떨어져 있는지를 재표현</strong>하는 방법이다.
  </li>
  <li>
    <strong>정규화(normalization)</strong>는 데이터 <strong>분포의 중심을 0</strong>으로 맞추고, 값의 분포가 <strong>특정 범위 안</strong>에 들어오도록 조정하고, <strong>표준화</strong>를 취하고, <strong>모든 값을 0에서 1사이의 값</strong>으로 재표현 하는 과정으로 진행된다.
  </li>
  <li>
    <strong>일반화</strong>는 <strong>모델 과적합을 방지</strong>하기 위한 기법이다. <strong>모델 표현식에 추가적인 제약 조건</strong>을 걸어 모델이 필요 이상으로 복잡해지지 않도록 조정한다.
  </li>
</ul>

```python
# 1. 로지스틱 회귀 알고리즘 객체화
#   - 레이블 컬럼과 특정 컬럼을 설정. (label과 features로 명명).
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(labelCol="label", featuresCol="features")
```

```python
# 2. 모델에서 사용할 수 있는 옵션을 검토.
print(lr.explainParams())
```

```python
# 3. 알고리즘을 객체화 한 뒤 데이터에 적합.
fittedLR = lr.fit(train)
```

```python
# 4. transform 메서드를 통해 예측을 수행.
#   - 예측의 결과는 조작 가능한 또다른 DataFrame이다.
fittedLR.transform(train).select("label", "prediction").show()
```

<br>

<h2>4-3. 워크플로를 파이프라인으로 만들기</h2>
<ul>
  <li>
    <strong>Pipeline</strong>은 마지막 단계인 추정자가 <strong>조건에 따라 자동 조정</strong>되고, 이에 따른 <strong>데이터 변환 과정을 설정</strong>할 수 있어 <strong>튜닝된 모델을 바로 사용</strong>할 수 있다.
  </li>
  <li>
    변환자 객체나 모델 객체는 다른 파이프라인에서 <strong>재사용</strong>하지 않으며 다른 파이프라인을 생성하기 전에 항상 <strong>새로운 모델 객체</strong>를 만들어야 한다.
  </li>
  <li>
    <strong>모델 과적합을 방지</strong>하기 위해 <strong>테스트셋을 생성</strong>하고 <strong>검증셋을 기반</strong>으로 하이퍼 파라미터를 조정한다. (검증셋은 원시 데이터를 기반으로 한다).
  </li>
</ul>

```python
# 1. 데이터 분할
train, test = df.randomSplit([0.7, 0.3])
```

```python
# 2. 분석 준비

# RFormula로 입력 특징 타입 데이터 분석 및 새로운 특징 생성을 위해 형상 변환.
rForm = RFormula()

# 모델 객체 생성.
lr = LogisticRegression().setLabelCol("label").setFeaturesCol("features")
```

```python
# 3. 변환과 모델 튜닝을 파이프라인으로 생성.
from pyspark.ml import Pipeline

stages = [rForm, lr]
pipeline = Pipeline().setStages(stages)
```

<br>

<h2>4-4. 모델 학습 및 평가</h2>

```python
# 1. 다양한 하이퍼 파라미터를 학습 전에 설정.
from pyspark.ml.tuning import ParamGridBuilder

params = ParamGridBuilder()\
    # 두 가지 다른 버전의 RFormula
    .addGrid(rForm.formula, [
      "lab ~ . + color:value1",
      "lab ~ . + color:value1 + color:value2"])\
    # 세 개의 다른 옵션의 ElasticNet
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\
    # 두 개의 다른 옵션의 일반화 파라미터
    .addGrid(lr.regParam, [0.1, 2.0])\
    .build()

# 총 12가지 파라미터 조합.
```

```python
# 2. 평가지표 작성.
#   - BinaryClasificationEvaluator를 사용하고, areaUnderROC 사용
from pyspark.ml.evaluation import BinaryClassificationEvaluator

evaluator = BinaryClassificationEvaluator()\
    .setMetricName("areaUnderROC")\
    .setRawPredictionCol("prediction")\
    .setLabelCol("label")
```

```python
# 3. TrainValidationSplit을 통해 모델 평가 수행.
#   - TrainValidationSplit: 데이터를 두 개의 서로 다른 그룹으로 무작위 임의 분할.
#   - CrossValidator: 데이터 집합을 겹치지 않게 임의로 구분된 k 개의 폴드로 분할하여 K-
#     fold cross-validation 수행.
from pyspark.ml.tuning import TrainValidationSplit

# TrainValidationSplit 객체 생성 (모델 튜닝 및 검증용)
tvs = TrainValidationSplit()\
    # 전체 데이터를 75% 학습(train), 25% 검증(validation)으로 나눔
    .setTrainRatio(0.75)\
    # 탐색할 하이퍼파라미터 조합들을 지정 (GridSearch 같은 역할)
    .setEstimatorParamMaps(params)\
    # 실제 학습할 Estimator(모델 파이프라인) 지정
    .setEstimator(pipeline)\
    # 모델 성능을 평가할 기준(Evaluator) 지정
    .setEvaluator(evaluator)
```

```python
# 4. 파이프라인 구동. (검증셋에 대한 모든 버전의 모델이 테스트된다).
tvsFitted = tvs.fit(train)
```

```python
# 5. 모델 성능 평가.
evaluator.evaluate(tvsFitted.transform(test))
```

```python
# 6. 각 반복 학습에 해당 알고리즘의 세부 정보를 확인.
from pyspark.ml import PipelineModel
from pyspark.ml.classification import LogisticRegressionModel

# tvsFitted는 TrainValidationSplitModel 결과 객체
# PipelineModel 타입
trained_pipeline = tvsFitted.bestModel 

# 파이프라인 stage 중 1번째(Stage[1])가 LogisticRegressionModel일 경우
# LogisticRegressionModel
trained_lr = trained_pipeline.stages[1]  

# LogisticRegressionTrainingSummary 객체
summary_lr = trained_lr.summary

# 학습 과정에서의 objective history (수렴 과정 로그)
objective_history = summary_lr.objectiveHistory
print(objective_history)
```

<br>

<h2>4-5. 모델 저장 및 적용</h2>

```python
# 1. 학습이 완료된 모델 디스크에 저장.
tvsFitted.write.overwrite().save("/tmp/modelLocation")
```

```python
# 2. 수동으로 모델을 읽어와 사용.
from pyspark.ml.tuning import TrainValidationSplitModel

# 저장된 모델 로드
model = TrainValidationSplitModel.load("/tmp/modelLocation")

# 데이터셋에 적용 (예: test DataFrame)
predictions = model.transform(test)

predictions.show(5)
```

<br><br>

<h1>5. 모델 배포 방식</h1>
<ul>
  <li>
    <strong>오프라인으로 모델을 학습</strong>시키고 <strong>온라인으로 배포</strong>. 오프라인이란 <strong>분석을 위해 저장된 데이터</strong>를 의미한다.
  </li>
  <li>
    <strong>오프라인에서 모델을 학습</strong> 시키고 <strong>데이터베이스(일반적인 key-value)</strong>에 결과를 저장. <strong>추천 분야</strong>에 적합하다.
  </li>
  <li>
    오프로인으로 학습 시키고 모델을 <strong>디스크에 저장</strong>한 뒤 서비스로 제공. <strong>시간</strong>이 길어질 수 있으며 <strong>병렬 처리</strong>가 어렵다.
  </li>
  <li>
    <strong>단일 시스템</strong>에서 <strong>사용자의 분산 모델</strong>을 훨신 더 빠르게 수행할 수 있도록 <strong>수동 변환</strong>. 시간이 지남에 따라 유지보수가 어려울 수 있다.
  </li>
  <li>
    머신러닝 알고리즘을 <strong>온라인으로 학습</strong>시키고 <strong>온라인에서 사용</strong>. <strong>구조적 스트리밍</strong>과 함께 사용할 때는 가능하지만 일부 모델은 복잡해 질 수 있다.
  </li>
</ul>