<h1>1. 활용 사례</h1>
<ul>
  <li>
    <strong>영화 관객 수 예측</strong>
  </li>
    <ul>
      <li>
        영화 및 관객과 관련된 정보를 기반으로 영화가 출시되면 얼마나 많은 사람이 영화를 볼 것인지 예측한다.
      </li>
    </ul>
  <li>
    <strong>회사 수익 예측</strong>
  </li>
    <ul>
      <li>
        회사의 현재 성장 궤도, 시장 및 계절성으래 고려하여 앞으로 회사가 얼마나 많은 이익을 얻을지 예측한다.
      </li>
    </ul>
  <li>
    <strong>농작물 수확량 예측</strong>
  </li>
    <ul>
      <li>
        농작물이 재배되는 특정 지역에 대한 정보와 1년 동안의 날씨 정보를 통해 특정 토지 구획에서의 농작물 수확량을 예측.
      </li>
    </ul>
</ul>

<br><br>

<h1>2. MLlib에서 제공하는 회귀 모델</h1>
<ul>
  <li>
    Spark 2.2에 내포된 회귀 모델들
  </li>
    <ul>
      <li>
        선형 회귀
      </li>
      <li>
        일반환 선형 회귀(generalized linear regression)
      </li>
      <li>
        등위 회귀(isotonic regression)
      </li>
      <li>
        의사결정트리
      </li>
      <li>
        랜덤 포레스트
      </li>
      <li>
        그래디언트 부스티드 트리
      </li>
      <li>
        생존 회귀(survival regression)
      </li>
    </ul>
  <li>
    이번 장에서 다루는 각 모델의 기본 사항들
  </li>
    <ul>
      <li>
        모델과 알고리즘에 대한 기본 설명
      </li>
      <li>
        모델 설정을 위한 하이퍼파라미터(모델을 초기화할 수 있는 방법)
      </li>
      <li>
        학습 파라미터(모델 학습에 영향을 미치는 파라미터)
      </li>
      <li>
        예측 파라미터(예측에 영향을 미치는 파라미터)
      </li>
    </ul>
</ul>

<br>

<h2>2-1. 모델 확장성</h2>
<ul>
  <li>
    확장 가능한 모델과 데이터의 수가 표 형태로 정리되어 있다.
  </li>
</ul>

```python
# 1. 이번 장에서 사용할 샘플 데이터 불러오기
df = spark.read.load("/workspace/Spark-The-Definitive-Guide/data/regression")

# 데이터 확인
df.show(10)
```

<br><br>

<h1>3. 선형 회귀</h1>
<ul>
  <li>
    선형 회귀 분석은 입력 특징들의 <strong>선형 조합</strong>이 <strong>가우시안 오차</strong>와 함께 최종 결과로 산출된다고 가정한다.
  </li>
  <li>
    로지스틱 회귀처럼 <strong>ElasticNet 일반화</strong>를 통해 L1과 L2 일반화를 혼합할 수 있다.
  </li>
</ul>

<br>

<h2>3-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    로지스틱 회귀와 동일한 모델 하이퍼파라미터를 사용한다.
  </li>
</ul>

<br>

<h2>3-2. 학습 하이퍼파라미터</h2>
<ul>
  <li>
    모델 학습을 위한 하이퍼파라미터 또한 로지스틱 회귀와 동일하다.
  </li>
</ul>

<br>

<h2>3-3. 실습 예제</h2>

```python
from pyspark.ml.regression import LinearRegression

lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)
print(lr.explainParams())
lrModel = lr.fit(df)
```

<br>

<h2>3-4. 학습 내용 요약</h2>
<ul>
  <li>
    summary 메서드는 여러 개의 필드로 구성된 요약 객체를 반환한다.
  </li>
    <ul>
      <li>
        <strong>잔차(residual)</strong>: 선형 회귀 모델로부터 구한 <strong>예측값과 실제값과의 차이</strong>.
      </li>
      <li>
        <strong>객체 히스토리(objective history)</strong>: 반복 학습 과정마다 <strong>모델 학습</strong>이 어떻게 진행되는지 나타난다.
      </li>
      <li>
        <strong>평균제곱근오차(root mean squared error)</strong>: 각 예측값과 실젯값 사이의 거리를 계산하여 모델의 성능을 측정한다.
      </li>
      <li>
        <strong>R-Squared</strong>: 모델에 의해 설명되는 예측변수의 분산 비율이다.
      </li>
    </ul>
</ul>

```python
# 1. 선형 회귀 요약 일부
summary = lrModel.summary

summary.residuals.show()
print(summary.totalIterations)
print(summary.objectiveHistory)
print(summary.rootMeanSquaredError)
print(summary.r2)
```

<br><br>

<h1>4. 일반화 선형 회귀</h1>
<ul>
  <li>
    일반화 선형 회귀라 불리는 알고리즘에는 두 가지 알고리즘이 있다.
  </li>
    <ul>
      <li>
        하나는 <strong>단순 선형 회귀</strong>로 다수의 입력 특징을 대상으로 작업하기 최적화 되어 있다.
      </li>
      <li>
        다른 하는 좀 더 일반적이고 더 많은 알고리즘을 지원하지만 다수의 입력 특징으로 확장이 어렵다.
      </li>
    </ul>
  <li>
    일반화 선형 모델은 <strong>예상되는 노이즈 분포</strong>에 맞춰 예측을 수행할 수 있다. (e.g. 가우스(선형 회귀), 이항(로지스틱 회귀), 푸아송(푸아송 회귀))
  </li>
  <li>
    가능한 family가 장표로 정리되어 있다.
  </li>
</ul>

<br>

<h2>4-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>family</strong>
  </li>
    <ul>
      <li>
        모델에서 사용할 <strong>오차 분포</strong>에 대한 설명이다. 지원되는 옵션은 가우스, 이항, 푸아송, 감마 그리고 트위디가 있다.
      </li>
    </ul>
  <li>
    <strong>link</strong>
  </li>
    <ul>
      <li>
        선형 함수와 분포 함수의 <strong>평균 사이의 관계</strong>를 지정하는 링크 함수의 이름이다. cloglog, probit, logit, inverse, sqrt, identity 그리고 log가 있다.
      </li>
    </ul>
  <li>
    <strong>solver</strong>
  </li>
    <ul>
      <li>
        최적화에 사용되는 <strong>해찾기</strong> 알고리즘이다.
      </li>
    </ul>
  <li>
    variancePower
  </li>
    <ul>
      <li>
        분포의 분산과 평균 사이의 관계를 특징짓는 <strong>트위디 분포 함수</strong>에서의 검증력. 지원되는 값은 0과 1사이이며 기본값은 0이다.
      </li>
    </ul>
  <li>
    <strong>linkPower</strong>
  </li>
    <ul>
      <li>
        트위디 family의 파워 링크 함수 색인.
      </li>
    </ul>
</ul>

<br>

<h2>4-2. 학습 파라미터</h2>
<ul>
  <li>
    일반화 선형 회귀의 학습 파라미터는 로지스틱 회귀와 동일하다.
  </li>
</ul>

<br>

<h2>4-3. 예측 파라미터</h2>
<ul>
  <li>
    <strong>linkPredictionCol</strong>
  </li>
    <ul>
      <li>
        예측에 적용할 <strong>각 링크 함수</strong>를 정의하는 컬럼명이다.
      </li>
    </ul>
</ul>

<br>

<h2>4-4. 실습 예제</h2>

```python
# 1. 일반화 선형 회귀 적용 예
from pyspark.ml.regression import GeneralizedLinearRegression

glr = GeneralizedLinearRegression()\
    .setFamily("gaussian")\
    .setLink("identity")\
    .setMaxIter(10)\
    .setRegParam(0.3)\
    .setLinkPredictionCol("linkOut")
print(glr.explainParams())
glrModel = glr.fit(df)
```

<br>

<h2>4-5. 학습 내용 요약</h2>
<ul>
  <li>
    <strong>R squared</strong>
  </li>
    <ul>
      <li>
        결정계수, 적합의 척도
      </li>
    </ul>
  <li>
    <strong>The residuals</strong>
  </li>
    <ul>
      <li>
        관측 레이블과 예측값의 차이.
      </li>
    </ul>
</ul>

<br><br>

<h1>5. 의사결정트리</h1>
<ul>
  <li>
    회귀의 의사결정트리는 말단 노드마다 <strong>하나의 숫자</strong>를 출력한다.
  </li>
  <li>
    함수를 모델링하기 위해 계수를 학습하는 대신 의사결정트리는 단순히 수치를 예측하는 트리를 생성한다. 따라서 입력 데이터의 <strong>비선형 패턴</strong>을 예측할 수 있기에 유의미하다.
  </li>
  <li>
    단, <strong>과적합의 위험</strong>이 존재한다.
  </li>
</ul>

<br>

<h2>5-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    impurity
  </li>
    <ul>
      <li>
        모델이 특정 값을 갖는 특정 말단 노드에서 <strong>분할</strong>되어야 하는지 아니면 그대로 <strong>유지</strong>되어야 하는지 여부에 대한 평가지표를 의미한다.
      </li>
    </ul>
</ul>

<br>

<h2>5-2. 학습 파라미터</h2>
<ul>
  <li>
    분류 및 회귀 트리는 동일한 학습 파라미터를 공유한다.
  </li>
</ul>

<br>

<h2>5-3. 실습 예제</h2>

```python
# 1. 의사결정트리 회귀자(regressor) 사용.
from pyspark.ml.regression import DecisionTreeRegressor

dtr = DecisionTreeRegressor()
print(dtr.explainParams())
dtrModel = dtr.fit(df)
```

<br><br>

<h1>6. 랜덤 포레스트와 그래디언트 부스티드 트리</h1>
<ul>
  <li>
    <strong>랜덤 포레스트 모델</strong>에서는 상관관계가 없는 다양한 의사결정트리가 학습되고 <strong>평균화</strong>된다.
  </li>
  <li>
    <strong>그래디언트 부스티드 트리 모델</strong>에서는 개별 트리가 학습될 때 <strong>별도의 가중치</strong>가 부여된다. 따라서 특정 클래스에 다른 트리보다 예측력이 높은 트리가 존재할 수 있다.
  </li>
  <li>
    랜덤 포레스트와 그래디언트 부스티드 트리 회귀는 DecisionTreeRegressor의 경우와 같이 순도(purity) 척도를 제외하면 일반 분류 모델과 동일한 모델 하이퍼파라미터와 학습 파라미터를 갖는다.
  </li>
</ul>

<br>

<h2>6-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    분류와 동이한 하이퍼파라미터를 제공한다.
  </li>
</ul>

<br>

<h2>6-2. 학습 파라미터</h2>
<ul>
  <li>
    뷴류와 동일하게 checkpointInterval 파라미터를 지원한다.
  </li>
</ul>

<br>

<h2>6-3. 실습 예제</h2>

```python
# 1. 랜덤 포레스트와 그래디언트 부스티드 트리 적용.
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.regression import GBTRegressor

rf = RandomForestRegressor()
print(rf.explainParams())
rfModel = rf.fit(df)
gbt = GBTRegressor()
print(gbt.explainParams())
gbtModel = gbt.fit(df)
```

<br><br>

<h1>7. 고급 방법론</h1>
<h2>7-1. 생존 회귀(가속 수명 시간 모델)</h2>
<ul>
  <li>
    Spark는 실제 생존 시간을 설명하는 모델 보다는 <strong>생존 시간의 로그를 모델화</strong>하는 <strong>가속 수명 시간(accfelerated failure time, AFT) 모델</strong>을 구현한다.
  </li>
  <li>
    즉, Spark는 잘 알려진 콕스 비례 위험 모델(Cox Proportional Hazard model)이 아닌 <strong>생존 회귀 모델</strong>을 사용한다.
  </li>
    <ul>
      <li>
        콕스 비례 위험 모델은 준모수적이기에 대용량 데이터셋으로 확장하기 어렵다.
      </li>
      <li>
        반면 <strong>가속 수명 시간 모델</strong>은 각 인스턴스(row)가 결과 모델에 독립적으로 기여하기 때문에 <strong>확장이 용이</strong>하다. 단, 다른 가설을 준수하기에 모든 상황에 유용한 것은 아니다.
      </li>
    </ul>
  <li>
    입력변수 요건은 다른 회귀 분석과 매우 유사하지만 <strong>중도절단 변수</strong>를 도입해야 한다. 중도절단 변수란 일정 시점부터 <strong>피험자의 정보가 알려지지 않는 경우</strong> 해당 변수를 처리하는 방법이다.
  </li>
</ul>

<br>

<h2>7-2. 등위 회귀</h2>
<ul>
  <li>
    <strong>등위 회귀</strong>는 항상 <strong>단조롭게 증가</strong>하는 <strong>구간적 선형(piecewise linear) 함수</strong>를 지정한다.
  </li>
    <ul>
      <li>
        즉, 데이터가 우상향 추이를 보일 때 적합한 모델이다.
      </li>
    </ul>
  <li>
    단순 선형 회귀보다는 적합도가 높다.
  </li>
</ul>

<br><br>

<h1>8. 평가기와 모델 튜닝 자동화</h1>
<ul>
  <li>
    회귀 분석을 위한 평가는 <strong>RegressionEvaluator</strong>라고 부른다.
  </li>
  <li>
    평가지표는 <strong>평균제곱근오차(rmse)</strong>, <strong>결정계수(r2)</strong> 그리고 <strong>평균절대오차(mae)</strong>이다.
  </li>
</ul>

```scala
// 1. RegressionEvaluator를 사용하기 위해 파이프라인 구축.
//   - 테스트하고자하는 파라미터를 지정하고 실행하면 Spark가 가장 적합한 모델을 자동 선택한
//     다.
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.regression.GeneralizedLinearRegression
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}

val glr = new GeneralizedLinearRegression()
  .setFamily("gaussian")
  .setLink("identity")
val pipeline = new Pipeline().setStages(Array(glr))
val params = new ParamGridBuilder().addGrid(glr.regParam, Array(0.0, 0.5, 1.0))
  .build()
val evaluator = new RegressionEvaluator()
  .setMetricName("rmse")
  .setPredictionCol("prediction")
  .setLabelCol("label")
val cv = new CrossValidator()
  .setEstimator(pipeline)
  .setEvaluator(evaluator)
  .setEstimatorParamMaps(params)
  .setNumFolds(2) // 예제 데이터셋이 작아 2로 설정, 보통은 3이상.
val model = cv.fit(df)
```

```python
# 2. 파이썬 버전
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import GeneralizedLinearRegression
from pyspark.ml import Pipeline
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

glr = GeneralizedLinearRegression().setFamily("gaussian").setLink("identity")
pipeline = Pipeline().setStages([glr])
params = ParamGridBuilder().addGrid(glr.regParam, [0.0, 0.5, 1.0]).build()
evaluator = RegressionEvaluator()\
    .setMetricName("rmse")\
    .setPredictionCol("prediction")\
    .setLabelCol("label")
cv = CrossValidator()\
    .setEstimator(pipeline)\
    .setEvaluator(evaluator)\
    .setEstimatorParamMaps(params)\
    .setNumFolds(2) # 예제 데이터셋이 작아 2로 설정, 보통은 최소 3 이상
model = cv.fit(df)
```

<br><br>

<h1>9. 평가지표</h1>
<ul>
  <li>
    평가기를 사용하면 하나의 <strong>특정한 평가지표</strong>에 따라 모델을 평가하고 적합하거나 <strong>RegressionMetrics 객체</strong>를 통해 <strong>다양한 회귀 평가지표</strong>를 검토할 수 있다.
  </li>
  <li>
    RegressionMetrics는 평가지표에 대해 <strong>(예측, 레이블) 쌍의 RDD</strong>를 기반으로 동작한다.
  </li>
</ul>

```python
# 1. 파이썬을 통해 여러 평가지표 사용.
from pyspark.mllib.evaluation import RegressionMetrics

out = model.transform(df)\
    .select("prediction", "label").rdd.map(lambda x: (float(x[0]), float(x[1])))
metrics = RegressionMetrics(out)
print("MSE: " + str(metrics.meanSquaredError))
print("RMSE: " + str(metrics.rootMeanSquaredError))
print("R-squared: " + str(metrics.r2))
print("MAE: " + str(metrics.meanAbsoluteError))
print("Explained variance: " + str(metrics.explainedVariance))
```