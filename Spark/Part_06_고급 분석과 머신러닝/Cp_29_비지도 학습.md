<ul>
  <li>
    일반적으로 비지도 학습은 성과를 측정하기 어렵기 때문에 지도 학습보다 자주 사용되지 않는다.
  </li>
  <li>
    대표적으로 <strong>차원의 저주</strong> 문제가 있다.
  </li>
    <ul>
      <li>
        특정 공간의 차원이 증가하면 <strong>밀도가 점점 희소</strong>해지는 현상을 의미한다.
      </li>
      <li>
        통계적으로 의미있는 결과를 얻기 위해 <strong>공간을 채우는 데 필요한 데이터양</strong>이 차원의 수가 증가할수록 급격히 증가한다.
      </li>
      <li>
        또한 모델에 필요한 실제 요인 대신 <strong>노이즈</strong>에 전념하여 그룹화나 특정 분석 결과 도출에 방해가 될 수 있다.
      </li>
    </ul>
  <li>
    비지도 학습의 핵심은 <strong>패턴을 발견</strong>하기 위해 노력하거나 주어진 데이터셋의 <strong>근본적인 구조적 특징을 간결하게 표현</strong>하는 것이다.
  </li>
</ul>

<br>

<h1>1. 활용 사례</h1>
<ul>
  <li>
    데이터 이상치 탐지
  </li>
    <ul>
      <li>
        데이터셋 내 다수의 값이 하나의 그룹으로 군집화되고 나머지 값은 <strong>몇몇 소그릅</strong>으로 군집화되는 경우 <strong>해당 소그룹을 조사하여 이상치를 탐지</strong>할 수 있다.
      </li>
    </ul>
  <li>
    토픽 모델링
  </li>
    <ul>
      <li>
        많은 양의 텍스트 문서를 사전에 학습하여 <strong>서로 다른 텍스트 문서 사이의 공통적인 주제</strong>를 도출할 수 있다.
      </li>
    </ul>
</ul>

<br><br>

<h1>2. 모델 확장성</h1>
<ul>
  <li>
    확장성에 대한 내용은 장표로 정리되어 있다.
  </li>
</ul>

```python
# 1. 비지도 학습의 몇 가지 예
from pyspark.ml.feature import VectorAssembler

va = VectorAssembler()\
    .setInputCols(["Quantity", "UnitPrice"])\
    .setOutputCol("features")
sales = va.transform(spark.read.format("csv")
    .option("header", "true")
    .option("inferSchema", "true")
    .load("/workspace/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv")
    .limit(50)
    .coalesce(1)
    .where("Description IS NOT NULL"))
sales.cache()
```

<br><br>

<h1>3. k-평균</h1>
<ul>
  <li>
    <strong>k-평균 알고리즘의 과정</strong>
  </li>
    <ul>
      <li>
        <strong>사용자가 지정한 군집 수 k</strong>만큼 데이터셋 내에 서로 다른 포인트를 <strong>무작위 할당</strong>한다.<br>→ 할당되지 않은 포인트들은 할당된 포인트들과의 <strong>거리(유클리드 거리)를 계산</strong>하여가장 <strong>가까운 군집으로 할당</strong>한다.<br>→ 각 포인트를 군집으로 모두 할당했다면 <strong>각 군집의 중심(세트로이드)을 계산</strong>한다.<br>→ 앞의 과정을 <strong>사전에 지정한 횟수</strong>만큼 혹은 <strong>수렴(convergence)</strong>할 때까지 반복한다. 즉, <strong>중심값</strong>이 변경되지 않을 때까지 반복한다.
      </li>
    </ul>
  <li>
    <strong>k 값을 선택</strong>하는 것이 알고리즘에 가장 중요한 부분이며 다양한 값으로 테스트를 진행한다.
  </li>
</ul>

<br>

<h2>3-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>k</strong>
  </li>
    <ul>
      <li>
        최종 생성하고자 하는 군집의 수
      </li>
    </ul>
</ul>

<br>

<h2>3-2. 학습 파라미터</h2>
<ul>
  <li>
    k-means 알고리즘은 일반적으로 파라미터에 크게 민감하지 않다. <strong>초기화 단계</strong>와 <strong>반복 수행 횟수</strong>가 많아지면 결과가 좋아질 수 있지만 그만큼 <strong>학습 시간</strong>이 오래 걸린다.
  </li>
  <li>
    <strong>initMode</strong>
  </li>
    <ul>
      <li>
        초기화 모드는 <strong>군집 중심의 위치를 결정</strong>하는 알고리즘이다.
      </li>
      <li>
        옵션은 random과 k-means가 있으며 k-means는 k-means++를 분산 처리가 가능하도록 변형한 알고리즘이다.
      </li>
    </ul>
  <li>
    <strong>initSteps</strong>
  </li>
    <ul>
      <li>
        k-means||초기화 모드의 <strong>단계 수</strong>이다. 
      </li>
      <li>
        기본은 2이며, 0보다 커야한다.
      </li>
    </ul>
  <li>
    <strong>maxIter</strong>
  </li>
    <ul>
      <li>
        군집화를 수행할 <strong>총 반복 횟수</strong>이다.
      </li>
      <li>
        결과가 크게 바뀌지 않기에 우선 순위가 낮다.
      </li>
    </ul>
  <li>
    <strong>tol</strong>
  </li>
    <ul>
      <li>
        중심값의 변화가 모델이 충분히 최적화되었다는 것을 알려주는 임곗값을 지정하여 maxIter 값만큼 반복을 수행하기 전 <strong>조기 중지</strong> 시킬 수 있다.
      </li>
      <li>
        기본값은 0.0001이다.
      </li>
    </ul>
</ul>

<br>

<h2>3-3. 실습 예제</h2>

```python
# 1. k-means 클러스터링 모델 적합
from pyspark.ml.clustering import KMeans

km = KMeans().setK(5)
print(km.explainParams())
kmModel = km.fit(sales)
```

<br>

<h2>3-4. k-평균 평가지표 요약 정보</h2>
<ul>
  <li>
    k-평균 모델을 평가할 때 summary 클래스를 사용할 수 있으며 <strong>생성된 군집</strong>에 대한 정보와 <strong>상대적 크기</strong>가 포함된다.
  </li>
  <li>
    computeCost를 사용하여 <strong>군집 내 오차제곱합(Sum of Squared Error)을 계산</strong>할 수 있으며 <strong>군집의 중심으로부터 값이 얼마나 가까운지</strong> 측정하는 데 도움이 된다.
  </li>
  <li>
    k-평균의 암묵적인 목표는 사용자가 지정한 군집 수 k에 따라 <strong>군집내 오차제곱합을 최소화</strong>하는 것이다.
  </li>
</ul>

```python
# 1. 모델 적용 및 중심 출력.
summary = kmModel.summary

print(summary.clusterSizes) # 데이터 포인터의 개수
print(kmModel.summary.trainingCost)
centers = kmModel.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)
```

<br>

<h2>4-4. 이분법 k-평균 요약 정보</h2>
<ul>
  <li>
    이분법 k-평균 또한 summary 클래스를 제공하며 k-평균과 거의 동일하다.
  </li>
</ul>

```python
# 1. 이분법 k-평균 요약 정보 확인.
summary = bkmModel.summary
print(summary.clusterSizes)
print(summary.trainingCost)
centers = kmModel.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)
```

<br><br>

<h1>5. 가우시안 혼합 모델</h1>
<ul>
  <li>
    <strong>가우시안 혼합 모델은(Gaussian mixture model, GMM)</strong> 앞서 k-평균 방식이 거리제곱합을 이용하는 반면 각 군집이 <strong>가우시안 분포(Gaussian distribution)</strong>로부터 <strong>무작위 추출</strong>을 하여 데이터를 생성한다고 가정한다.
  </li>
    <ul>
      <li>
        따라서 <strong>군집 중앙</strong>에 데이터가 포함될 확률이 높고 <strong>군집 가장자리</strong>에 데이터가 포함될 확률은 낮다.
      </li>
      <li>
        각 가우시안 군집은 <strong>자체적인 평균 및 표준편차</strong>를 보유한 임의의 크기일 수 있다.
      </li>
      <li>
        마찬가지로 군집 수 k를 지정할 수 있다.
      </li>
    </ul>
  <li>
    k-평균 모델이 고정된 군집을 형성하는 반면 <strong>가우시안 군집</strong>은 군집의 <strong>경계를 엄격</strong>하게 제한하는 대신 <strong>확률과 연관된 미묘한 군집</strong>을 허용하기에 더 유연하다 볼 수 있다.
  </li>
</ul>

<br>

<h2>5-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>k</strong>
  </li>
    <ul>
      <li>
        생성하고자 하는 군집의 수
      </li>
    </ul>
</ul>

<br>

<h2>5-2. 학습 파라미터</h2>
<ul>
  <li>
    <strong>maxIter</strong>
  </li>
    <ul>
      <li>
        군집화를 수행할 총 반복 횟수. 우선순위가 낮은 파라미터이며 기본값은 100이다.
      </li>
    </ul>
  <li>
    <strong>tol</strong>
  </li>
    <ul>
      <li>
        파라미터값의 변화가 모델이 충분히 <strong>최적화</strong>되었음을 알려주는 임곗값이다.
      </li>
      <li>
        <strong>값이 작을수록</strong> 더 높은 정확도를 끌어내지만 <strong>더 많은 반복 수행</strong>을 하게 된다.
      </li>
      <li>
        기본값은 0.01이다.
      </li>
    </ul>
</ul>

<br>

<h2>5-3. 실습 예제</h2>

```python
# 1. 가우시안 혼합 모델 적용
from pyspark.ml.clustering import GaussianMixture

gmm = GaussianMixture().setK(5)
print(gmm.explainParams())
model = gmm.fit(sales)
```

<br>

<h2>5-4. 가우시안 혼합 모델 요약 정보</h2>
<ul>
  <li>
    summary 클래스에 요약 정보가 포함되어 있으며 <strong>생성된 클러스터 정보</strong>가 포함되어 있어 더 많은 데이터 내부 기본 구조를 확인할 수 있다.
  </li>
</ul>

```python
# 1. 가우시안 혼합 모델 요약 정보 확인.
summary = model.summary

print(model.weights)
model.gaussiansDF.show()
summary.cluster.show()
summary.clusterSizes
summary.probability.show()
```

<br><br>

<h1>6. 잠재 디리클레 할당</h1>
<ul>
  <li>
    <strong>잠재 디리클레 할당(Latent Dirichlet Allocaiton, LDA)</strong>은 일반적으로 <strong>텍스트 문서</strong>에 대한 <strong>토픽 모델링</strong>을 수행하는 데 사용되는 <strong>계층적 군집화 모델</strong>이다.
  </li>
  <li>
    LDA는 주제와 관련된 일련의 <strong>문서와 키워드</strong>로부터 <strong>주제</strong>를 추출하고 각 <strong>문서</strong>가 <strong>입력된 여러 주제</strong>에 얼마나 기여했는지 <strong>횟수</strong>를 계산한다.
  </li>
  <li>
    Spark에서 LDA를 구현하는 방법은 두 가지이다.
  </li>
    <ul>
      <li>
        <strong>Online LDA</strong>: 샘플 데이터가 많은 경우에 적합하다.
      </li>
      <li>
        <strong>Expectation maximization</strong>: 어휘 수가 많은 경우에 적합하다.
      </li>
    </ul>
  <li>
    텍스트 데이터를 LDA에 입력하려면 <strong>수치형</strong>으로 변환해야 하며 <strong>CountVectorizer</strong>를 사용하여 이를 수행한다.
  </li>
</ul>

<br>

<h2>6-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>k</strong>
  </li>
    <ul>
      <li>
        입력된 데이터로부터 최종적으로 추론할 <strong>총 주제의 수</strong>이다.
      </li>
    </ul>
  <li>
    <strong>docConcentration</strong>
  </li>
    <ul>
      <li>
        파라미터('alpha')문서가 갖는 주제 분포('theta')의 사전(prior) 추정치이다.
      </li>
      <li>
        디리클레 분포 파라미터이며, 값이 클수록 <strong>편평화(smoothing, 좀 더 일반화)</strong>된다는 의미이다.
      </li>
    </ul>
  <li>
    <strong>topicConcentration</strong>
  </li>
    <ul>
      <li>
        파라미터('beta' 또는 'eta')는 주제가 갖는 단어 분포의 사전 추정치이다.
      </li>
      <li>
        대칭 디리클레 분포 파라미터이며, 사용자가 topicConcentration의 값을 따로 설정하지 않으면 자동으로 설정된다.
      </li>
    </ul>
</ul>

<br>

<h2>6-2. 학습 파라미터</h2>
<ul>
  <li>
    maxIter
  </li>
    <ul>
      <li>
        LDA를 수행할 총 반복횟수이다. 우선 순위가 낮으며 기본값은 20이다.
      </li>
    </ul>
  <li>
    <strong>optimizer</strong>
  </li>
    <ul>
      <li>
        LDA 모델을 <strong>학습시킬 알고리즘</strong>을 선택한다.
      </li>
      <li>
        <strong>Expectation Maximization</strong>과 <strong>온라인 학습 최적화</strong> 중 선택할 수 있으며 기본값은 온라인이다.
      </li>
    </ul>
  <li>
    <strong>learningDecay</strong>
  </li>
    <ul>
      <li>
        지수적 감쇠율로 설정된 <strong>학습 속도</strong>이다.
      </li>
      <li>
        <strong>점근적으로 수렴</strong>하기 위해 <strong>(0.5, 1.0] 사이</strong>로 설정해야 한다.
      </li>
      <li>
        기본값은 0.51이며 <strong>온라인 최적화 방식</strong>에만 사용한다.
      </li>
    </ul>
  <li>
    <strong>learningOffset</strong>
  </li>
    <ul>
      <li>
        <strong>초기 반복 수행 횟수</strong>를 줄이는 학습 파라미터이다.
      </li>
      <li>
        값이 클수록 초기 반복 횟수가 줄어든다.
      </li>
      <li>
        기본값은 1,024.0이며 <strong>온라인 최적화 방식</strong>에만 사용한다.
      </li>
    </ul>
  <li>
    <strong>optimizeDocConcentration</strong>
  </li>
    <ul>
      <li>
        학습 과정에서 문서-주제 분포를 나타내는 디리클레 파라미터인 <strong>docConcentration이 최적화될지 여부</strong>를 나타낸다. 
      </li>
      <li>
        기본값은 true이며 <strong>온라인 최적화 방식</strong>에만 적용된다.
      </li>
    </ul>
  <li>
    <strong>subsamplingRate</strong>
  </li>
    <ul>
      <li>
        <strong>미니배치 경사 하강법</strong>의 각 반복 수행에서 <strong>샘플링</strong> 및 적용되는 <strong>말뭉치(corpus)의 비율</strong>이다.
      </li>
      <li>
        (0, 1]의 범위를 가지며 기본값은 0.5이다. <strong>온라인 최적화 방식</strong>에만 적용된다.
      </li>
    </ul>
  <li>
    <strong>seed</strong>
  </li>
    <ul>
      <li>
        LDA 모델의 재현성을 위해 임의의 시드를 지정할 수 있다.
      </li>
    </ul>
  <li>
    <strong>checkpointInterval</strong>
  </li>
    <ul>
      <li>
        이전 장과 동일한 파라미터이다.
      </li>
    </ul>
</ul>

<br>

<h2>6-3. 예측 파라미터</h2>
<ul>
  <li>
    <strong>topicDistributionCol</strong>
  </li>
    <ul>
      <li>
        각 문서의 <strong>주제 혼합 분포의 결과</strong>를 출력하는 컬럼이다.
      </li>
    </ul>
</ul>

<br>

<h2>6-4. 실습 예제</h2>
<ul>
  <li>
    <strong>복잡도</strong>를 성과 평가지표로 사용하는 경우 모델의 전반적인 복잡도를 줄이기 위해 이 척도를 <strong>홀드아웃셋</strong>에 적용해야 한다.
  </li>
  <li>
    또한 홀드아웃셋(테스트셋)의 <strong>로그 우도 값을 증가</strong>시키기위해 최적화를 수행해야 한다.
  </li>
  <li>
    <strong>로그 우도</strong>와 <strong>복잡도</strong>를 계산하는 함수는 <strong>model.logLikelihood</strong>와 <strong>model.logPerplexity</strong>이다.
  </li>
</ul>

```python
# 1. 잠재 디리클레 할당 수행.

# 데이터 준비
from pyspark.ml.feature import Tokenizer, CountVectorizer

tkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")
tokenized = tkn.transform(sales.drop("features"))
cv = CountVectorizer()\
    .setInputCol("DescOut")\
    .setOutputCol("features")\
    .setVocabSize(500)\
    .setMinTF(0)\
    .setMinDF(0)\
    .setBinary(True)
cvFitted = cv.fit(tokenized)
prepped = cvFitted.transform(tokenized)
```

```python
# 2. 모델 적합
from pyspark.ml.clustering import LDA

lda = LDA().setK(10).setMaxIter(5)
print(lda.explainParams())
model = lda.fit(prepped)
```

```python
# 3. 최종 선정된 단어 출력.
model.describeTopics(3).show()
cvFitted.vocabulary
```