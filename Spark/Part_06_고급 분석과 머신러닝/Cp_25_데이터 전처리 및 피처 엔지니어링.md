<h1>1. 사용 목적에 따라 모델 서식 지정하기</h1>
<ul>
  <li>
    대부분의 <strong>분류 및 회귀 알고리즘</strong>의 경우 데이터를 <strong>Double 타입</strong>의 컬럼으로 가져와 <strong>레이블</strong>을 표시하고, <strong>Vector 타임</strong>의 컬럼을 사용해 <strong>특징</strong>을 나타내야한다.
  </li>
  <li>
    <strong>추천 알고리즘</strong>의 경우 데이터를 사용자 컬럼, 영화 또는 서적 등을 나타내는 <strong>아이템 컬럼</strong> 그리고 사용자 평점 등을 나타내는 <strong>등급 컬럼</strong>으로 표현해야 한다.
  </li>
  <li>
    <strong>비지도 학습 알고리즘</strong>의 경우 <strong>입력 데이터</strong>로 사용할 특징을 <strong>Vector 타입의 컬럼</strong>으로 표현한다.
  </li>
  <li>
    <strong>다양한 형태의 데이터</strong>를 확보하는 가장 좋은 방법은 <strong>변환자</strong>를 사용하는 것이다.
  </li>
    <ul>
      <li>
        변환자는 <strong>인수로 DataFrame</strong>을 받고, 새로운 <strong>DataFrame을 반환</strong>한다.
      </li>
    </ul>
  <li>
    변환자는 꾸준히 업데이트 되기에 직접 확인하는 것이 좋다.
  </li>
</ul>

```python
# 1. 샘플 데이터셋 로드
sales = spark.read.format("csv")\
    .option("header", "true")\
    .option("inferSchema", "true")\
    .load("/workspace/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv")\
    .coalesce(5)\
    .where("Description IS NOT NULL")
fakeIntDF = spark.read.parquet("/workspace/Spark-The-Definitive-Guide/data/simple-ml-integers")
simpleDF = spark.read.json("/workspace/Spark-The-Definitive-Guide/data/simple-ml")
scaleDF = spark.read.parquet("/workspace/Spark-The-Definitive-Guide/data/simple-ml-scaling")

# 결과 확인.
sales.cache()
sales.show()
```

<br><br>

<h1>2. 변환자</h1>
<ul>
  <li>
    변환자는 다양한 방식으로 <strong>원시 데이터를 변환</strong>시키는 함수이다.
  </li>
  <li>
    Spark의 변환자는 <strong>변환 메서드</strong>만 포함한다. 데이터 변환 기능은 입력 데이터에 따라 변경되지 않기 때문이다.
  </li>
</ul>

```scala
// 1. Tokenizer 예.
//   - 주어진 문자열을 토큰화(tokenization)하고, 주어진 문자로 분할한다.
//   - 해당 과정에서 데이터를 학습하지는 않는다.
import org.apache.spark.ml.feature.Tokenizer

val tkn = new Tokenizer().setInputCol("Description")
tkn.transform(sales.select("Description")).show(false)
```

<br><br>

<h1>3. 전처리 추정자</h1>
<ul>
  <li>
    <strong>추정자</strong>는 수행하려는 변환이 <strong>입력 컬럼에 대한 데이터</strong> 또는 <strong>정보</strong>로 <strong>초기화</strong>되어야 할 때 필요하다.
  </li>
  <li>
    추정자는 <strong>단순 변환</strong>을 위해 맹목적으로 적용하는 <strong>일반 변환자 유형</strong>과 <strong>데이터에 따라</strong> 변환을 수행하는 <strong>추정자 유형</strong>이 있다.
  </li>
</ul>

```scala
// 1. 추정자인 StandardScaler를 사용해 각 차원이 평균이 0이고 분산이 1이 되도록 스케일링.
import org.apache.spark.ml.feature.StandardScaler

val ss = new StandardScaler().setInputCol("features")
ss.fit(scaleDF).transform(scaleDF).show(false)
```

<br>

<h2>3-1. 변환자 속성 정의하기</h2>
<ul>
  <li>
    모든 변환자는 적어도 <strong>입력 이름</strong>을 나타내는 <strong>inputCol</strong>과 <strong>출력 이름</strong>을 나타내는 <strong>outputCol</strong>을 지정해야한다.
  </li>
    <ul>
      <li>
        기본값이 정해져 있지만 <strong>set() 메서드를 활용해 수동</strong>으로 지정하는 것이 좋다.
      </li>
    </ul>
  <li>
    <strong>추정자</strong>는 특정 데이터셋에 <strong>변환자를 적합</strong>시키고 결과 객체에 대한 <strong>transform 메서드</strong>를 호출한다.
  </li>
  <li>
    <strong>MLlib</strong>은 각 DataFrame에서 사용하는 <strong>컬럼에 대한 메타데이터</strong>를 컬럼 자체의 <strong>속성</strong>으로 지정한다. 따라서 컬럼의 데이터 타입 뿐만아니라 <strong>메타데이터의 타입</strong>까지 활용하여 처리한다.
  </li>
</ul>

<br><br>

<h1>4. 고수준 변환자</h1>
<ul>
  <li>
    RFormula와 같은 <strong>고수준 변환자</strong>는 <strong>하나의 변환에서 여러 가지 변환</strong>을 간결하게 지정할 수 있다. 즉, 상위 수준에서 전체 처리.
  </li>
</ul>

<br>

<h2>4-1. RFormula</h2>
<ul>
  <li>
    <strong>RFormula</strong>는 <strong>R</strong>에서 빌려온 변환자로 데이터에 대한 <strong>변환을 선언적으로 간단히 지정</strong>할 수 있게 해준다.
  </li>
  <li>
    RFormula는 <strong>원-핫 인코딩(one-hot encoding)</strong>을 수행해 <strong>문자열로 지정된 범주화된 입력변수</strong>를 자동으로 처리한다.
  </li>
  <li>
    RFormula를 사용하면 <strong>숫자 컬럼은 Double 타입</strong>으로 변환 되지만 원-핫 인코딩되지는 않는다.
  </li>
  <li>
    <strong>레이블 컬럼이 String 타입</strong>인 경우 먼저 <strong>StringIndexer</strong>를 사용해 <strong>Double 타입으로 변환</strong>한다.
  </li>
  <li>
    RFormula의 기본 연산자는 R과 동일하며 이미 앞에서 정리 하였다.
  </li>
  <li>
    RFormula는 <strong>기본 설정된 레이블</strong>과 <strong>특정 컬럼</strong>을 사용하여 <strong>레이블을 지정</strong>하고 <strong>레이블 및 출력되는 특징들을 추정</strong>한다. 
  </li>
</ul>

```python
# 1. RFormula를 사용하여 모든 변수(.)를 사용하고 value1과 color, value2와 color 과의 
#    상호작용을 지정하여 새로운 특징 생성.
from pyspark.ml.feature import RFormula

supervised = RFormula(formula="lab ~ . + color:value1 + color:value2")
supervised.fit(simpleDF).transform(simpleDF).show()
```

<br>

<h2>4-2. SQL 변환자</h2>
<ul>
  <li>
    <strong>SQLTransformer</strong>를 사용하면 MLlib 변환 기능을 사용할 때처럼 <strong>Spark의 SQL</strong> 데이터 처리 라이브러리를 활용할 수 있다.
  </li>
    <ul>
      <li>
        유일한 차이점은 테이블명 대신 THIS 키워드를 사용한다는 것이다.
      </li>
      <li>
        일부 DataFrame 조작을 <strong>전처리 단계로 공식적으로 코딩</strong>하거나, 하이퍼파라미터 튜닝 시 특징에 <strong>서로 다른 SQL 식</strong>을 적용하고자 할 때 SQLTransformer을 사용하는 것이 좋다.
      </li>
      <li>
        SQLTransformer를 사용해서 변환하는 경우 출력 DataFrame에 <strong>새로운 컬럼</strong>으로 추가된다.
      </li>
    </ul>
  <li>
    초기 수집 상태의 원시 데이터로부터 현재 상태에 이르기까지 <strong>모든 조작</strong>을 SQLTransformer를 사용할 수 있다.
  </li>
    <ul>
      <li>
        각각의 조작을 <strong>변환자로 버전화</strong>하여 식별할 수 있다. 따라서 <strong>변환자만 교체</strong>하면 다양한 파이프라인을 구축하고 테스트가 가능하다.
      </li>
    </ul>
</ul>

```python
# 1. SQLTransformer 기본 예
from pyspark.ml.feature import SQLTransformer

basicTransformation = SQLTransformer()\
    .setStatement("""
        SELECT sum(Quantity), count(*), CustomerID
        FROM __THIS__
        GROUP BY CustomerID
        """)

basicTransformation.transform(sales).show()
```

<br>

<h2>4-3. 벡터 조합기</h2>
<ul>
  <li>
    <strong>VectorAssembler</strong>는 모든 특징을 <strong>하나의 큰 벡터</strong>로 연결하여 <strong>추정자에게 전달</strong>하는 기능을 한다.
  </li>
  <li>
    일반적으로 <strong>머신러닝 파이프라인의 마지막 단계</strong>에 사용되며 Boolean, Double 또는 Vector와 다양한 컬럼을 입력으로 사용한다.
  </li>
  <li>
    <strong>다양한 변환자</strong>를 사용하여 여러 가지 조작을 수행하고 <strong>결과를 모아야하는 경우</strong> 유용하다.
  </li>
</ul>

```python
# 2. 실행 예제
from pyspark.ml.feature import VectorAssembler

va = VectorAssembler().setInputCols(["int1", "int2", "int3"])
va.transform(fakeIntDF).show()
```

<br><br>

<h1>5. 연속형 특징 처리하기</h1>
<ul>
  <li>
    연속형 변수를 처리하는데 크게 두 개의 변환자가 있다.
  </li>
    <ul>
      <li>
        <strong>버켓팅</strong>을 통해 연속형 특징을 <strong>범주형 특징으로 변환</strong>한다.
      </li>
      <li>
        여러 요구사항에 따라 특징을 <strong>스케일링</strong>하거나 <strong>정규화</strong>.
      </li>
    </ul>
  <li>
    <strong>Double 타입</strong>에서만 작동하기에 다른 형식의 숫자값이 있다면 <strong>Double 타입으로 변경</strong>해야 한다.
  </li>
</ul>

```python
# 1. Double 타입 변환
contDF = spark.range(20).selectExpr("cast(id as double)")
```

<br>

<h2>5-1. 버켓팅</h2>
<ul>
  <li>
    <strong>버켓팅 또는 구간화(binning)</strong>에 대한 가장 직접적인 접근법은 <strong>Bucketizer</strong>를 사용하는 것이다.
  </li>
  <li>
    Bucketizer를 사용하면 주어진 연속형 특징을 지정한 <strong>버켓으로 분할</strong>한다.
  </li>
  <li>
    버켓이 어떤 형태로 생성되어야 하는지는 <strong>Double 타입 값으로 된 배열</strong>이나 <strong>리스트</strong>로 지정할 수 있다.
  </li>
  <li>
    버켓을 지정할 때에는 다음 세 가지 요구사항을 중족해야 한다.
  </li>
    <ul>
      <li>
        분할 배열의 최솟값은 DataFrame의 최솟값보다 작아야 한다. <strong>(분할 배열 최솟값 < DataFrame의 최솟값)</strong>.
      </li>
      <li>
        분할 배열의 최댓값은 DataFrame의 최댓값보다 커야 한다. <strong>(분할 배열의 최댓값 > DataFrame의 최댓값)</strong>.
      </li>
      <li>
        분할 배열은 <strong>최소 세 개 이상의 값</strong>을 지정해서 <strong>두 개 이상의 버켓</strong>을 만들도록 해야 한다.
      </li>
    </ul>
  <li>
    가능한 모든 범위를 포함하기 위해 scala에서는 scala.Double.NegativeInfinity 혹은 scala.Double.PositiveInfinity를 활용할 수 있다.
  </li>
  <li>
    <strong>파이썬</strong>에서는 가능한 모든 범위를 <strong>float("inf")</strong>와 <strong>float("-inf")</strong>와 같이 지정한다.
  </li>
  <li>
    <strong>null</strong> 또는 <strong>NaN</strong>의 경우 <strong>handleInvalid</strong>를 통해 처리할 수 있으며 <strong>유지(keep)하거나</strong>, <strong>error 또는 null로 처리</strong>하거나, 해당 row를 아예 <strong>건너뛸(skip)</strong>수 있다.
  </li>
  <li>
    <strong>QuantileDiscretizer</strong>를 통해 <strong>백분위수</strong>를 기준으로 분할할 수도 있다.
  </li>
  <li>
    <strong>setRelativeError</strong>를 사용해 <strong>근사치 계산에 대한 상대적 오류</strong>를 설정하여 버켓을 분할할 수도 있다.
  </li>
</ul>

```python
# 1. Bucketizer 사용.
from pyspark.ml.feature import Bucketizer

bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]
bucketer = Bucketizer().setSplits(bucketBorders).setInputCol("id")
bucketer.transform(contDF).show()
```

```python
# 2. QunantileDiscretizer()를 통해 백분위수로 버켓팅
from pyspark.ml.feature import QuantileDiscretizer

bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol("id")\
    .setOutputCol("result")
fittedBucketer = bucketer.fit(contDF)
fittedBucketer.transform(contDF).show()
```

<h3>5-1-1. 고급 버켓팅 기술</h3>
<ul>
  <li>
    앞서 소개한 방식 뿐만 아니라 지역성 기반 해싱(locality sensitivityhashing, LSH)과 같은 고급 기능 또한 제공한다.
  </li>
</ul>

<br>

<h2>5-2. 스케일링과 정규화</h2>
<ul>
  <li>
    버켓팅 뿐만 아니라 연속형 데이터 범위를 <strong>스케일링(scaling)</strong>하고 <strong>정규화</strong>하는 기능 또한 활용한다.
  </li>
  <li>
    스케일링과 정규화의 목적은 데이터가 <strong>동일한 척도</strong>로 구성되어 서로 합리적인 비교가 가능하게 하는 것이며 여기에는 다양한 기법이 존재하기에 찾아보는 것이 좋다.
  </li>
  <li>
    MLlib에서는 항상 <strong>Vector 타입의 컬럼</strong>에서 작업이 수행된다. 
  </li>
    <ul>
      <li>
        MLlib은 주어진 컬럼(Vector 타입)의 모든 row를 조산한 뒤 <strong>해당 벡터의 모든 차원</strong>을 <strong>고유한 특정 컬럼</strong>으로 처리한다.
      </li>
      <li>
        특정 컬럼으로 처리된 뒤에는 <strong>별도 스케일링</strong> 혹은 <strong>정규화 기능</strong>을 적용한다.
      </li>
      <li>
        즉, 하나의 컬럼이 조정되면 다른 컬럼도 해당 컬럼을 기준으로 조정되며 이를 <strong>구성 요소별 비교</strong>라 한다.
      </li>
    </ul>
</ul>

<br>

<h2>5-3. StandardScaler</h2>
<ul>
  <li>
    <strong>StandardScaler</strong>는 특징들이 <strong>평균이 0</strong>이고 <strong>표준편차가 1</strong>인 분포를 갖도록 데이터를 <strong>표준화</strong> 한다.
  </li>
  <li>
    <strong>withStd 플래그</strong>는 <strong>표준편차가 1</strong>이 되도록 스케일링 하는 것을 의미한다. 
  </li>
  <li>
    <strong>withMean 플래그</strong>는 스케일링을 하기 전 데이터를 <strong>센터링(centering)</strong>한다. 기본값은 false이다.
  </li>
</ul>

```python
# 1. StandardScaler 사용 예
from pyspark.ml.feature import StandardScaler

sScaler = StandardScaler().setInputCol("features")
sScaler.fit(scaleDF).transform(scaleDF).show()
```

<h3>5-3-1. MinMaxScaler</h3>
<ul>
  <li>
    <strong>MinMaxScaler</strong>는 벡터의 값을 주어진 <strong>최솟값에서 최댓값</strong>까지 <strong>비례 값</strong>으로 스케일링 한다.
  </li>
  <li>
    최솟값을 0으로 지정하고, 최댓값을 1로 지정하면 모든 값이 0에서 1사이가 된다.
  </li>
</ul>

```python
# 1. MinMaxScaler 적용.
from pyspark.ml.feature import MinMaxScaler

minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol("features")
fittedminMax = minMax.fit(scaleDF)
fittedminMax.transform(scaleDF).show()
```

<h3>5-3-2. MaxAbsScaler</h3>
<ul>
  <li>
    <strong>최대 절댓값 스케일러(MaxAbsScaler)</strong>는 각 값을 해당 컬럼의 <strong>최대 절댓값(최댓값을 절댓값 취한 값)으로 나눠서</strong> 데이터 범위를 조정한다.
  </li>
  <li>
    최대 절댓값을 기준으로 하기에 <strong>-1과 1사이의 값</strong>을 갖는다.
  </li>
</ul>

```python
# 1. MaxAbsScaler 적용.
from pyspark.ml.feature import MaxAbsScaler

maScaler = MaxAbsScaler().setInputCol("features")
fittedmaScaler = maScaler.fit(scaleDF)
fittedmaScaler.transform(scaleDF).show()
```

<h3>5-3-3. ElementwiseProduct</h3>
<ul>
  <li>
    <strong>ElementwiseProduct</strong>를 사용하면 벡터의 각 값을 <strong>임의의 값</strong>으로 조정할 수 있다.
  </li>
  <li>
    당연히 스케일링이 적용된 벡터의 차원은 관련 컬럼 내부의 벡터 차원과 일치해야 한다.
  </li>
</ul>

```python
# 1. ElementwiseProduct 적용
from pyspark.ml.feature import ElementwiseProduct
from pyspark.ml.linalg import Vectors

scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)
scalingUp = ElementwiseProduct()\
    .setScalingVec(scaleUpVec)\
    .setInputCol("features")
scalingUp.transform(scaleDF).show()
```

<h3>5-3-4. Normalizer</h3>
<ul>
  <li>
    <strong>Normalizer</strong>를 사용하면 <strong>여러 가지 표준 중 하나</strong>를 사용해 다차원 벡터를 스케일링할 수 있다.
  </li>
</ul>

```python
# 1. 맨해튼 표준을 1로 지정하여 Normalizer 실행.
from pyspark.ml.feature import Normalizer

manhattanDistance = Normalizer().setP(1).setInputCol("features")
manhattanDistance.transform(scaleDF).show()
```

<br><br>

<h1>6. 범주형 특징 처리하기</h1>
<ul>
  <li>
    일반적으로 데이터 전처리를 할 때는 일관성을 위해 <strong>모든 범주형 변수의 색인</strong>을 다시 생성하는 것이 좋다.
  </li>
    <ul>
      <li>
        시간이 지남에 따라 작업자의 인코딩 방식이 변경될 수 있기에 <strong>장기적으로 모델을 유지 관리</strong>하는 데 도움이 될 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>6-1. StringIndexer</h2>
<ul>
  <li>
    <strong>StringIndexer</strong>를 통해 <strong>문자열을 다른 숫자 ID에 매핑</strong>하는 방법은 가장 쉬운 색인 방법이다.
  </li>
  <li>
    Spark의 StringIndexer는 <strong>DataFrame에 첨부된 메타데이터를 생성</strong>하여 <strong>어떤 입력</strong>이 <strong>어떤 출력</strong>에 해당하는지 지정한다.
  </li>
    <ul>
      <li>
        이를 통해 후에 색인값에서 <strong>입력값</strong>을 다시 가져올 수 있다.
      </li>
    </ul>
  <li>
    StringIndexer는 입력 데이터에 적합시켜야 하는 추정자로 숫자 ID에 매핑되는 입력을 선택하려면 모든 입력을 확인해야 한다. 즉, <strong>매핑이 되는 label이 없다면 오류를 반환</strong>한다.
  </li>
    <ul>
      <li>
        옵션은 인덱서나 파이프라인을 <strong>학습시키기 전</strong> 또는 <strong>후</strong>에 설정할 수 있다.
      </li>
      <li>
        Spark 2.2에서는 유효하지 않은 입력에 대해 <strong>오류를 출력</strong>하거나 <strong>row를 건너뛰고</strong> 처리하는 옵션만 선택할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. StringIndexer를 통한 색인 적용.
from pyspark.ml.feature import StringIndexer

lblIndxr = StringIndexer().setInputCol("lab").setOutputCol("labelInd")
idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)
idxRes.show()
```

```python
# 2. 문자열이 아닌 컬럼에 StringIndexer 적용.
#   - 문자열이 아닌 경우 색인 생성 전 문자열로 변환한다.
valIndexer = StringIndexer().setInputCol("value1").setOutputCol("valueInd")
valIndexer.fit(simpleDF).transform(simpleDF).show()
```

<br>

<h2>6-2. 색인된 값을 텍스트로 변환하기</h2>
<ul>
  <li>
    일반적으로 머신러닝 <strong>수행 결과를 검사</strong>할 때는 다양하게 변환된 결과값을 <strong>기존 값으로 다시 매핑</strong>하여 진행한다.
  </li>
  <li>
    MLlib 분류 모델은 <strong>색인된 값</strong>을 사용하여 예측을 수행하기에 <strong>IndexToString</strong>을 통해 <strong>다시 원래 범주</strong>로 되돌릴 수 있다.
  </li>
  <li>
    MLlib은 <strong>메타데이터</strong>를 유지 관리하기 때문에 필요할 때마다 <strong>선택적으로 출력을 지정</strong>할 수 있다.
  </li>
</ul>

```python
# 1. IndexToString을 통해 색인값을 다시 원래 범주로 반환.
from pyspark.ml.feature import IndexToString

labelReverse = IndexToString().setInputCol("labelInd")
labelReverse.transform(idxRes).show()
```

<br>

<h2>6-3. 벡터 인덱싱하기</h2>
<ul>
  <li>
    <strong>VectorIndexer</strong>는 입력 벡터 내에 존재하는 범주형 데이터를 자동으로 찾아 <strong>0부터 시작하는 카테고리 색인</strong>을 사용하여 범주형 특징으로 변환한다.
  </li>
    <ul>
      <li>
        <strong>maxCategories</strong>를 사용해 <strong>지정한 값 이하의 고유한 값을 갖는 벡터</strong>의 모든 컬럼을 가져와 범주형 변수로 변환하도록 한다.
      </li>
      <li>
        가장 많으 고유한 값을 갖는 컬럼을 기준으로 하면 되기에 편하지만 간혹 <strong>값이 적은 연속형 변수가 포함</strong>되는 것을 주의해야 한다.
      </li>
    </ul>
</ul>

```python
# 1. VectorIndxer를 통해 범주형 변수 인덱싱
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.linalg import Vectors

idxIn = spark.createDataFrame([
    (Vectors.dense(1, 2, 3),1),
    (Vectors.dense(2, 5, 6),2),
    (Vectors.dense(1, 8, 9),3)
]).toDF("features", "label")

indxr = VectorIndexer()\
    .setInputCol("features")\
    .setOutputCol("idxed")\
    .setMaxCategories(2)
indxr.fit(idxIn).transform(idxIn).show()
```

<br>

<h2>6-4. 원-핫 인코딩</h2>
<ul>
  <li>
    원-핫 인코딩은 범주형 벼내수를 인덱싱한 후 추가로 수행하는 매우 보편적인 데이터 변환 기법이다.
  </li>
  <li>
    <strong>OneHotEncode</strong>는 각각의 고유한 값을 <strong>불리언 플래스(boolean flag)타입</strong>의 벡터 구성 요소로 변환한다.
  </li>
</ul>

```python
# 1. OneHotEncoder()를 통해 원-핫 인코딩 적용.
# 1. OneHotEncoder()를 통해 원-핫 인코딩 적용.
from pyspark.ml.feature import OneHotEncoder, StringIndexer

# 색상 라벨 → 숫자 인덱스
lblIndxr = StringIndexer().setInputCol("color").setOutputCol("colorInd")
colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select("color"))

# 인덱스 → 원-핫 인코딩
ohe = OneHotEncoder(inputCols=["colorInd"], outputCols=["colorVec"])
oheModel = ohe.fit(colorLab)       # fit()으로 모델 생성
encoded = oheModel.transform(colorLab)

encoded.show(truncate=False)
```

<br><br>

<h1>7. 텍스트 데이터 변환자</h1>
<h2>7-1. 텍스트 토큰화하기</h2>
<ul>
  <li>
    토큰화는 자유형 텍스트를 <strong>'토큰'</strong> 또는 <strong>개별 단어 목록</strong>으로 변환하는 프로세스이다.
  </li>
  <li>
    <strong>Tokenizer 클래스</strong>는 가장 쉬운 변환 방법이며 단어 컬럼을 <strong>공백</strong>을 기준으로 구분하여 <strong>단어 배열로 변환</strong>한다.
  </li>
</ul>

```python
# 1. Tokenizer 적용.
from pyspark.ml.feature import Tokenizer

tkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")
tokenized = tkn.transform(sales.select("Description"))
tokenized.show(20, False)
```

```python
# 2. RegexTokenizer를 통해 공백과 정규 표현식을 이용한 Tokenizer 생성.
#   - 정규 표현식은 Java의 정규 표현식(RegEx) 구문을 준수한다. 
from pyspark.ml.feature import RegexTokenizer

rt = RegexTokenizer()\
    .setInputCol("Description")\
    .setOutputCol("DescOut")\
    .setPattern(" ")\
    .setToLowercase(True)
rt.transform(sales.select("Description")).show(20, False)
```

```python
# 3. RegexTokenizer에 패턴을 사용.
from pyspark.ml.feature import RegexTokenizer

rt = RegexTokenizer()\
    .setInputCol("Description")\
    .setOutputCol("DescOut")\
    .setPattern(" ")\
    .setGaps(False)\
    .setToLowercase(True)
rt.transform(sales.select("Description")).show(20, False)
```

<br>

<h2>7-2. 일반적인 단어 제거하기</h2>
<ul>
  <li>
    텍스트를 토큰화한 뒤에는 일반적으로 <strong>불용어</strong>나 <strong>분석과 관련 없는 무의미한 용어</strong>를 필터링한다.
  </li>
  <li>
    Spark에서는 메서드를 통해 <strong>기본적으로 정의된 불용어</strong> 목록을 살펴볼 수 있으며 불용어는 <strong>대소문자를 구분하지 않는다</strong>.
  </li>
</ul>

```python
# 1. "DescOut" 컬럼에 들어 있는 토큰화된 문장에서 영어 불용어를 제거해서 새로운 컬럼(DescOut_removed)을 만들어 주는 기능을 수행합니다.
from pyspark.ml.feature import StopWordsRemover

englishStopWords = StopWordsRemover.loadDefaultStopWords("english")
stops = StopWordsRemover()\
    .setStopWords(englishStopWords)\
    .setInputCol("DescOut")
stops.transform(tokenized).show()
```

<br>

<h2>7-3. 단어 조합 만들기</h2>
<ul>
  <li>
    불용어를 제거하면서 함께 배치된 <strong>단어 조합</strong>을 살펴보는 것이 중요하다.
  </li>
  <li>
    단어 조합이란 기술적으로 <strong>길이가 n</strong>인 단어의 시퀀스, 즉 <strong>n-gram</strong>으로 간주된다.
  </li>
    <ul>
      <li>
        길이가 1인 n-gram을 <strong>unigram</strong>, 길이가 2인 것은 <strong>bigram</strong>, 길이가 3인 경우 <strong>trigram</strong>이라 한다.
      </li>
      <li>
        unigram은 문장을 한 단어씩, bigram은 두 단어씩, trigram은 세 단어씩 묶기에 순서가 중요하다.
      </li>
    </ul>
  <li>
    n-gram의 목적은 <strong>문장의 구조와 정보</strong>를 기존의 모든 단어를 개별적으로 살펴보는 것보다 더 잘 포착하기 위해서이다.
  </li>
  <li>
    Big Data Processing Made Simple을 bigram으로 변환하면 다음과 같다.
  </li>
    <ul>
      <li>
        Big Data
      </li>
      <li>
        Data Processing
      </li>
      <li>
        Processing Made
      </li>
      <li>
        Made Simple
      </li>
    </ul>
  <li>
    n-gram을 사용하면 <strong>공통으로 발생하는 단어 시퀀스</strong>를 파악할 수 있고 다시 <strong>머신러닝 알고리즘에 입력</strong>으로 사용할 수 있다.
  </li>
</ul>

```python
# 1. n-gram 적용.
from pyspark.ml.feature import NGram

unigram = NGram().setInputCol("DescOut").setN(1)
bigram = NGram().setInputCol("DescOut").setN(2)
unigram.transform(tokenized.select("DescOut")).show(10, False)
bigram.transform(tokenized.select("DescOut")).show(10, False)
```

<br>

<h2>7-4. 단어를 숫자로 변환하기</h2>
<ul>
  <li>
    단어 특징을 생성하면 모델에서 사용하기 위해 <strong>단어</strong>와 <strong>단어 조합 수</strong>를 세어야 한다.
  </li>
  <li>
    가장 간단한 방법은 주어진 문서(혹은 row)에서 단어 포함 여부를 <strong>binary 형식으로 셈</strong>하는 것이다.
  </li>
  <li>
    또 다른 방법은 <strong>CountVectorizer</strong>를 사용하는 것이다. <strong>TF-IDF</strong> 변환을 사용하면 모든 문서에서 주어진 단어의 발생 빈도에 따라 단어의 가중치를 재측정할 수 있다.
  </li>
  <li>
    CountVectorizer는 <strong>토큰화된 데이터</strong>에서 작동하며, 다음 두 가지 작업을 수행한다.
  </li>
    <ul>
      <li>
        모델을 적합하는 프로세스 동안 <strong>모든 문서</strong>에서 <strong>단어 집합</strong>을 찾은 다음 <strong>문서별</strong>로 <strong>해당 단어의 출현 빈도</strong>를 계산한다.
      </li>
      <li>
        변환 과정에서 DataFrame 컬럼의 각 row에서 주어진 단어의 발생 빈도를 계산하고 <strong>해당 row에 포함된 용어를 벡터 형태</strong>로 출력한다.
      </li>
    </ul>
  <li>
    CountVectorizer는 모든 <strong>row를 문서(document)</strong>로 취급하고 모든 <strong>단어를 용어(term)</strong>로 취급하며, 모든 <strong>용어의 집합을 어휘집(vocabulary)</strong>로 취급한다. 또한 이는 모두 조정 가능한 파라미터이다.
  </li>
    <ul>
      <li>
        어휘집에 포함될 용어에 대한 <strong>최소 용어 빈도</strong>와 어휘집에 포함되기 전에 용어가 나타나야 하는 <strong>최소 문서 빈도</strong> 그리고 전체 <strong>최대 어휘집 크기</strong>를 각각 설정할 수 있다.
      </li>
    </ul>
  <li>
    문서에 <strong>단어가 존재</strong>하는지 여부를 반환하기 위해서는 <strong>setBinary(true)</strong>를 사용할 수 있다.
  </li>
</ul>

```python
# 1. CountVectorizer를 기반으로한 단어 빈도 측정.
from pyspark.ml.feature import CountVectorizer

cv = CountVectorizer()\
    .setInputCol("DescOut")\
    .setOutputCol("countVec")\
    .setVocabSize(500)\
    .setMinTF(1)\
    .setMinDF(2)
fittedCV = cv.fit(tokenized)
fittedCV.transform(tokenized).show(10, False)
```

<h3>7-4-1. TF-IDF</h3>
<ul>
  <li>
    <strong>TF-IDF</strong>는 얼마나 많은 문서가 <strong>해당 용어를 포함</strong>하고 있는지에 따라 <strong>가중치</strong>를 부여한다. 
  </li>
  <li>
    더많이 문서에서 등장하는 단어는(the 등의 조사)낮은 가중치를, 더 적은 문서에서 등장하는 단어는 높은 가중치를 받는다.
  </li>
  <li>
    TF-IDF는 <strong>비슷한 주제를 갖는 문서</strong>를 찾는대도 활용할 수 있다.
  </li>
</ul>

```python
# 1. TF-IDF 활용하는 예

# 토큰화된 텍스트 중에서 특정 단어(red)가 포함된 문장만 골라내서 샘플링
tfIdfIn = tokenized\
    .where("array_contains(DescOut, 'red')")\
    .select("DescOut")\
    .limit(10)
tfIdfIn.show(10, False)

# TF-IDF 적용.
#   - red는 너무 많은 문서에 등장하기에 출력값이 많다.
#   - 또한 가중치가 나다.
#   - 출력은 총 어휘 크기, 문서에 나타나는 모든 단어의 해시, 각 용어의 가중치 등 세 가지 
#     다른 값을 사용하여 표현된다.
from pyspark.ml.feature import HashingTF, IDF

tf = HashingTF()\
    .setInputCol("DescOut")\
    .setOutputCol("TFOut")\
    .setNumFeatures(10000)
idf = IDF()\
    .setInputCol("TFOut")\
    .setOutputCol("IDFOut")\
    .setMinDocFreq(2)

idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show()
```

<br>

<h2>7-5. Word2Vec</h2>
<ul>
  <li>
    
  </li>
</ul>