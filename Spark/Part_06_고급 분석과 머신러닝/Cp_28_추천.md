<h1>1. 활용 사례</h1>
<ul>
  <li>
    <strong>영화 추천</strong>
  </li>
    <ul>
      <li>
        대표적으로 넷플릭스가 대규모 영화 추천을 위해 Spark를 기반으로한 추천 시스템을 활용하고 있다.
      </li>
    </ul>
  <li>
    <strong>과목 추천</strong>
  </li>
    <ul>
      <li>
        학교에서 비슷한 성향의 학습들이 공통으로 선호하거나 수강한 수업이 무엇인지 연구하여 학생들에게 수업을 추천.
      </li>
    </ul>
  <li>
    Spark는 추천 알고리즘으로 <strong>교차최소제곱(Alternating Least Square, ALS)</strong>을 제공한다.
  </li>
  <li>
    교차최소제곱 알고리즘은 <strong>협업 필터링(collaborative filtering) 기술</strong>을 사용해 <strong>사용자가 과거에 상호작용</strong>한 이벤트를 기반으로 추천을한다.
  </li>
  <li>
    추가로 교차최소제곱 변형이나 장바구니 분석을 위한 연관 규칙용 빈발 패턴 마이닝(Frequent Pattern Mining) 또한 제공한다.
  </li>
</ul>

<br><br>

<h1>2. 교차최소제곱 알고리즘을 사용하여 협업 필터링 구현하기</h1>
<ul>
  <li>
    교차죄소제곱은 각 <strong>아이템의 특징 벡터와 각 사용자의 특징 벡터의 내적</strong>이 해당 아이템에 대한 <strong>사용자의 평점과 근사</strong>하도록 각 <strong>사용자 및 아이템에 대해 k 차원의 특징 벡터</strong>를 찾는다.
  </li>
    <ul>
      <li>
        <strong>세 개의 컬럼(사용자 아이디, 아이템 아이디, 평점)</strong>을 갖는 데이터셋을 활용한다.
      </li>
      <li>
        <strong>평점</strong>은 직접 예측하고자 하는 수치형 평점과 같이 <strong>명시적</strong>일 수도 있고, 사용자와 아이템 간에 관찰되는 상호작용의 강도와 같이 <strong>암시적</strong>일 수도 있다.
      </li>
      <li>
        데이터셋이 준비되면 교차최소제곱 알고리즘은 사용자가 아직 평가하지 않은 아이템의 평점을 예측하는 데 사용할 수 있다.
      </li>
    </ul>
  <li>
    단, Spark가 제공하는 교차최소제곱 알고리즘은 <strong>아주 일반적</strong>이거나 <strong>많은 정보</strong>를 갖고 있는 서비스에 적합하다.
  </li>
    <ul>
      <li>
        사용자의 피드백 정보가 없다면 추천이 불가하기 때문이다.
      </li>
    </ul>
  <li>
    Spark는 수많은 데이터를 활용할 수 있기에 교차최소제곱을 매우 선호한다.
  </li>
</ul>

<br>

<h2>2-1. 모델 하이퍼파라미터</h2>
<ul>
  <li>
    <strong>rank</strong>
  </li>
    <ul>
      <li>
        사용자와 아이템을 학습하기 위한 <strong>특징 벡터의 차원을 결정</strong>한다.
      </li>
      <li>
        일반적으로는 <strong>반복 실험</strong>을 통해 값을 결정한다.
      </li>
      <li>
        값이 너무 큰 경우 학습셋에 과적합될 수 있고 반대로 너무 작은 값은 과소적합의 원인이 될 수 있다.
      </li>
    </ul>
  <li>
    <strong>alpha</strong>
  </li>
    <ul>
      <li>
        <strong>암시적 피드백 데이터(관찰된 행동 정보)</strong>를 학습할 때 사용한다.
      </li>
      <li>
        기본값은 10이고, 반복 실험을 통해 값을 유도한다.
      </li>
    </ul>
  <li>
    <strong>regParam</strong>
  </li>
    <ul>
      <li>
        모델 <strong>과적합을 방지</strong>하기 위해 <strong>일반화를 제어</strong>한다. 기본값은 0.1이며 다양한 값을 시도한다.
      </li>
    </ul>
  <li>
    <strong>implicitPrefs</strong>
  </li>
    <ul>
      <li>
        분석하려는 데이터가 <strong>암시적(true)</strong>인지 <strong>명시적(false)</strong>인지 지정하는 불리언값이다.
      </li>
      <li>
        클릭 또는 페이지 방문을 기반으로 하는 경우 암시적이기에 <strong>implicit</strong>을 지정한다.
      </li>
      <li>
        별점 같은 경우 명시적이기에 <strong>explicit</strong>을 지정한다.
      </li>
    </ul>
  <li>
    nonnegative
  </li>
    <ul>
      <li>
        true로 설정하면 비음수 제약 조건을 설정하여 <strong>음수가 아닌 특징 벡터</strong>만 반환되도록 할 수 있다.
      </li>
      <li>
        일부 앱에서 성능이 향상될 수 있으며 기본값은 false이다.
      </li>
    </ul>
</ul>

<br>

<h2>2-2. 학습 파라미터</h2>
<ul>
  <li>
    클러스터가 <strong>저수준에 분산</strong>되는 방식이기에 <strong>저수준까지 제어</strong>를 필요로한다. 따라서 학습 파라미터가 조금 다르다.
  </li>
  <li>
    클러스터 상에 분산된 데이터 그룹을 <strong>블록</strong>이라 한다.
  </li>
    <ul>
      <li>
        각 블록에 배치할 <strong>데이터양</strong>을 결정하는 것은 <strong>알고리즘을 학습시키는 데 걸리는 시간</strong>에 큰 영향을 미칠 수 있다.
      </li>
      <li>
        경험상 블록당 100~500만 정도로 설정하는 것이 좋다.
      </li>
      <li>
        마찬가지로 더 많은 블록이 좋은 성능을 보장하지 않는다.
      </li>
    </ul>
  <li>
    학습 파라미터들
  </li>
    <ul>
      <li>
        <strong>numUserBlocks</strong>
      </li>
        <ul>
          <li>
            <strong>사용자를 분할할 블록 수</strong>를 결정한다. 기본은 10이다.
          </li>
        </ul>
      <li>
        <strong>numItemBlocks</strong>
      </li>
        <ul>
          <li>
            <strong>아이템을 분할할 블록 수</strong>를 결정한다. 기본값은 10이다.
          </li>
        </ul>
      <li>
        <strong>checkpointInterval</strong>
      </li>
        <ul>
          <li>
            테크포인팅은 <strong>학습 과정 동안</strong> 진행되는 모델의 <strong>작업 내용을 저장</strong>하여 <strong>노드 오류</strong>를 보다 신속하게 복수할 있다.
          </li>
          <li>
            <strong>SparkContect.setCheckpointDir</strong>을 사용하여 <strong>체크포인트 디렉터리</strong>를 설정할 수 있다.
          </li>
        </ul>
      <li>
        <strong>seed</strong>
      </li>
        <ul>
          <li>
            임의 시드(random seed)를 지정하면 <strong>결과를 재연</strong>하는 데 도움이 된다.
          </li>
        </ul>
    </ul>
</ul>

<br>

<h2>2-3. 예측 파라미터</h2>
<ul>
  <li>
    교차최소제곱은 <strong>콜드 스타트 전략(coldStartStrategy로 설정) 예측 파라미터</strong>만 제공한다.
  </li>
  <li>
    <strong>콜드 스타트 문제</strong>는 새로운 사용자나 아이템 평점 이력이 없는 등 모델이 <strong>적절한 추천을 생성할 수 없을 때</strong> 발생한다.
  </li>
  <li>
    <strong>CrossValidator</strong> 혹은 <strong>TrainValidationSplit</strong>과 같은 단순 무작위 분할을 사용할 때에도 학습 데이터셋에 <strong>없었던 사용자 혹은 아이템</strong>이 평가셋에 존재하기에 콜드 스타트 문제가 발생한다.
  </li>
  <li>
    콜드 스타트 문제에 대해 <strong>coldStartStragegy 파라미터를 drop</strong>하여 <strong>NaN</strong>을 값을 포함하는 예측 <strong>DataFrame의 row를 제거</strong>하고 평가지표 또한 <strong>NaN 이외의 데이터를 대상</strong>으로 유효한 계산을 수행한다.
  </li>
</ul>

<br>

<h2>2-4. 실습 예제</h2>

```python
# 1. MovieLens 영화 평점 데이터셋을 사용한 추천 알고리즘 실습.
from pyspark.ml.recommendation import ALS
from pyspark.sql import Row

ratings = spark.read.text("/workspace/Spark-The-Definitive-Guide/data/sample_movielens_ratings.txt")\
    .rdd.toDF()\
    .selectExpr("split(value, '::') as col")\
    .selectExpr(
        "cast(col[0] as int) as userId",
        "cast(col[1] as int) as movieId",
        "cast(col[2] as float) as rating",
        "cast(col[3] as long) as timestamp")
training, test = ratings.randomSplit([0.8, 0.2])
als = ALS()\
    .setMaxIter(5)\
    .setRegParam(0.01)\
    .setUserCol("userId")\
    .setItemCol("movieId")\
    .setRatingCol("rating")
print(als.explainParams())
alsModel = als.fit(training)
predictions = alsModel.transform(test)
```

```python
# 2. movieId와 영화별 상위 사용자를 DataFrame으로 반환.
alsModel.recommendForAllUsers(10)\
    .selectExpr("userId", "explode(recommendations)").show()
alsModel.recommendForAllItems(10)\
    .selectExpr("movieId", "explode(recommendations)").show()
```

<br><br>

<h1>3. 추천을 위한 평가기</h1>
<ul>
  <li>
    교차최소제곱을 사용하면서 콜드 스타트 전략을 다루는 경우 <strong>자동 모델 평가기</strong>를 설정할 수 있다.
  </li>
  <li>
    명확한 근거는 없지만 콜드 스타트 같은 문제는 <strong>회귀 문제</strong>이며 따라서 특정 사용자가 주어졌을 때 값(평점)을 예측하므로 사용자의 <strong>평점 예측값과 실젯값 사이의 차이</strong>를 줄이는 최적화를 수행한다.
  </li>
  <li>
    최적화를 위한 평가는 <strong>RegressionEvaluator</strong>를 사용하여 작업을 수행하며 파이프라인에 배치하여 자동화할 수도 있다.
  </li>
    <ul>
      <li>
        이 경우 우선 콜드 스타트 전략을 NaN 대신 <strong>drop</strong>으로 설정한 후 운영 시스템에서 <strong>실제로 예측할 시점이 되면 NaN</strong>으로 다시 전환해야 한다.
      </li>
    </ul>
</ul>

```python
# 1. python을 통해 추천 시스템 평가하기
from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator()\
    .setMetricName("rmse")\
    .setLabelCol("rating")\
    .setPredictionCol("prediction")
rmse = evaluator.evaluate(predictions)
print("Root-mean-square error = %f" % rmse)
```

<br><br>

<h1>4. 성과 평가지표</h1>
<h2>4-1. 회귀 평가지표</h2>
<ul>
  <li>
    각 예측값이 사용자 및 아이템의 실제 평가 결과와 <strong>얼마나 가까운지</strong> 간단히 볼 수 있다.
  </li>
</ul>

```python
# 1. 회귀 평가지표 예
from pyspark.mllib.evaluation import RegressionMetrics

regComparison = predictions.select("rating", "prediction")\
    .rdd.map(lambda x: (x(0), x(1)))
metrics = RegressionMetrics(regComparison)
```

<br>

<h2>4-2. 순위 평가지표</h2>
<ul>
  <li>
    <strong>RankingMetric</strong>을 사용하면 추천 결과를 <strong>사용자가 표현한 실제 평점 정보</strong>와 비교할 수 있다.
  </li>
    <ul>
      <li>
        순위보다 알고리즘이 이미 순위가 매겨진 아이템을 사용자에게 <strong>다시 추천</strong>하는지 여부에 초점을 맞춘다.
      </li>
    </ul>
</ul>

```python
# 1. 실습을 위해 사용자가 순위를 높게 매긴 영화를 수집한다.
from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics
from pyspark.sql.functions import col, expr

perUserActual = predictions\
    .where("rating > 2.5")\
    .groupBy("userId")\
    .agg(expr("collect_set(movieId) as movies"))
```

```python
# 2. 예측 결과를 DataFrame으로 반환.
perUserPredictions = predictions\
    .orderBy(col("userId"), expr("prediction DESC"))\
    .groupBy("userId")\
    .agg(expr("collect_list(movieId) as movies"))
```

```python
# 3. RankingMetrics에 전달.
#   - 두 DataFrame의 조합으로 구성된 RDD를 받는다.
perUserActualvPred = perUserActual.join(perUserPredictions, ["userId"]).rdd\
    .map(lambda row: (row[1], row[2][:15]))
ranks = RankingMetrics(perUserActualvPred)
```

```python
# 4. 순위로부터 평가지표 검토.
ranks.meanAveragePrecision
ranks.precisionAt(5)
```

<br><br>

<h1>5. 빈발 패턴 마이닝</h1>
<ul>
  <li>
    교차최소제곱 외에도 MLlib에서 추천을 위해 제공하는 또 다른 도구로는 <strong>빈발 패턴 마이닝(frequent pattern mining)</strong>이 있다.
  </li>
    <ul>
      <li>
        <strong>원시 데이터</strong>를 기반으로 연관 규칙을 찾아내는 알고리즘으로 종종 <strong>장바구니 분석</strong>이라고도 불린다.
      </li>
      <li>
        Spark에서 빈발 패턴 마이닝 알고리즘으로는 <strong>FP-성장 알고리즘</strong>을 사용한다.
      </li>
    </ul>
</ul>