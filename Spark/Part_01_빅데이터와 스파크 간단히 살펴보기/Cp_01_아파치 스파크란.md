<ul>
  <li>
    Apache Spark는 통합 컴퓨팅 엔진이며 클러스터 환경에서 데이터를 <strong>병렬로 처리</strong>하는 라이브러리이다.
  </li>
  <li>
    다양한 언어(Python, R, Java, SQL, etc)를 지원한다.
  </li>
  <li>
    단일 노트북 환경부터 수천 대의 서버로 구성된 클러스터 환경까지 다양한 환경에서 실행할 수 있다.
  </li>
</ul> 

<br>
<h1>1. 아파치 스파크의 철학</h1>
<h2>1-1. 통합</h2>
<ul>
  <li>
    데이터 모델링에는 python의 통합 라이브러리를, 웹 개발에는 Django와 같은 통합 프레임워크를 사용하는 것처럼 Spark는 <strong>병렬 데이터</strong>를 처리하는 통합 엔진이다.
  </li>
  <li>
    데이터 읽기, SQL 처리, 머신러닝, 스트림 처리 등 다양한 데이터 분석 작업을 같은 <strong>연산 엔진</strong>과 <strong>일관성 있는 API</strong>로 수행할 수 있도록 설계되었다.
  </li>
    <ul>
      <li>
        만약 API가 없다면 스파크 기반의 <strong>라이브러리를 직접 생성</strong>할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>1-2. 컴퓨팅 엔진</h2>
<ul>
  <li>
    Spark는 데이터 처리에 집중 하였기에 별도로 <strong>데이터 저장소</strong>를 지원하지 않는다.
  </li>
    <ul>
      <li>
        대신 Azure Storage, S3, Apache Hadoop, Apache Cassandra 등 여러 저장소와 <strong>호환</strong>이 된다.
      </li>
    </ul>
  <li>
    분산 환경은 '분산 저장소 → 중앙 저장소'로 데이터의 송수신 네트워크 비용이 크지만 Spark는 <strong>'개별 저장소 연산 처리 → 중앙 저장소'</strong> 구조로 비용이 적다.
  </li>
  <li>
    하둡의 경우 파일 시스템과 컴퓨팅 시스템(맵리듀스)이 혼합되어 있어 <strong>다른 저장소의 데이터에 접근</strong>하는 애플리케이션을 개발하는 것이 어려우나 Spark는 그렇지 않다는 장점이 있다.
  </li>
</ul>

<br>

<h2>1-3. 라이브러리</h2>
<ul>
  <li>
    Spark의 가장 큰 장점은 데이터 분석 작업에 필요한 <strong>통합 API</strong>를 제공하는 <strong>통합 엔진 기반의 자체 라이브러리</strong>이다.
  </li>
  <li>
    Spark에는 <strong>Spark SQL</strong>, <strong>Spark MLlib</strong>, <strong>Spark Streaming</strong>, <strong>GraphX</strong> 등 여러 라이브러리가 존재하며 서드 파이 라이프러리도 존재한다.
  </li>
  <li>
    Spark의 외부 라이브러리 목록은 <strong>spark-packages.org</strong>에서 확인할 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 스파크의 등장 배경</h1>
<ul>
  <li>
    과거에는 프로세서의 성능을 올리는 <strong>Scale Up</strong>을 기반으로 발전하였다.<br>→ 어느 순간 Scale Up의 방식은 <strong>비용의 과다</strong>로 <strong>Scale Out</strong>을 채택하는 비율이 증가.<br>→ 데이터 저장과 수집 기술은 발전하지만 <strong>Scale Out 환경을 처리</strong>할 애플리케이션이 필요.<br>→ <strong>Spark</strong>와 같은 병렬 처리 애플리케이션 등장.
  </li>
</ul>

<br><br>

<h1>3. 스파크의 역사</h1>
<ul>
  <li>
    병렬 처리의 대표주자는 <strong>MapReduce</strong>를 활용하는 Hadoop.<br>→ 이를 통해 <strong>클러스터 컴퓨팅</strong>이 엄청난 잠재력이 있음을 확인.<br>→ 문제는 MapReduce를 사용하는 대규모 애플리케이션의 <strong>난이도와 효율성</strong>.<br>→ 전통 ML은 10~20회의 처리를 필요로 하였고 MapReduce는 <strong>디스크 방식</strong>이기에 각 반복마다 데이터를 처음부터 읽어야 했음. (캐시 X).<br>→ 스파크는 여러 단계로 된 애플리케이션을 <strong>함수형 프로그래밍 API</strong>로 설계하고 연산 단계 사이의 <strong>메모리를 효율적으로 공유</strong>할 수 있는 새로운 엔진 기반 API 구현<br>→ 시작은 배치 애플리케이션이었지만 점차 대화형 데이터 분석, 비정형 쿼리 등의 기능을 제공.<br>→ Spark의 가능성을 확인하고 <strong>라이브러리들이 확장</strong>되기 시작<br>→ 2013년 <strong>Apache 재단</strong>에 Spark를 기부<br>→ 시대에 맞게 지속적으로 API가 추가되며 현재까지도 사용.
  </li>
</ul>

<br><br>

<h1>4. 스파크의 현재와 미래</h1>
<ul>
  <li>
    단순히 Spark가 빠르게 성장하고 있다는 이야기.
  </li>
</ul>

<br><br>

<h1>5. 스파크 실행하기</h1>
<ul>
  <li>
    책이 오래되었기에 Spark:3.5.2를 사용하여 Docker Container를 띄운다.
  </li>
  <li>
    추후 기능을 확장하게 되면 Dockerfile 등을 사용하는 것으로.
  </li>
</ul>