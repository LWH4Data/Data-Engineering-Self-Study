<ul>
  <li>
    spark-submit 명령으로 운영용 애플리케이션 실행
  </li>
  <li>
    Dataset: 타입 안정성(type-safe)을 제공하는 구조적 API
  </li>
  <li>
    구조적 스트리밍
  </li>
  <li>
    머신러닝과 고급 분석
  </li>
  <li>
    RDD: 스파크의 저수준 API
  </li>
  <li>
    SparkR
  </li>
  <li>
    서드파티 패키지 에코시스템
  </li>
</ul>

<br>

<h1>1. 운영용 애플리케이션 실행하기</h1>
<ul>
  <li>
    <strong>spark-submit</strong> 명령을 사용하면 대화형 shell에서 개발한 프로그램을 <strong>운영용 애플리케이션으로 전환</strong>할 수 있다.
  </li>
  <li>
    spark-submit 명령은 애플리케이션 코드를 <strong>클러스터에 전송</strong>하여 <strong>실행</strong>하는 역할을 한다.
  </li>
  <li>
    클러스터에 전송된 애플리케이션은 <strong>작업이 종료</strong>되거나 <strong>에러가 발생</strong>할 때까지 실행된다.
  </li>
  <li>
    spark application은 Stand Alone, Mesos, YARN 클러스터 매니저를 이용해 실행된다.
  </li>
</ul>

```bash
# 1. pyspark를 사용하지 않고 spark 컨테이너 접속
docker run --rm -it \
  # 컨테이너에 접속했을 때 디렉터리
  -w /opt/spark \
  apache/spark:3.5.2 bash
```

```bash
# 2. ./bin/spark-submit의 spark-submit을 통해 애플리케이션을 실행한다.
# (현재 버전에 맞게 코드 수정).

# jar 파일
./bin/spark-submit \
--class org.apache.spark.examples.SparkPi \
--master local ./examples/jars/spark-examples_2.12-3.5.2.jar 10

# python
# ./bin/spark-submit을 통해 파이썬 스크랩트 실행
./bin/spark-submit \
--master local \
./example/src/main/python/pi.py 10
```

<br><br>

<h1>2. Dataset: 타입 안정성을 제공하는 구조적 API</h1>
<ul>
  <li>
    Dataset은 자바와 스칼라의 <strong>정적 타입 코드(statically typed code)</strong>를 지원하기 위해 고안된 구조적 API이다.
  </li>
    <ul>
      <li>
        <strong>정적 타입 코드</strong>: 변수/표현식의 <strong>타입</strong>을 컴파일 시점에 결정하는 것을 의미한다. (e.b. int a = 3).
      </li>
      <li>
        정적 타입 코드를 지원하기에 <strong>타입 안정성</strong>을 지원하고, 초기화에 사용한 클래스 대신 다른 클래스를 사용해 접근할 수 없다.
      </li>
      <li>
        DataFrame의 <strong>record</strong>를 사용자가 자바나 스칼라로 정의한 <strong>클래스</strong>에 할당하고, <strong>자바의 ArrayList</strong> 혹은 <strong>스칼라의 Seq 객체</strong> 등의 고정 타입형 컬렉션으로 다룰 수 있다.
      </li>
      <li>
        Dataset 클래스는 내부 객체의 데이터 타입(T)을 매개변수로 사용한다.<br>(자동 타입 분석 → Dataset의 표 형식에 적합한 스키마 생성).
      </li>
    </ul>
  <li>
    Dataset은 필요한 경우 <strong>선택적으로 활용</strong>이 가능하다.
  </li>
    <ul>
      <li>
        (PySpark/SQL/DF 기준) 연산 결과는 보통 <strong>DataFrame</strong>으로 다뤄진다.<br>
        → 다만 Scala/Java에서 <strong>Dataset[T]</strong>의 typed 연산을 쓰면 결과는 <strong>Dataset[T]</strong>로 유지되고,<br>
        → RDD API를 쓰면 결과는 <strong>RDD</strong>로 유지된다.
      </li>
      <li>
        <strong>타입 안정성</strong>은 <strong>Dataset[T]</strong>가 제공하는 <strong>구조적(고수준) API</strong>의 특성이다.<br>
        → RDD는 <strong>저수준 API</strong>로, 스키마 기반 최적화나 SQL 통합이 없다.
      </li>
      <li>
        반환된 DataFrame을 뷰로 등록하면 <strong>SQL</strong>로 빠르게 분석할 수 있다.
      </li>
    </ul>
  <li>
    collect 메서드나 take 메서드를 호출하면 Dataset에 <strong>a매개변수로 지정한 타입</strong>의 객체를 반환한다.
  </li>
    <ul>
      <li>
        이는 <strong>코드 변경없이 안정성</strong>을 보장할 수 있음을 의미하며 로컬이나 분산 클러스터에서 데이터를 안전하게 다룰 수 있게한다.
      </li>
    </ul>
</ul>

<br><br>

<h1>3. 구조적 스트리밍</h1>
<ul>
  <li>
    <strong>구조적 스트리밍</strong>은 스파크 2.2 버전에서 <strong>안정화(production-ready)</strong>된 <strong>스트림 처리용 고수준 API</strong>이다.
  </li>
  <li>
    구조적 스트리밍 방식을 활용하면 <strong>지연 시간</strong>을 줄이고, <strong>증분 처리</strong>할 수 있다.
  </li>
    <ul>
      <li>
        <strong>증분</strong>: 기존의 데이터는 두고 <strong>새로 추가된 데이터를 활용</strong>하는 방식. (캐시와 유사).
      </li>
    </ul>
  <li>
    배치 처리용 코드를 일부 수정하여 스트리밍 처리를 구현하고, 값을 빠르게 얻을 수 있다는 장점이 있다.
  </li>
  <li>
    <strong>프로토타입을 배치 잡으로 개발 → 스트리밍 잡으로 변환</strong>이 가능하기에 개념을 잡기 쉽다. 
  </li>
  <li>
    스트리밍 코드의 차이는 read 메서드 대신 <strong>readStream 메서드</strong>를 사용한다는 점이다.
  </li>
  <li>
    스트리밍 action은 어디가에 <strong>흐르는 데이터를 저장</strong>할 곳이 필요하기에 count와 같은 일반적인 <strong>정적 메서드와는 다른 특성</strong>을 갖는다.
  </li>
  <li>
    스트리밍 데이터는 실제로 흐르기 때문에 <strong>데이터를 읽을 수록</strong> 새로운 데이터로 테이블의 구성이 변경된다.
  </li>
  <li>
    Spark는 데이터를 처리하는 시점이 아니라 <strong>이벤트 시간</strong>에 따라 윈도우를 구성한다.
  </li>
    <ul>
      <li>
        기존 Spark streaming의 단점을 구조적 스트리밍으로 보완할 수 있으며 5부에 다룬다.
      </li>
    </ul>
</ul>

```bash
# 1. 데이터 경로를 볼륨으로 마운트하여 Spark 도커 띄우기
docker run --rm -it \
  -p 4040:4040 \
  -v /mnt/c/Users/SSAFY/Desktop/spark-prac:/opt/spark-data \
  apache/spark:3.5.2 /opt/spark/bin/pyspark
```

```python
# 2. 샘플 데이터 불러오기
staticDataFrame = spark.read.format("csv")\
.option("header", "true")\
.option("inferSchema", "true")\
.load("/opt/spark-data/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv")

# 현재 DataFrame을 임시 뷰로 등록하여 retail_data로 조회되도록 함.
staticDataFrame.createOrReplaceTempView("retail_data")

# 데이터프레임의 스키마를 가져 옴.
staticSchema = staticDataFrame.schema
```

```python
# 3. 날짜 데이터를 그룹화하여 조회
from pyspark.sql.functions import window, col

staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")\
  .show(5)
```

```python
# 4. 3번의 코드를 로컬에서 실행하기 위해 partition의 수를 5 개로 제한.

# < partition의 수를 200 → 5 줄이는 이유 >
# - 로컬에서 작은 데이터를 테스트하는데 너무 많은 shuffle partition이 존재.
#   → 작업 시간은 짧은데 오히려 partition 수로 인해 오버헤드가 너무 커진다.
#   → Partition의 수를 줄여 개별 파티션당 작업 량을 늘리고 오버헤드를 낮춘다.
# - 오버헤드: 실제 일과 직접 관련이 없는 부가 비용.
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

```python
# 5. 스트리밍 데이터프레임 생성
# - 스트리밍: 계속 들어오는 데이터흐름.
# - Spark는 스트리밍을 증분처리함. (기본 마이크로배치).
#   → 지정한 폴더에 새 csv 파일이 추가될 때마가 해당 파일을 마이크로배치로 읽는다.
streamingDataFrame = spark.readStream\
.schema(staticSchema)\
.option("maxFilesPerTrigger", 1) \
.format("csv")\
.option("header", "true")\
.load("/opt/spark-data/Spark-The-Definitive-Guide/data/retail-data/by-day/*.csv")
```

```python
# 6. 생성됐는지 확인.
streamingDataFrame.isStreaming
# True 출력
```

```python
# 7. 생성한 스트리밍 데이터프레임에 DataFrame과 동일한 비즈니스 로직 테스트. 
# (총 판매 금액 계산).

# 현재 작업도 역시나 지연 연산의 상태이다.
purchaseByCustomerPerHour = streamingDataFrame\
.selectExpr(
"CustomerId",
"(UnitPrice * Quantity) as total_cost",
"InvoiceDate")\
.groupBy(
col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
.sum("total_cost")
```

```python
# 8. 7 번에서 작성한 스트리밍 지연 연산을 액션으로 수행.

# 트리거 실행
purchaseByCustomerPerHour.writeStream\
# 트리거 싫행 후 데이터를 갱신하게 될 인메모리 테이블에 데이터 저장.
# 스파크는 이전 집계값보다 더 큰 값이 발생할 경우에만 인메모리 테이블을 갱신한다.
#   → 언제나 가장 큰 값을 얻을 수 있다.
.format("memory")\
.queryName("customer_purchases")\
.outputMode("complete")\
.start()
```

```python
# 9. 스트림이 시작되면 쿼리 실행 결과가 어떤 형태로 인메모리 테이블에 기록되는지 확인 가능.
spark.sql("""
SELECT * 
FROM customer_purchases
ORDER BY `sum(total_cost)` DESC
""")\
.show(5)
```

```python
# 10. 출력을 메모리가 아니라 console에서 확인
purchaseByCustomerPerHour.writeStream
# console 설정
.format("console")
.queryNmae("customer_purchases_2")
.outputMode("complete")
.start()
```

<br><br>

<h1>4. 머신러닝과 고급 분석</h1>
<ul>
  <li>
    Spark는 내장된 머신러닝 알고리즘 라이브러리인 <strong>MLlib</strong>을 활용하여 대규모 머신러닝을 수행한다.
  </li>
  <li>
    MLlib을 활용하면 대용량 데이터를 대상으로 전처리(preprocessing), 멍잉(munging), 모델 학습(model training), 예측(prediction)을 할 수 있다.
  </li>
  <li>
    <strong>구조적 스트리밍</strong>을 예측할 때에도 MLlib에서 학습시킨 다양한 모델을 사용할 수 있다.
  </li>
  <li>
    Spark는 분류(classification), 회귀(regression), 군집화(clustering), 딥러닝(deep learning) 등 다양한 머신러닝과 정교한 API를 제공한다.
  </li>
  <li>
    실습 과정이기에 하단의 실습을 따라가며 추가적인 학습을 진행하면 된다.
  </li>
</ul>

```python
# 1. k-means 군집화를 하기 위해 샘플 데이터의 스키마 확인
staticDataFrame.printSchema()
```

```python
# 2. k-means 군집화를 수행하기 위해 모든 데이터를 수치형으로 변환.
from pyspark.sql.functions import date_format, col

preppedDataFrame = staticDataFrame\
.na.fill(0)\
.withColumn("day_of_week", date_format(col("InvoiceDate"), "EEEE"))\
.coalesce(5)
```

```python
# 3. 특정 구매가 이루어진 날짜를 기준으로 학습 데이터와 검증 데이터를 분리

# train dataset
trainDataFrame = preppedDataFrame\
.where("InvoiceDate < '2011-07-01'")

# validation dateset
testDataFrame = preppedDataFrame\
.where("InvoiceDate >= '2011-07-01'")
```

```python
# 4. 분할 결과 확인
# train dataset
trainDataFrame.count()

# validation dataset
testDataFrame.count()
```

```python
# 5. StringIndexer 트랜스포메이션을 통한 데이터 전처리
# Label encoding을 수행한다.
from pyspark.ml.feature import StringIndexer

indexer = StringIndexer()\
.setInputCol("day_of_week")\
.setOutputCol("day_of_week_index")
```

```python
# 6. OneHotEncoder를 통한 인코딩 (실제 예제 적용)
from pyspark.ml.feature import OneHotEncoder

encoder = OneHotEncoder()\
.setInputCol("day_of_week_index")\
.setOutputCol("day_of_week_encoded")
```

```python
# 7. UnitPrice, Quantity, day_of_week_encoded 세 속성을 벡터화

# 하나의 Vector 컬럼 안에는 하나의 벡터가 들어가고, 해당 백터의 성분으로 개별 속성들의 
# 값이 들어간다.
from pyspark.ml.feature import VectorAssembler

vectorAssembler = VectorAssembler()\
.setInputCols(["UnitPrice", "Quantity", "day_of_week_encoded"])\
.setOutputCol("features")
```

```python
# 8. 기존의 데이터를 제외하고 새로 들어온 데이터에만 전처리가 적용되도록 파이프 라인 구축.
from pyspark.ml import Pipeline

transformationPipeline = Pipeline()\
.setStages([indexer, encoder, vectorAssembler])
```

```python
# 9. transformer를 dataset에 fit(적합)한다.
fittedPipeline = transformationPipeline.fit(trainDataFrame)
```

```python
# 10. train dataset에 적합한 transformer를 적용
transformedTraining = fittedPipeline.transform(trainDataFrame)
```

```python
# 11. 캐싱을 사용하여 변환된 데이터 셋의 복사본을 메모리에 저장.
# - 전체 파이프라인을 재실행하는 것보다 빠르게 반복적으로 데이터 셋에 접근이 가능하여 속도가 빠르다.
transformedTraining.cache()
```

```python
# 12. 모델 적용을 위해 모델 관련 클래스를 import 하고, 인스턴스 생성.
# - 학습되지 않은 모델 초기화
# - 학습 전 알고리즘 명칭: Algoritm

from pyspark.ml.clustering import KMeans

kmeans = KMeans()\
.setK(20)\
.setSeed(1)
```

```python
# 13. 초기화된 모델 학습.
# 학습 후 알고리즘 명칭: AlgoritmModel
kmModel = kmeans.fit(transformedTraining)
```

```python
# 14. 모델 성능 평가를 위해 학습된 데이터 평가.

# TestDataFrame 전처리 적용
transformedTest = fittedPipeline.transform(testDataFrame)

# 모델 적용
predTest = kmModel.transform(transformedTest)

# 모델 평가 (Spark 3.0 이상으로 리팩토링)\
from pyspark.ml.evaluation import ClusteringEvaluator

evaluator = ClusteringEvaluator(
  metricName="silhouette",
  distanceMeasure="squaredEuclidean",
  featuresCol="features",
  predictionCol="prediction",
)
sil = evaluator.evaluate(predTest)
print(f"Silhouette (test): {sil:4f}")
```

<br><br>

<h1>5. 저수준 API</h1>
<ul>
  <li>
    Spark는 RDD를 통해 Java와 Python 객체를 다루는 데 필요한 <strong>다양한 기본 기능(저수준 API)</strong>을 제공한다.
  </li>
  <li>
    Spark의 거의 모든 기능은 RDD를 기반으로 만들어 졌다.
  </li>
  <li>
    <strong>원시 데이터</strong>를 읽을 때에는 <strong>구조적 API</strong>를 사용하는 것이 좋으나 RDD를 사용하면 다음의 장점들이 있다.
  </li>
    <ul>
      <li>
        partition과 같은 물리적 실행 특성을 결정할 수 있어 DataFrame보다 <strong>더 세밀한 제어</strong>가 가능하다.
      </li>
      <li>
        드라이버 시스템의 메모리에 저장된 원시 데이터를 <strong>병렬처리(parallelize)</strong>할 수 있다.
      </li>
    </ul>
  <li>
    DataFrame은 언어에 상관없이 동일한 실행 특성을 제공하지만, <strong>RDD</strong>는 <strong>세부 구현 방식에 차이</strong>를 보인다.
  </li>
  <li>
    최신 버전의 Spark는 기본적으로 RDD를 사용하지 않는다. 단, <strong>비정형 데이터</strong>나 <strong>정제되지 않은 원시 데이터</strong>를 처리할 때에는 RDD를 사용해야 한다.
  </li>
</ul>

```python
# 1. 간단한 숫자를 이용해 RDD를 생성.
from pyspark.sql import Row

# RDD 생성 후 DataFrame으로 변환
spark.sparkContext.parallelize([Row(1), Row(2), Row(3)]).toDF()
```

<br><br>

<h1>6. SparkR</h1>
<ul>
  <li>
    프로젝트에서 pyspark만을 사용할 것이기에 다루지 않는다.
  </li>
</ul>

<br><br>

<h1>7. 스파크의 에코시스템과 패키지</h1>
<ul>
  <li>
    spark-packages.org에서는 커뮤니티에서 개발된 여러 패키지들이 있고 이를 활용하여 개발을 진행할 수 있다.
  </li>
</ul>