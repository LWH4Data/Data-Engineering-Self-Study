<ul>
  <li>
    DataFrame과 SQL을 사용해 클러스터, 스파크 애플리케이션 그리고 구조적 API를 배운다.
  </li>
</ul>

<br>

<h1>1. 스파크의 기본 아키텍처</h1>
<ul>
  <li>
    <strong>컴퓨터 클러스터</strong>란 여러 컴퓨터의 자원을 모아 하나의 컴퓨터처럼 사용할 수 있게하는 것을 의미한다.
  </li>
  <li>
    Spark는 클러스터의 <strong>데이터 터리 작업</strong>을 관리하고 조율한다.
  </li>
    <ul>
      <li>
        Spark의 연산에 사용되는 <strong>클러스터 매니저</strong>로는 Spark standalone 클러스터 매니저, Hadoop YARN, Mesos 등이 있다.
      </li>
    </ul>
  <li>
    사용자 Spark 매니저에 Spark application 제출.<br>→ Spark가 application에 필요한 자원을 할당.<br>→ 사용자는 할당 받은 자원으로 작업 처리.
  </li>
</ul>

<br>

<h2>1-1. 스파크 애플리케이션</h2>
<ul>
  <li>
    Spark application은 <strong>드라이버(driver) 프로세스</strong>와 다수의 <strong>익스큐터(executor) 프로세스</strong>로 구성된다.
  </li>
  <li>
    클러스터 매니저로는 Spark standalone, Hadoop YARN, Mesos 중 하나를 선택할 수 있다.
  </li>
  <li>
    Spark는 <strong>로컬 모드</strong>를 지원한다. 로컬 모드란 <strong>단일 머신</strong>서 드라이버와 익스큐터가 실행되는 것을 의미한다.
  </li>
  <li>
    <strong>핵심 사항 두 가지</strong>
  </li>
    <ul>
      <li>
        Spark는 <strong>사용 가능한 자원을 파악</strong>하기 위해 <strong>클러스터 매니저</strong>를 사용한다.
      </li>
      <li>
        <strong>드라이버 프로세스</strong>는 주어진 작업을 완료하기 위해 드라이버 프로그램의 명령을 <strong>익스큐터에서 실행할 책임</strong>이 있다.
      </li>
    </ul>
  <li>
    <strong>드라이버</strong>는 Spark 언어 API를 통해 <strong>다양한 언어</strong>로 실행이 가능하며 <strong>익스큐터</strong>는 대부분 <strong>Spark 코드를 실행</strong>하는 역할을 한다.
  </li>
</ul>

<h3>1-1-1. 드라이버(driver) 프로세스</h3>
<ul>
  <li>
    클러스터 노드 중 하나에서 실행되며 <strong>main() 함수</strong>를 실생한다.
  </li>
    <ul>
      <li>
        main() 함수란 Spark의 작업을 시작하는 <strong>엔트리 포인트(entry point)</strong>를 실행한다고 이해하면 된다.
      </li>
    </ul>
  <li>
    Spark application의 작업과 관련된 분석, 배포, 스케줄링 등의 역할을 수행한다.
  </li>
  <li>
    드라이버 프로세스는 Spark application의 심장과 같으며 application 수명 주기 동안 관련 정보를 모두 유지한다.
  </li>
</ul>

<h3>1-1-2. 익스큐터(executor) 프로세스</h3>
<ul>
  <li>
    <strong>익스큐터</strong>는 드리이버 프로세스가 <strong>할당한 작업</strong>을 의미한다.
  </li>
  <li>
    <strong>1) 드라이버가 할당한 코드를 실행</strong>하고 <strong>2) 진행 상황을 드라이버 노드에 보고</strong>한다.
  </li>
</ul>

<br><br>

<h1>2. 스파크의 다양한 언어 API</h1>
<ul>
  <li>
    Spark는 모든 언어에 맞는 몇몇 <strong>핵심 개념</strong>을 제공.<br>→ 핵심 개념은 클러스터 머신에서 실행되는 <strong>스파크 코드로 변환</strong>됨.
  </li>
  <li>
    각 언어 API는 핵심 개념을 유지하고, 사용자는 Spark 코드를 실행하기 위해 <strong>SparkSession 객체</strong>를 <strong>진입점</strong>으로 사용할 수 있다.
  </li>
  <li>
    Python이나 R은 스파크를 사용할 때 JVM을 명시적으로 작성하지 않는다. 대신 Spark가 사용자를 대신하여 Python 혹은 R로 작성한 코드를 익스큐터의 JVM에서 실행 가능한 코드로 변환한다.
  </li>
</ul>

<h3>2-1. Spark 지원 API</h3>
<ul>
  <li>
    <strong>Scala</strong>: Spark는 스칼라로 개발 되었으며 Spark의 기본 언어이다.
  </li>
  <li>
    <strong>Java</strong>: Spark 창시자들은 Java를 이용하여 코드를 작성할 수 있도록 함.
  </li>
  <li>
    <strong>Python</strong>: Scala가 지원하는 거의 모든 구조를 지원.
  </li>
  <li>
    <strong>SQL</strong>: ANSI SQL:2003 표준 중 일부를 지원. 분석가나 비프로그래머도 SQL로 Spark를 활용할 수 있음.
  </li>
  <li>
    <strong>R</strong>: SparkR과 sparklyr 두 라이브러리를 제공한다. (통합 사용 가능).
  </li>
</ul>

<br><br>

<h1>3. 스파크 API</h3>
<ul>
  <li>
    Spark가 다양한 언어를 제공할 수 있는 배경에는 두 가지 API가 있기때문이다.
  </li>
  <li>
    두 가지 API 중 하나는 <strong>저수준의 비구조적(unstructural) API</strong>이고, 다른 하나는 <strong>고수준의 구조적(structural) API</strong>이다.
  </li>
</ul>

<br><br>

<h1>4. 스파크 시작하기</h1>
<ul>
  <li>
    실제 Spark application을 개발하기 위해서는 <strong>사용자 명령과 데이터를 전달</strong>하기 위해 <strong>SparkSession</strong>에 대해 알아야 한다.
  </li>
  <li>
    대화형 모드로 Spark를 시작하면 Spark application을 관리하는 SparkSession이 자동 생성되지만, <strong>Spark standalone</strong>을 사용할 경우 <strong>직접 SparkSession을 생성</strong>해 주어야 한다.
  </li>
</ul>

<br><br>

<h1>5. SparkSession</h1>
<ul>
  <li>
    Spark application은 <strong>SparkSession</strong>이라 불리는 드라이버 프로세스로 제어한다.
  </li>
  <li>
    SparkSession 인스턴스는 사용자가 정의한 처리 명령을 <strong>클러스터에서 실행</strong>한다.
  </li>
  <li>
    <strong>하나</strong>의 SparkSession은 <strong>하나</strong>의 Spark application에 대응한다.
  </li>
  <li>
    Python 콘솔에서 <strong>spark 변수</strong>를 통해 SparkSession을 사용할 수 있다.
  </li>
</ul>

```plaintext
spark
→ <pyspark.sql.session.SparkSession object at 0x7b96c54fa7c0>이 출력되며 
SparkSession을 사용할 수 있다.
```

<br><br>

<h1>6. DataFrame</h1>
<ul>
  <li>
    <strong>DataFrame</strong>: 테이블의 데이터를 row와 column으로 단순하게 표현한다.
  </li>
  <li>
    <strong>스키마(schema)</strong>: 컬럼과 컬럼의 타입을 정의한 목록을 의미한다.
  </li>
  <li>
    Python과 R 등의 DataFrame은 단일 컴퓨터에 존재하는 반면 Spark의 DataFrame은 수천 대의 컴퓨터에 <strong>분산</strong>되어 있다.
  </li>
    <ul>
      <li>
        Spark의 DataFrame이 분산되어 존재하기에 <strong>물리적 자원의 한계</strong>가 비교적 높다.
      </li>
    </ul>
  <li>
    Spark는 <strong>Dataset</strong>, <strong>DataFrame</strong>, <strong>SQL 테이블</strong> 그리고 <strong>RDD</strong>라는 몇 가지 추상화 개념을 갖고 있다.
  </li>
</ul>

<br>

<h2>6-1. 파티션</h2>
<ul>
  <li>
    <strong>분할된 청크 단위 데이터</strong>이다. Spark의 모든 익스큐터가 <strong>병렬로 작업을 수행</strong>할 수 있도록 파티션을 이용한다.
  </li>
  <li>
    <strong>물리적</strong>으로는 클러스터의 물리적 머신에 존재하는 <strong>row의 집합</strong>을 의미한다.
  </li>
  <li>
    <strong>DataFrame의 파티션</strong>은 실행 중 데이터가 컴퓨터 클러스터에서 <strong>물리적으로 분산</strong>되는 방식을 나타낸다.
  </li>
  <li>
    <strong>'다수의 파티션 + 하나의 익스큐터'</strong> 혹은 <strong>'하나의 파티션 + 다수의 익스큐터'</strong> 둘 모두 <strong>병렬성은 1</strong> 이다.
  </li>
  <li>
    <strong>DataFrame</strong>은 파티션을 수동 혹은 개별적으로 처리할 필요 없다. 물리적 파티션에서 <strong>데이터 변환용 함수</strong>를 지정하면 <strong>Spark</strong>가 알아서 실제 처리 방법을 결정한다.
  </li>
</ul>

<br>

<h1>7. 트랜스포메이션</h1>
<ul>
  <li>
    Spark의 핵심 데이터 구조는 <strong>불변성(immutable)</strong>을 갖는다. 즉,한번 생성되면 변경할 수 없다.
  </li>
  <li>
    DataFrame을 변경하기 위해서는 <strong>원하는 변경 방법</strong>을 Spark에 알려주어야 한다. 이때 사용하는 명령을 <strong>트랜스포메이션</strong>이라 한다.
  </li>
    <ul>
      <li>
        실제로는 변경이 아니라 <strong>새로운 DataFrame을 반환</strong>한다. 즉, 기존 DataFrame을 남기고 새로운 DataFrame을 반환하는 방식이다.
      </li>
    </ul>
  <li>
    Spark가 클러스터에서 <strong>파티션을 교환</strong>하는 것을 <strong>셔플(shuffle)</strong>이라 한다. 트랜스포메이션 중에는 셔플이 발생한다.
  </li>
</ul>

<h2>7-1. 트랜스포메이션의 두 가지 유형</h2>
<h3>7-1-1. 좁은 의존성(narrow dependency)</h3>
<ul>
  <li>
    각 입력 파티션이 <strong>하나의 출력 파티션</strong>에만 영향을 미친다.
  </li>
  <li>
    좁은 트랜스포메이션을 사용하면 Spark는 <strong>파이프라이닝(pipelining)</strong>을 자동 수행한다.
  </li>
    <ul>
      <li>
        <strong>Pipelining</strong>: 여러 개의 좁은 트랜스포메이션을 하나의 <strong>stage 내에서 연속적으로 실행</strong>하는 것.
      </li>
      <li>
        <strong>Stage</strong>: Spark Job을 실행할 때 <strong>DAG를 shuffle 단위</strong>로 나눈 실행단위. 
      </li>
    </ul>
  <li>
    <strong>Action</strong>이 실행되면 실제로 트랜스포메이션이 진행된다.
  </li>
    <ul>
      <li>
        Action 호출<br>→ DAG를 shuffle 단위로 stage 분할<br>→ 좁은 트랜스포메이션을 pipelining.
      </li>
    </ul>
</ul>

<h3>7-1-2. 넓은 의존성(wide dependency)</h3>
<ul>
  <li>
    하나의 입력 파티션이 <strong>다수의 출력 파티션</strong>에 영향을 미친다.
  </li>
</ul>

```plaintext
divisBy2 = myRange.where("number % 2 = 0")

- 트랜스포메이션 정의 
- 추상적 트랜스포메이션만 지정한 것이기 때문에 액션(action)을 호출하지 않으면 실제로 
트랜스포메이션을 수행하지는 않는다.
```

<br>

<h2>7-2. 지연 연산</h2>
<ul>
  <li>
    <strong>지연 연산(lazy evaluation)</strong>은 Spark가 연산 그래프를 처리하기 직전까지 <strong>기다리는 동작 방식</strong>이다.
  </li>
  <li>
    Spark는 명령어가 내려지면 바로 수행하지 않고 원시 데이터에 적용할 트랜스포메이션 <strong>실행 계획</strong>을 생성한다.
  </li>
  <li>
    그 후 <strong>최종 단계</strong>에서 DataFrame 트랜스포메이션을 <strong>간결한 물리적 계획으로 컴파일</strong> 한다.
  </li>
    <ul>
      <li>
        이러한 방식은 데이터 흐름을 최적화 함.
      </li>
    </ul>
  <li>
    예를 들어 DataFrame의 <strong>조건절 푸시다운(predicate pushdown)</strong>이 있다.
  </li>
    <ul>
      <li>
        <strong>하나의 row</strong>만 가져오는 필터<br>→ 드라이버가 아닌 <strong>익스큐터의 데이터베이스에 위임</strong> <br>→ 익스큐터가 처리하고 하나의 row만 <strong>드라이버에 전달</strong><br>→ 실질적으로 Spark 드라이버는 <strong>하나의 row</strong>만을 받음.
      </li>
    </ul>
</ul>

<br><br>

<h1>8. 액션</h1>
<ul>
  <li>
    액션은 논리 계획을 <strong>실행</strong>하는 명령어이다. 즉, 트랜스포메이션으로부터 <strong>결과 계산을 실행</strong>한다.
  </li>
  <li>
    <strong>액션 지정</strong> → <strong>Spark Job 생성</strong> → <strong>Stage 분리</strong> → <strong>Task 실행</strong>
  </li>
  <li>
    <strong>Spark UI</strong>로는 클러스터에서 실행 중인 Spark Job을 <strong>모니터링</strong>할 수 있다.
  </li>
</ul>

```plaintext
- count()를 통해 DataFrame의 행 수를 반환 받는다.

divisBy2.count()
```

<br><br>

<h1>9. 스파크 UI</h1>
<ul>
  <li>
    Spark UI는 Spark Job의 <strong>진행 상황을 모니터링</strong>할 때 사용한다.
  </li>
  <li>
    Spark UI는 <strong>드라이버 노드의 4040 포트</strong>로 접속할 수 있다. (로컬의 경우 http://localhost:4040).
  </li>
  <li>
    Spark UI에서는 Spark Job의 상태, 환경 설정, 클러스터 상태 등의 정보를 확인할 수 있다.
  </li>
  <li>
    Spark UI는 Spark Job을 <strong>튜닝</strong>하고 <strong>디버깅</strong>할 때 유용하다.
  </li>
</ul>

<br><br>

<h1>10. 종합 예제</h1>
<ul>
  <li>
    Spark는 DataFrame의 스키마 정보를 알아낼 때 <strong>키마 추론(schema inference)</strong>을 활용한다.
  </li>
    <ul>
      <li>
        스키마 추론이란 전체 데이터를 읽는 것이 아니라 <strong>일부 데이터</strong>를 읽고 스파크 데이터 타입에 맞게 분석하는 방법이다.
      </li>
      <li>
        DataFrame에서 CSV 파일을 읽어 <strong>배열</strong>이나 <strong>리스트</strong> 형태로 반환한다.
      </li>
      <li>
        데이터를 읽는 과정에서 <strong>지연 연산 형태</strong>의 트랜스포메이션을 활용하기에 <strong>행의 수</strong>를 알수는 없다.
      </li>
    </ul>
  <li>
    트랜스포메이션의 논리적 실행 계획<br>→ DataFrame <strong>계보</strong>를 정의<br>→ 계보를 통해 입력 데이터에 수행한 연산을 전체 파티션에서 어떻게 <strong>재연산</strong>하는지 확인 가능.<br>→ 변환 규칙이 일정할 경우 <strong>같은 입력</strong>에 대해서는 <strong>같은 출력</strong>을 생성한다는 <strong>함수형 프로그래밍</strong>의 핵심 반영. 
  </li>
  <li>
    사용자는 물리적 데이터를 직접 다루지 않고, 셔플 파티션 파라미터와 같은 속성으로 <strong>물리적 실행 특성을 제어</strong>한다.
  </li>
  <li>
    <strong>Spark UI(4040 포트)</strong>로 접속하여 Job의 실행 상태와 물리적, 논리적 실행 특성을 확인할 수 있다.
  </li>
</ul>

```bash
# 1. 교재에서 제공되는 데이터를 로컬 경로에 받고, 바인드 마운트로 컨테이너 재가동.
docker run --rm -it \
  -v /mnt/c/Users/SSAFY/Desktop/spark-prac:/opt/spark-data \
  apache/spark:3.5.2 /opt/spark/bin/pyspark
```

```python
# 2. 바인드 마운트된 경로에서 데이터 DataFrame의 스키마 추론.
flightData2015 = spark\
.read\
# 스키마 추론 사용
.option("inferSchema", "true")\
# 첫 번째 row를 header로 설정.
.option("header", "true")\
.csv("/opt/spark-data/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv")
```

```python
# 3. 결과 확인
# Row의 결과가 출력된다.
flightData2025.take(3)

# 일부 데이터가 조회된다.
flightData2025.show(5)
```

```python
# 4. 정수 타입인 count 컬럼을 기준으로 데이터를 정렬하는 트렌스포메이션을 추가.
# - explain()
#   - DataFrame의 계보(lineage) 혹은 스파크의 쿼리 실행 계획을 확인하는 메서드.
#   - 실행 계획은 시파크의 실행 과정과 디버깅을 이해하는 데 도움이 된다.
#   - 최종 결과는 가장 위에, 데이터소스는 가장 아래에 배치된다.
flightData2015.sort("count").explain()
```

```python
# 5. 셔플 수를 5로 설정하여 트랜스포메이션 실행 계획을 시작한다. (기본 파티션은 200).
spark.conf.set("spark.sql.shuffle.partitions", "5")
```

```python
# 6. 정렬 결과 확인.
flightData2015.sort("count").take(2)
```

<br>

<h2>10-1. DataFrame과 SQL</h2>
<ul>
  <li>
    사용자가 SQL이나 DataFrame으로 비즈니스 로직을 표현하면 실제 코드를 실행하기 전에 해당 로직을 <strong>기본 실행 계획</strong>으로 컴파일 한다.
  </li>
    <ul>
      <li>
        실행 계획은 <strong>explain() 메서드</strong>로 확인할 수 있다.
      </li>
    </ul>
  <li>
    <strong>createOrReplaceTempView</strong>를 통해 DataFrame을 명명하면 <strong>Spark SQL</strong>을 사용할 수 있다.
  </li>
  <li>
    Spark는 SQL 쿼리를 DataFrame(다른 언어) 코드와 같은 실행 계획으로 컴파일 하기에 SQL을 사용한다 하여 <strong>성능 차이가 존재하지는 않는다</strong>.
  </li>
  <li>
    실행 계획은 트랜스포메이션의 <strong>지향성 비순환 그래프(DAG, Directed Acycle Graph)</strong>이며 액션이 호출되면 결과를 만든다.
  </li>
  <li>
    DAG의 각 단계는 <strong>불변성을 갖는 DataFrame</strong>을 생성한다.
  </li>
    <ul>
      <li>
        Spark는 해당 DataFrame이나 자신의 원본 DataFrame에 <strong>Action을 호출</strong>하기 전까지 데이터를 읽지 않는다.
      </li>
    </ul>
  <li>
    <strong>데이터를 변환</strong>하는 <strong>Trasformation</strong> 작업(select(), filter(), groupBy(), withColumn 등)에서는 <strong>Lazy Evaluation</strong>으로 실행 계획만 DAG에 추가된다.<br>→ <strong>데이터를 조회</strong>하는 <strong>Action</strong> 작업(show(), take(), count(), collect() 등)이 실행되면 이때 실제 DAG의 실행 계획이 <strong>수행</strong>된다.
  </li>
  <li>
    데이터를 드라이버로 모으는 대신 PostgreSQL과 같은 <strong>데이터베이스</strong> 혹은 <strong>다양한 format의 파일</strong>로 결과를 <strong>저장</strong>할 수 있다.
  </li>
</ul>

```python
# 1. createOrReplaceTempView를 통해 데이터프레임을 테이블이나 뷰로 만둘 수 있다.
flightData2015.createOrReplaceTempView("flight_data_2015")

# 이후 SQL로 데이터 조회 가능.
```

```python
# 2. spark.sql을 통한 조회 (두 코드는 동일한 실행 계획으로 컴파일 됨).
# spark.sql
sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")

# python
dataFrameWay = flightData2015\
.groupBy("DEST_COUNTRY_NAME")\
.count()
```

```python
# 3. 결과 확인
# spark.sql
sqlWay.explain()

# python
dataFrameWay.explain()
```

```python
# 4. max 함수를 활용하여 데이터 출력.
# spark.sql
spark.sql("SELECT max(count) from flight_data_2015").take(1)

# python
from pyspark.sql.functions import max

flightData2015.select(max("count")).take(1)
```

```python
# 5. max 함수를 활용한 다중 트랜스포메이션 수행.
# SQL 쿼리문 (transformation)
maxSql = spark.sql("""
SELECT DEST_COUNTRY_NAME, sum(count), as destination_total
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
ORDER BY sum(count)DESC
LIMIT 5
""")

# 결과 확인 (Action)
maxSql.show()

# python
from pyspark.sql.functions import desc

# Transformation
flightData2015\ 
.groupBy("DEST_COUNTRY_NAME")\
.sum("count")\
.withColumnRenamed("sum(count)", "destination_total")\
.sort(desc("destination_total"))\
.limit(5)\

# Action
.show()
```

```python
# 6. 실행 게획 확인.
flightData2015\
.groupBy("DEST_COUNTRY_NAME")\
.sum("count")\
.withColumnRenamed("sum(count)", "destination_total")\
.sort(desc("destination_total"))\
.limit(5)\
.explain()
```
