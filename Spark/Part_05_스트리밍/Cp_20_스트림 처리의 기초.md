<ul>
  <li>
    과거에는 map과 reduce, 그리고 DStream을 활용하였다. 2016년 부터는 DataFrame과 Dataset 코드와 쉽게 통합될 수 있는 신규 스트리밍 API가 <strong>DataFrame을 기반으로 구현</strong>되었으며 <strong>구조적 스트리밍</strong>이라는 이름으로 Spark에 추가 되었다.
  </li>
  <li>
    <strong>구조적 스트리밍</strong>은 DStream의 주요 기능에 대한 <strong>상위 기능</strong>을 제공하며 <strong>코드 생성 기능</strong>과 카탈리스트 옵티마이저를 사용한 <strong>최적화 기법</strong>을 제공한다.
  </li>
</ul>

<br>

<h1>1. 스트림 처리란</h1>
<ul>
  <li>
    <strong>스트림 처리(stream processing)</strong>는 신규 데이터를 <strong>끊임없이 처리</strong>해 결과를 만들어 내는 행위이다.
  </li>
  <li>
    스트림 처리의 <strong>입력 데이터</strong>는 <strong>무한</strong>하며 <strong>시작과 끝을 사전에 정의</strong>하지 않는다.
  </li>
  <li>
    <strong>입력 데이터</strong>는 스트림 처리 시스템에 도작한 일련의 <strong>이벤트</strong>이다.
  </li>
  <li>
    스트리밍 앱은 이벤트 스트림에 <strong>다양한 쿼리를 수행</strong>하고, <strong>다양한 버전의 결과</strong>를 출력하거나 key-value 저장소 같은 외부 sink 시스템에 <strong>최신 데이터를 저장</strong>할 수도 있다.
  </li>
  <li>
    <strong>배치 처리</strong>는 스트림 터리와 유사한 연산용 쿼리를 사용하지만 <strong>결과를 한 번만</strong> 만들어 낸다.
  </li>
  <li>
    스트림 처리와 배치 처리는 달라보이지만 실전에서는 <strong>함께 사용</strong>하기도 한다.
  </li>
    <ul>
      <li>
        스트리밍 앱이 <strong>스트림 입력 데이터</strong>를 배치 작업에서 주기적으로 만들어내는 <strong>데이터셋</strong>과 <strong>조인</strong>해야하는 경우가 있다.
      </li>
      <li>
        <strong>스트리밍 작업의 출력</strong>이 <strong>배치 작업용 쿼리</strong>에 필요한 파일이나 테이블일 수 있다.
      </li>
    </ul>
  <li>
    구조적 스트리밍은 배치 앱을 포함한 여러 컴포넌트와 <strong>쉽게 연동</strong>할 수 있도록 설계 되었다.
  </li>
  <li>
    구조적 스트리밍 개발자는 <strong>연속형 애플리케이션(continuous application)</strong>이라는 용어를 만들고 Spark에 개념을 추가하였다.
  </li>
    <ul>
      <li>
        연속형 애플리케이션(continuous application)이란 스트리밍, 배치 그리고 대화형 작업으로 구성된 <strong>통합 앱</strong>을 의미한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-1. 스트림 처리 사례</h2>
<h3>1-1-1. 통보와 알림</h3>
<ul>
  <li>
    통보(notification)와 알림(alerting)은 <strong>연속적인 이벤트</strong>에서 <strong>특정 이벤트</strong>나 <strong>이벤트의 패턴</strong>을 탐지했을 때 발생한다.
  </li>
</ul>

<h3>1-1-2. 실시간 리포트</h3>
<ul>
  <li>
    직원을 위한 실시간 대시보드를 구축할 때 스트리밍 시스템을 사용한다.
  </li>
</ul>

<h3>1-1-3. 증분형 ETL</h3>
<ul>
  <li>
    <strong>데이터 웨어하우스</strong>에서 정보를 얻는 시간을 줄이기 위해 스트리밍 앱을 사용한다. (배치 처리).
  </li>
  <li>
    Spark 배치 job은 원시 데이터를 ETL하여 <strong>파케이 같은 구조로 변환</strong>할 때 주로 사용한다.
  </li>
  <li>
    구조적 스트리밍을 이용할 경우 반드시 <strong>내고장성을 보장</strong>하고 <strong>정확히 한 번(exactly-once) 처리</strong>해야 한다. (데이터 중복 혹은 유실 X).
  </li>
  <li>
    <strong>transaction을 보장</strong>하면서 데이터 웨어하우스를 갱신해야 한다.
  </li>
</ul>

<h3>1-1-3. 실시간 제공용 데이터 갱신</h3>
<ul>
  <li>
    종종 다른 앱의 서비스용 데이터를 만들 때 사용한다. 예를 들어 페이지 방문자 수를 연속적으로 추적하면서 최신 방문자 수를 갱신한다.
  </li>
  <li>
    최신 방문자 수를 표시하기 위해 스트리밍 시스템은 key-value 저장소나 다른 저장소 시스템에 대한 <strong>동기식 증분 업데이트(incremental update)</strong>와 <strong>데이터 변형 방지를 위한 transaction</strong>을 지원해야 한다.
  </li>
</ul>

<h3>1-1-4. 실시간 의사결정</h3>
<ul>
  <li>
    신규 입력을 분석하고 자동으로 비즈니스 로직에 따라 처리하는 작업에서 활용한다.
  </li>
    <ul>
      <li>
        신용 카드 고객의 최근 이력을 기준으로 카드 transaction이 부정행위에 속하는지 판단하고 부정행위라면 카드 transaction을 거부한다.
      </li>
      <li>
        스트리밍 시스템 내부에 비즈니스 로직을 구현하고 transaction stream 대상으로 실행한다.
      </li>
    </ul>
</ul>

<h3>1-1-5. 온라인 머신러닝</h3>
<ul>
  <li>
    온라인 머신러닝은 여러 사용자의 <strong>실시간 데이터</strong>와 <strong>이력 데이터</strong>를 조합해 모델을 학습한다.
  </li>
    <ul>
      <li>
        <strong>단일 고객</strong>의 고정 규칙을 적용하지 않고 <strong>모든 고객</strong>의 행위에 기반한 모델을 연속적으로 갱신하는 경우.
      </li>
    </ul>
</ul>

<br>

<h2>1-2. 스트림 처리의 장점</h2>
<ul>
  <li>
    스트림 처리는 <strong>대기 시간</strong>이 짧다.
  </li>
  <li>
    자동으로 <strong>연산 결과의 증분</strong>을 생성하기에 반복적인 배치 작업보다 <strong>결과를 수정하는 데 더 효율적</strong>이다.
  </li>
    <ul>
      <li>
        이전 연산의 상태를 기억하고 신규 데이터만 계산할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>1-3. 스트림 처리의 과제</h2>
<ul>
  <li>
    앱플리케이션 타임스탬프(이벤트 시간) 기준으로 순서가 뒤섞인 데이터 처리하기
  </li>
  <li>
    대규모의 상태 정보 유지하기
  </li>
  <li>
    높은 데이터 처리량 보장하기
  </li>
  <li>
    장애 상황에서도 정확히 한 번 처리하기
  </li>
  <li>
    부하 불균형과 뒤쳐진 서버 다루기
  </li>
  <li>
    이벤트에 빠르게 응답하기
  </li>
  <li>
    다른 저장소 시스템의 외부 데이터와 조인하기
  </li>
  <li>
    신규 이벤트 도착 시 출력 싱크의 갱신 방법 결정하기
  </li>
  <li>
    출력 시스템에 데이터 저장 시 트랜잭션 보장하기
  </li>
  <li>
    런타임에 비즈니스 로직 변경하기
  </li>
</ul>

<br><br>

<h1>2. 스트림 처리의 핵심 설계 개념</h1>
<h2>2-1. 레코드 단위 처리와 선언형 API</h2>
<ul>
  <li>
    기존의 <strong>레코드 단위 처리(recored-at-a-time) API</strong>를 사용하는 스트리밍 시스템은 앱 내부에서 <strong>여러 처리 파이프라인을 연결</strong>하는 기능만 제공한다.
  </li>
    <ul>
      <li>
        상태 관리 같은 복잡한 문제를 갖는다.
      </li>
      <li>
        이런 레코드 단위 처리 API를 정교하게 개발하는 것은 매우 어려울 수 있으며 저수준 API를 개발하고 유지 보수하려면 높은 숙련도가 필요하다.
      </li>
    </ul>
  <li>
    최신 스트리밍 시스템에서는 <strong>선언형(declarative) API</strong>를 제공한다. 앱을 정의할 때 어떻게(how) 신규 데이터를 처리하고 장애 상황을 복구할지를 지정하는 대신 <strong>무엇(what)</strong>을 처리할지 지정한다.
  </li>
  <li>
    Spark의 DStream API는 map, reduce 그리고 filter와 같은 연산을 기반으로하는 함수형 API를 제공한다.
  </li>
  <li>
    <strong>Spark 구조적 스트리밍</strong>은 별도의 프로그래밍 없이 <strong>함수형 연산을 효율적</strong>으로 처리할 수 있는 <strong>SQL 형태의 관계형 연산</strong>으로 변환해 한 단계 발전시켰다.
  </li>
</ul>

<br>

<h2>2-2. 이벤트 시간과 처리 시간</h2>
<ul>
  <li>
    선언형 API를 사용하는 시스템을 구현할 때에는 자체적으로 <strong>이벤트 시간 처리</strong>를 지원할지 고민해야 한다.
  </li>
    <ul>
      <li>
        <strong>이벤트 시간 처리</strong>는 원천 시스템에서 <strong>각 레코드에 기록한 타임스탬프</strong>를 기반으로 데이터를 처리하는 방식을 의미한다.
      </li>
    </ul>
  <li>
    <strong>처리 시간</strong> 기준 처리는 스트리밍 앱에 <strong>레코드가 도착한 시간</strong>을 기반으로 처리하는 방식이다.
  </li>
  <li>
    스트리밍 레코드는 <strong>순서</strong>가 뒤섞여 들어오거나 <strong>여러 원천 시스템 사이</strong>의 순서가 뒤섞일 수 있는 문제가 있다.
  </li>
  <li>
    이벤트 시간 처리를 하지 않으면 일부 데이터가 <strong>늦게 도착</strong>하거나 <strong>중요한 패턴</strong>을 인식하지 못할 수 있다. 
  </li>
  <li>
    시스템은 늦게 도착한 이벤트를 처리할 수 있도록 <strong>상태를 추적</strong>해야 한다.
  </li>
  <li>
    이벤트 시간이 속한 <strong>특정 시간 윈도우의 결과</strong>를 가장 <strong>적절하게 출력할 수 있는 시점</strong>을 결정해야 한다.
  </li>
    <ul>
      <li>
        시스템이 해당 시점까지의 <strong>모든 입력 데이터를 수신했을 가능성이 높은 시기</strong>가 언제인지 결정해야한다.
      </li>
    </ul>
  <li>
    <strong>구조적 스트리밍</strong>을 포함한 많은 선언형 시스템은 API에 <strong>이벤트 시간 처리를 기본적으로 탑재</strong>하고 있어 <strong>문제를 자동으로 해결</strong>한다.
  </li>
</ul>

<br>

<h2>2-3. 연속형 처리와 마이크로 배치 처리</h2>
<ul>
  <li>
    연속형 처리는 <strong>레코드별</strong>로 데이터를 처리한다. <strong>입력량이 비교적 적을 때</strong> 가장 빠르게 응답하지만 레코드 단위 부하가 커 <strong>최대 처리량이 작다</strong>.
  </li>
  <li>
    연속형 처리는 <strong>전체 시스템을 중지</strong>해야 앱을 변경할 수 있다는 문제 또한 존재한다.
  </li>
  <li>
    <strong>마이크로 배치</strong>는 입력 데이터를 <strong>작은 배치</strong>로 모으기 위해 <strong>대기</strong>한다.
  </li>
  <li>
    <strong>마이크로 배치</strong>는 다수의 분산 태스크를 이용해 <strong>각 배치를 병렬로 처리</strong>한다. 따라서 <strong>배치 시스템의 최적화 기법</strong>을 사용할 수 있다.
  </li>
  <li>
    마이크로 배치는 <strong>더 적은 노드</strong>로 같은 양의 데이터를 처리할 수 있으며 워크로드 변화에 대응할 수 있도록 태스트 수를 늘리거나 줄이는 <strong>부하 분산 기술</strong>을 동적으로 사용 가능하다.
  </li>
    <ul>
      <li>
        하지만 기본적으로 <strong>마이크로 배치를 모으기 위한 시간</strong>이 필요하다는 것은 변함없다.
      </li>
    </ul>
  <li>
    스트리밍과 마이크로 배치 중 하나를 선택할 때에는 <strong>지연 시간 요건</strong>과 <strong>총 운영비용(total cost of operation, TCO)</strong>을 고려해야 한다.
  </li>
  <li>
    <strong>마이크로 배치 시스템</strong>은 <strong>더 적은 노드 수로 동일한 처리량</strong>을 얻을 수 있다는 점, 노드 장애가 덜 발생하기에 <strong>운영비용이 낮다</strong>는 장점이 있다.
  </li>
  <li>
    <strong>더 빠른 응답</strong>이 필요한 경우 <strong>연속형 처리 시스템</strong>을 사용하거나 빠른 응답성을 제공하는 <strong>고속 서빙 계층(fast serving layer)</strong>을 마이크로 배치 서비스와 함께 사용할 수 있다. 
  </li>
</ul>

<br><br>

<h1>3. 스파크의 스트리밍 API</h1>
<h2>3-1. DStream API</h2>
<ul>
  <li>
    Spark의 DStream API는 2012년 처음 공개된 후 스트림 처리 분야에서 널리 사용되었다.
  </li>
  <li>
    DStream API의 몇 가지 제약사항
  </li>
    <ul>
      <li>
        <strong>Java나 python의 객체와 함수에 매우 의존적</strong>이다. 이로인해 스트림 처리 엔진이 제공하는 <strong>최적화 기법</strong>을 적용하지 못한다.
      </li>
      <li>
        <strong>처리 시간을 기준</strong>으로 동작하며 <strong>이벤트 시간을 기준</strong>으로 처리하고 싶은 경우 <strong>직접 구현</strong>해야 한다.
      </li>
      <li>
        <strong>마이크로 배치 형태</strong>로만 동작한다.
      </li>
    </ul>
</ul>

<br>

<h2>3-2. 구조적 스트리밍</h2>
<ul>
  <li>
    <strong>구조적 스트리밍</strong>은 <strong>Spark의 구조적 API</strong>를 기반으로 하는 <strong>고수준 스트리밍 API</strong>이다.
  </li>
  <li>
    구조적 스트리밍은 DStream과 다르게 <strong>이벤트 시간 데이터 처리</strong>를 지원한다.
  </li>
  <li>
    구조적 스트리밍은 <strong>연속형 애플리케이션</strong>을 쉽게 만들 수 있도록 설계 되었으며 <strong>DataFrame과 같은 API</strong>를 사용한다.
  </li>
  <li>
    구조적 스트리밍은 데이터가 도착할 때마다 자동으로 <strong>증분 형태의 연산 결과</strong>를 만들어 낸다.
  </li>
  <li>
    구조적 스트리밍은 Spark SQL에서 지원하는 파케이 테이블 등의 <strong>표준 싱크로 데이터를 출력</strong>할 수 있다.
  </li>
  <li>
    Spark 구조적 스트리밍은 Spark의 <strong>다른 Spark 앱</strong>에서 쉽게 <strong>스트림 상태를 조회</strong>할 수 있다.
  </li>
</ul>