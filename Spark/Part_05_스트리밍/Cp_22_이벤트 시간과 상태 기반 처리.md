<h1>1. 이벤트 시간 처리</h1>
<ul>
  <li>
    스트림 처리 시스템은 <strong>이벤트가 실제로 발생한 시간(이벤트 시간, event-time)</strong>과 이벤트가 시스템에 <strong>도착한 시간 또는 처리된 시간(처리 시간, processing time)</strong> 두 가지 유형의 이벤트 시간을 갖는다.
  </li>
  <li>
    <strong>이벤트 시간</strong>
  </li>
    <ul>
      <li>
        이벤트 시간은 <strong>데이터에 기록된 시간</strong>이며 대부분의 경우 이벤트가 <strong>실제로 발생한 시간</strong>을 의미한다.
      </li>
      <li>
        이벤트 시간은 <strong>다른 이벤트와 비교</strong>하는 방법을 제공하기에 강력하지만, <strong>지연되거나 무작위로 도착</strong>하는 이벤트를 해결해야하는 문제가 있다.
      </li>
    </ul>
  <li>
    <strong>처리 시간</strong>
  </li>
    <ul>
      <li>
        처리 시간은 처리 시스템이 실제로 데이터를 <strong>수신한 시간</strong>이다. 
      </li>
      <li>
        처리 시간은 <strong>세부 구현</strong>과 관련된 내용이기에 이벤트 시간보다 덜 중요하다.
      </li>
      <li>
        처리 시간은 시스템 내부에서 제공하는 속성이기에 <strong>순서가 뒤섞일 일이 없다</strong>.
      </li>
    </ul>
</ul>

<br><br>

<h1>2. 상태 기반 처리</h1>
<ul>
  <li>
    상태 기반 처리는 오랜 시간에 걸쳐 <strong>중간 처리 정보를 사용</strong>하거나 <strong>갱신</strong>하는 경우에만 필요하다.
  </li>
  <li>
    상태 기반 처리는 <strong>이벤트 시간</strong>을 사용하거나 <strong>key에 대한 집계</strong>를 사용하는 상황에서 일어난다. 집계 key가 반드시 이벤트 시간과 <strong>연관성</strong>을 가져야 할 필요는 없다.
  </li>
  <li>
    Spark는 상태 기반 연산에 필요한 복잡한 처리를 대신하며 사용자는 단순하게 <strong>로직만 정의</strong>하면 된다.
  </li>
  <li>
    Spark는 상태 기반 연산에 필요한 중간 정상태 정보를 <strong>상태 저장소(state store)</strong>에 저장한다.
  </li>
  <li>
    책 시점에서 Spark는 상태 저장소의 구체적인 <strong>인메모리 상태 저장소</strong>를 제공하며 중간 상태를 체크포인트 디렉터리에 저장해 <strong>내고장성을 보장</strong>한다.
  </li>
</ul>

<br><br>

<h1>3. 임의적인 상태 기반 처리</h1>
<ul>
  <li>
    상태 기반 처리로 다양한 스트리밍 문제를 해결할 수 있으나 <strong>상태의 유형</strong>, <strong>갱신 방법</strong> 그리고 <strong>제거 시점</strong>에 따라 세밀한 제어가 필요할 수 있으며 이런 처리를 <strong>임의적인(또는 사용자 저의) 상태 기반 처리</strong>라 한다.
  </li>
  <li>
    사용자는 스트림 처리에 필요한 모든 정보를 Spark에 저장할 수 있기에 유연성과 강력함을 기반으로 복잡한 비즈니스 로직을 쉽게 제어할 수 있다.
  </li>
</ul>

<br><br>

<h1>4. 이벤트 시간 처리의 기본</h1>
<ul>
  <li>
    이벤트 시간은 데이터셋에 있는 <strong>하나의 컬럼</strong>이며 유일하게 신경 써야하는 부분이다.
  </li>
</ul>

```python
# 1. 단순 이벤트 시간 컬럼 사용 예

# 셔플 파티션 개수를 5로 설정 (기본값은 200 → 소규모 데이터에서는 너무 큼)
spark.conf.set("spark.sql.shuffle.partitions", 5)

# 정적 DataFrame을 JSON 파일에서 읽어옴 (스키마 추출용)
static = spark.read.json("/workspace/Spark-The-Definitive-Guide/data/activity-data")

# 스키마를 변수에 저장
static_schema = static.schema

# 스트리밍 DataFrame 정의
streaming = spark\
    # 스트리밍 소스에서 데이터를 읽겠다고 선언
    .readStream\
    # 미리 정해둔 스키마를 적용 (스트리밍에서는 스키마 추론 불가)
    .schema(static_schema)\
    # 트리거 한 번마다 최대 10개의 파일을 읽도록 설정
    .option("maxFilesPerTrigger", 10)\
    # 입력 경로(새 JSON 파일 모니터링)
    .json("/workspace/Spark-The-Definitive-Guide/data/activity-data")

# 스트리밍 DataFrame의 스키마 출력 (컬럼 구조 확인)
streaming.printSchema()

# 출력 결과
#   - Creation_Time: 이벤트가 생성된 시간
#   - Arrival_Time: 서버에 도착한 시간
```

<br><br>

<h1>5. 이벤트 시간 윈도우</h1>
<ul>
  <li>
    이벤트 시간 분석의 첫 단계는 타임스탬프 컬럼을 <strong>적절한 Spark SQL 타임스탬프 데이터 타입</strong>으로 변환하는 것이다.
  </li>
</ul>

```python
# 1. 타임스탬프 컬럼을 적합한 Spark SQL 타임스탬프 데이터 타입으로 변경.\
withEventTime = streaming.selectExpr(
    "*",
    "cast(cast(Creation_Time as double)/1000000000 as timestamp) as event_time")
```

<br>

<h2>5-1. 텀블링 윈도우</h2>
<ul>
  <li>
    트리거가 실행될 때마다 <strong>마지막 트리거 이후 수신한 데이터</strong>를 처리해 <strong>결과 테이블을 갱신</strong>한다.
  </li>
  <li>
    결과로 반환되는 window 컬럼은 <strong>struct 타입(복합 데이터 타입)</strong>이며 특정 윈도우의 <strong>시작과 종료 시각</strong>을 얻을 수 있다.
  </li>
  <li>
    중요한 점은 이벤트 시간 컬럼을 포함해 <strong>여러 컬럼에 대한 집계</strong>를 수행할 수 있다.
  </li>
  <li>
    다중 key 집계 방식은 모든 윈도우 기반 집계 또는 상태 기반 집계 연산에 사용할 수 있다.
  </li>
</ul>

```python
# 1. 텀블리 윈도우를 통한 스트리밍 처리
from pyspark.sql.functions import window, col

# 10분 단위로 윈도우를 처리하여 이벤트를 집계.
withEventTime.groupBy(window(col("event_time"), "10 minutes")).count()\
    .writeStream\
    .queryName("pyevents_per_window")\
    # 디버깅 목적으로 결과를 메모리 싱크에 저장.
    .format("memory")\
    .outputMode("complete")\
    .start()
```

```python
# 2. 결과 확인
spark.sql("SELECT * FROM pyevents_per_window").show()
```

```python
# 3. 윈도우 기반 집계 수행.
from pyspark.sql.functions import window, col

# event_time 컬럼을 기준으로 10분 텀블링 윈도우 생성
# User 별 그룹화 후 이벤트 개수(count) 집계
withEventTime.groupBy(window(col("event_time"), "10 minutes"), "User").count()\
    .writeStream\
    # 결과는 메모리 싱크(pyevents_per_window)로 쿼리명 지정하여 저장
    .queryName("pyevents_per_window_v2")\
    .format("memory")\
    .outputMode("complete")\
    .start()

# 결과 조회
spark.sql("SELECT * FROM pyevents_per_window_v2").show()
```

<h3>5-1-1. 슬라이딩 윈도우</h3>
<ul>
  <li>
    <strong>시간 증분</strong>을 보기 위해 <strong>슬라이딩 윈도우</strong>를 사용하지만 10분 마다 상태를 얻는다. <strong>지난 시간</strong>에 대한 <strong>데이터를 유지</strong>하면서 <strong>연속적으로 값을 갱신</strong>한다.
  </li>
  <li>
    슬라이딩 위도우는 윈도우 크기와 슬라이드 간격에 따라 <strong>하나의 이벤트</strong>가 <strong>다수의 윈도우</strong>에 속할 수 있다.
  </li>
</ul>

```python
# 1. 슬라이딩 윈도우를 통해 스트리밍 구현
from pyspark.sql.functions import window, col

# event_time 기준으로 길이 10분짜리 윈도우를 만들되, 시작 시점을 5분 간격으로 미는 슬라이
# 딩 윈도우를 생성
# → 따라서 하나의 이벤트는 최대 2개의 윈도우에 포함될 수 있음
withEventTime.groupBy(window(col("event_time"), "10 minutes", "5 minutes"))\
    .count()\
    .writeStream\
    .queryName("pyevents_per_window_v3")\
    .format("memory")\
    .outputMode("complete")\
    .start()

# 결과 조회
spark.sql("SELECT * FROM pyevents_per_window_v3").show()
```

<br>

<h2>5-2. 워터마크로 지연 데이터 처리하기</h2>
<ul>
  <li>
    <strong>워터마크</strong>는 특정 시간 이후에 <strong>처리에서 제외</strong>할 이벤트나 이벤트 집합에 대한 시간 기준이다. 지정하지 않는 경우 Spark는 중간 결과 데이터를 <strong>영원히 저장</strong>한다.
  </li>
  <li>
    원래 속해야하는 윈도우 처리용 배치가 이미 시작된 경우 <strong>다른 윈도우 처리용 배치</strong>에서 이벤트가 처리된다. 
  </li>
    <ul>
      <li>
        구조적 스트리밍은 <strong>이벤트 시간</strong>과 <strong>상태 기반 처리</strong>를 사용하면 특정 윈도우의 상태나 데이터셋이 <strong>처리 윈도우에서 분리</strong>되기에 문제를 해결할 수 있다.
      </li>
      <li>
        즉, 더 많은 이벤트가 유입될수록 <strong>구조적 스트리밍</strong>이 더 많은 정보를 이용해 <strong>윈도우를 계속 갱신</strong>함을 의미한다.
      </li>
      <li>
        이를 기반으로 다른 윈도우에 속하더라도 <strong>원래 속할 윈도우로 배정</strong>할 수 있다.
      </li>
    </ul>
  <li>
    <strong>워터마크</strong>를 지정해 지연된 데이터를 <strong>어느 시점</strong>까지 받아볼 것인지 지정할 수 있으며 이를 통해 지연된 데이터를 <strong>메모리에서 제거</strong>해 스트림 처리를 오랜 시간 걸쳐 안정적으로 수행할 수 있다. 
  </li>
</ul>

```python
# 1. 데이터를 몇 분 이내에 처리해야하는데 이벤트가 최대 30 분 지연되어 도착하는 현상이 발생
#    하는 경우 워터마크 처리 예
from pyspark.sql.functions import window, col

withEventTime\
    # 워터마크 설정
    #   - event_time 기준으로 최대 30분 늦게 도착한 이벤트까지 허용
    #   - 30분이 지난 후 도착한 이벤트는 해당 윈도우에 반영되지 않고 드롭됨
    .withWatermark("event_time", "30 minutes")\

    # 슬라이딩 윈도우 집계
    #   - 윈도우 크기: 10분
    #   - 슬라이드 간격: 5분
    #     → 윈도우가 5분마다 겹치게 생성되므로 이벤트는 최대 2개 윈도우에 속할 수 있음
    .groupBy(window(col("event_time"), "10 minutes", "5 minutes"))\
    .count()\

    # 스트리밍 쿼리 설정
    .writeStream\
    # 메모리 테이블 이름 (SQL로 조회 가능)
    .queryName("pyevents_per_window_v4")\
    # 디버깅/테스트용 메모리 싱크
    .format("memory")\
    # 전체 윈도우 집계 결과를 매번 갱신
    # complete 모드를 사용하기에 테이블을 쿼리해서 중간에 결과를 확인할 수 있다.
    # (append는 불가능)
    .outputMode("complete")\
    .start()

# 결과 조회
spark.sql("SELECT * FROM pyevents_per_window_v4").show()
```

<br><br>

<h1>6. 스트림에서 중복 데이터 제거하기</h1>
<ul>
  <li>
    스트림 처리 결과를 활용하는 앱과 집계 연산에서는 각 메시지가 <strong>정확히 하나</strong>만 있다고 확신할 수 있어야 한다.
  </li>
  <li>
    구조적 스트리밍은 태생적으로 최소 한 번(at-least-once) 처리하는 방식을 제공하는 메시지 시스템을 활용할 수 있으며 <strong>key를 기준으로 중복을 제거</strong>해 정확히 한 번 처리 방식을 지원할 수 있다.
  </li>
  <li>
    Spark는 데이터 중복을 제거하기 위해 <strong>사용자가 지정한 key를 유지</strong>하면서 <strong>중복 여부</strong>를 확인한다.
  </li>
</ul>

```python
# 1. 중복 이벤트를 제거해 사용자별 이벤트 수를 줄인다.
from pyspark.sql.functions import expr

withEventTime\
    .withWatermark("event_time", "5 seconds")\
    .dropDuplicates(["User", "event_time"])\
    .groupBy("User")\
    .count()\
    .writeStream\
    .queryName("pydeduplicated")\
    .format("memory")\
    .outputMode("complete")\
    .start()

# 결과 확인
spark.sql("SELECT * FROM pydeduplicated").show()
```

<br><br>

<h1>7. 임의적인 상태 기반 처리</h1>
<ul>
  <li>
    복잡한 윈도우 처리 방식을 사용하는 경우 <strong>임의적인 상태 기반 처리</strong>를 사용해야 한다.
  </li>
  <li>
    상태 기반 처리는 책 집필 시점에서 Spark 2.2 버전의 scala를 이용하는 경우만 사용이 가능하다.
  </li>
  <li>
    상태 기반 처리를 할 경우 다음과 같은 처리를 할 수 있다.
  </li>
    <ul>
      <li>
        <strong>특정 key의 개수</strong>를 기반으로 윈도우 생성하기
      </li>
      <li>
        <strong>특정 시간 범위 안</strong>에 일정 개수 이상의 이벤트가 있는 경우 <strong>알림 발생</strong>시키기.
      </li>
      <li>
        결정되지 않은 시간 동안 <strong>사용자 세션을 유지</strong>하고 향후 분석을 위해 <strong>세션 저장</strong>하기.
      </li>
    </ul>
  <li>
    임의적인 상태 기반 처리를 수행하면 결과적으로 두 가지 처리 유형을 만나게된다.
  </li>
    <ul>
      <li>
        데이터의 각 그룹에 <strong>맵 연산</strong>을 수행하고 각 그룹에서 <strong>최대 한 개의 row</strong>를 만들어낸다. 이런 처리는 mapGroupsWithStateAPI를 이용한다.
      </li>
      <li>
        데이터의 각 그룹에 <strong>맵 연산</strong>을 수행하고 각 그룹에서 <strong>하나 이상의 row</strong>를 만들어 낸다. 이런 처리는 flatMapGroupsWithState API를 이용한다.
      </li>
    </ul>
  <li>
    데이터의 <strong>각 그룹에 연산</strong>을 수행하면 각 그룹을 <strong>임의로 갱신</strong>할 수 있다. 이를 통해 <strong>임의의 윈도우 유형</strong>을 정의할 수 있다.
  </li>
  <li>
    <strong>임의적 상태 기반 처리</strong>는 <strong>사용자가 정의한 개념</strong>에 따라 상태를 관리하기 때문에 윈도우와 워터마크를 활용할 수 없다. 따라서 상태 정보를 적절히 제거해야 한다.
  </li>
</ul>

<br>

<h2>7-1. 타임아웃</h2>
<ul>
  <li>
    타임 아웃은 <strong>중간 상태를 제거</strong>하기 전에 <strong>기다려야 하는 시간</strong>을 정의한다.
  </li>
  <li>
    각 기별로 그룹이 존재한다고 했을 때 타임아웃은 <strong>전체 그룹에 대한 전역 파라미터</strong>로 동작한다.
  </li>
  <li>
    타임아웃은 <strong>처리 시간(GroupStateTimeout.ProcessingTimeTimeout)</strong>이나 <strong>이벤트 시간(GroupStateTimeout.EventTimeTimeout)</strong> 중 하나가 될 수 있다. 
  </li>
  <li>
    값을 처리하기 전 항상 타임아웃을 먼저 확인해야하며 <strong>state.hasTimedOut 값</strong>이나 <strong>values 이터레이터가 비어 있는지 확인</strong>하는 방식으로 타임아웃 정보를 얻을 수 있다.
  </li>
  <li>
    타임아웃을 설정하려면 반드시 <strong>상태</strong>를 정의해야한다.
  </li>
  <li>
    <strong>처리 시간 기반</strong>의 타임아웃을 사용할 때에는 <strong>GroupState.setTimeoutDuration</strong>으로 <strong>주기</strong>를 설정할 수 있으며 시간이 <strong>설정한 주기만큼 흐르면</strong> 타임아웃이 발생한다. <strong>D 밀리세컨드 간격의 타임아웃</strong>은 다음을 보장한다.
  </li>
    <ul>
      <li>
        <strong>D 밀리세컨드가 지나지 않으면</strong> 절대 타임아웃이 일어나지 않는다.
      </li>
      <li>
        타임아웃은 쿼리에 <strong>트리거</strong>가 있는 경우(D 밀리세컨드 이후)에만 일어난다.
      </li>
      <li>
        처리 시간 기반의 타임아웃은 <strong>시스템 시간의 변화</strong>에 영향을 받는다. 따라서 <strong>시간대 변경</strong>과 <strong>시간 지연</strong>을 고려해야한다.
      </li>
    </ul>
  <li>
    <strong>이벤트 시간 기준의 타임아웃</strong> 사용할 때에도 쿼리에 이벤트 시간 <strong>워터마크</strong>를 반드시 지정해야 한다. 이를 통해 타임아웃을 설정하여 <strong>워터마크보다 오래된 데이터</strong>를 필터링할 수 있다.
  </li>
  <li>
    개발자는 <strong>GroupState.setTimeoutTImestamp(...) API</strong>를 이용해 워터마크에서 참조할 <strong>타임아웃 타임스탬프</strong>를 설정할 수 있다.
  </li>
  <li>
    타임아웃은 워터마크가 <strong>타임스탬프를 초과</strong>했을 때 발생한다. 타임스탬프란 어떤 이벤트가 발생한 정확한 시각을 의미한다.
  </li>
    <ul>
      <li>
        <strong>워터마크를 더 길게</strong> 지정하거나 스트림을 처리할 때 <strong>타임아웃 시간을 변경</strong>하여 <strong>타임아웃 지연 시간을 조절</strong>할 수 있다. <strong>직접 작성한 코드</strong>를 사용하여 그룹별로 처리 또한 가능하다.
      </li>
    </ul>
  <li>
    <strong>이벤트 시간 기준 타임아웃</strong>은 <strong>워터마크가 설정한 타임아웃 시간</strong>이 되기 전까지 절대로 타임아웃이 일어나지 않는다. 처리 시간 기반의 타임아웃과 유사하게 엄격한 상한선은 없다.
  </li>
  <li>
    워터마크는 <strong>스트림</strong>에 데이터가 있고 데이터의 <strong>이벤트 시간이 실제 경과</strong>했을 경우에만 함께 변경된다. 즉, 실제로 지금까지 본 최대 event_time보다 큰(더 최신) event_time 데이터가 들어왔을 때만 최신 시간으로 업데이트된다. 
  </li>
</ul>

<br>

<h2>7-2. 출력 모드</h2>
<ul>
  <li>
    append 모드는 <strong>타임아웃 이후</strong>에 결과 셋에서 데이터를 볼 수 있다. 즉, <strong>워터마크</strong>를 지나야한다. 이런 처리는 자동으로 일어나지 않기에 <strong>사용자가 적합한 row를 출력</strong>해야 한다.
  </li>
</ul>

<br>

<h2>7-3. mapGroupWithState</h2>
<ul>
  <li>
    <strong>mapGroupsWithState</strong> 기능은 <strong>갱신된 데이터셋</strong>을 입력으로 받고, 값을 <strong>특정 key로 분배</strong>하는 사용자 정의 집계 함수와 유사하다.
  </li>
  <li>
    mapGroupsWithState응 다음의 사항을 정의해야한다.
  </li>
    <ul>
      <li>
        세 가지 클래스 정의: 입력 클래스, 상태 클래스, 출력 클래스. (선택정 정의).
      </li>
      <li>
        key, event 이터레이터 그리고 이전 상태를 기반으로 상태를 갱신하는 함수
      </li>
      <li>
        22.7.1 절 '타임아웃'에서 설명한 타임아웃 파라미터
      </li>
    </ul>
  <li>
    mapGroupsWithState 기능을 정의하여 <strong>임의 상태를 생성</strong>하고 <strong>연속적으로 수정</strong>한 후 <strong>제거</strong>할 수 있다.
  </li>
</ul>

```scala
// 1. 특정 사용자가 수행한 행동 중 한 가지를 선택해 처음과 마지막 타임스탬프를 찾는다.
//   - 그룹화와 매핑에 사용할 키는 사용자와 행동의 조합으로 이루어진다.

// 입력, 상태 그리고 출력용 케이스 클래스 정의.
case class InputRow(user:String, timestamp:java.sql.Timestamp, activity:String)
case class UserState(user:String,
  var activity:String,
  var start:java.sql.Timestamp,
  var end:java.sql.Timestamp)
```

```scala
// 2. 특정 row의 상태를 갱신하는 방법을 정의한 함수를 작성.
def updateUserStateWithEvent(state:UserState, input:InputRow):UserState = {
  if (Option(input.timestamp).isEmpty) {
    return state
  }
  if (state.activity == input.activity) {

    if (input.timestamp.after(state.end)) {
      state.end = input.timestamp
    }
  } else {
    if (input.timestamp.after(state.end)) {
      state.start = input.timestamp
      state.end = input.timestamp
      state.activity = input.activity
    }
  }

  state
}
```

```scala
// 3. 각 row에 epoch를 기준으로 상태를 갱신하는 방법을 함수로 정의
import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}

def updateAcrossEvents(user:String,
  inputs: Iterator[InputRow],
  oldState: GroupState[UserState]):UserState = {
  var state:UserState = if (oldState.exists) oldState.get else UserState(user, "",
      new java.sql.Timestamp(6284160000000L),
      new java.sql.Timestamp(6284160L)
  )
  // 시간 비교를 위해 이전 날짜를 간단하게 지정하고 데이터의 값을 기준으로 즉시 변경한다.
  for (input <- inputs) {
    state = updateUserStateWithEvent(state, input)
    oldState.update(state)
  }
  state
}
```

```scala
// 4. mapGroupsWithState를 사용할 때는 특정 그룹의 상태 제거 여부를 추가로 지정한다.
//  - 특정 시간 이후에 데이털르 받지 못할 경우 상태 정보에 어떤 처리를 해야 하는지 제어할 수 있는 메커니즘을 제공한다.
import ort.apache.spark.sql.streaming.GroupStateTimeout

withEventTime
  // 컬럼 선택 및 변환
  .selectExpr("User as user",
    "cast(Creation_Time/1000000000 as timestamp) s timestamp", "gt as activity")
  .as[InputRow]
  // key 기반 그룹핑
  .groupByKey(_.user)
  // Stateful 연산 적용
  //   - 각 key별로 상태(state)를 관리하며 updateAcrossEvents 함수 호출
  //   - GroupStateTimeout.NoTimeout → 상태에 대한 타임아웃 없이 무한 유지
  .mapGroupsWithState(GroupStateTimeout.NoTimeout)(updateAcrossEvents)
  .writeStream
  .queryName("events_per_window")
  .format("memory")
  .outputMode("update")
  .start()

// 결과 조회
SELECT * FROM events_per_window order by user, start
```

```scala
// < 예제: 카운트 기반 윈도우 >
// 1. 발생하는 이벤트 수를 기반으로 윈도우를 만들고 해당 데이터 윈도우에 특정 집계 연산 수
//    행.
//    - 각 장비가 주기적으로 읽은 값의 평균을 출력

// device와 timestamp 필드가 있는 입력용 케이스 클래스 정의
case class InputRow(device: String, timestamp: java.sql.Timestamp, x:Double)

// 수집된 레코드의 현재 수, 장비ID 그리고 윈도우에서 읽은 이벤트의 배열을 갖는 케이스 클래스
case class DeviceState(device: String, var values: Array[Double],
  var count:Int)

// 출력용 케이스 클래스
case class OutputRow(deivce: String, previousAverage: Double)
```

```scala
// 2. 특정 row를 이용해 상태를 갱신할 때 정확히 어떤 방식을 이용하면 좋은지 명확하게 나타낸
//    다.
def updateWithEvent(state:DeviceState, input:InputRow):DeviceState = {
  state.count += 1
  // x 축 값의 배열을 유지
  state.values = state.values ++ Array(input.x)
  state
}
```

```scala
// 3. 여러 입력 row를 갱신하는 함수 정의
import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}

//
def updateAcrossEvents(device:String, inputs: Iterator[InputRow],
  oldState: GroupState[DeivceState]):Iterator[OutputRow] = {
  inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>
    val state = if (oldState.exists) oldState.get
      else DeviceState(device, Array(), 0)
    
    val new State = updateWithEvent(state, input)\
    if (newState.count => 500) {
      // 윈도우 중 하나가 완료되면 비어 있는 DeviceState로 교체하고
      // 과거 상탯값에서 지난 500개 아이템의 평균을 구한 후 출력한다.
      oldState.update(DeviceState(device, Array(), 0))
      Iterator(OutputRow(device,
        newState.values.sum / newState.values.length.toDouble))
    }
    else {
      // 현재 DeviceState 객체로 업데이트하며 레코드를 출력하지 않는다.
      oldState.update(newState)
      Iterator()
    }
  }
}
```

```scala
// 4. 스트림 실행
import org.apache.spark.sql.streaming.GroupStateTimeout

withEventTime
  // 필요한 컬럼만 선택
  .selectExpr("Device as device",
    "cast(Creation_Time/1000000000 as timestamp) as timestmap", "x")
  // Dataset[InputRow] 형태로 캐스팅 (case class InputRow 필요)
  .as[InputRow]
  // 키별 그룹핑 (device 단위로 상태 관리)
  .groupByKey(_.device)
  // 상태 기반 처리 (사용자 정의 updateAcrossEvents 함수)
  // OutputMode.Append = 새로운 결과만 출력
  // GroupStateTimeout.NoTimeout = 상태가 시간으로 자동 삭제되지 않음
  .flatMapGroupsWithState(OutputMode.Append,
    GroupStateTimeout.NoTimeout)(updateAcrossEvents)
  .writeStream
  .queryName("count_based_device")
  .format("memory")
  // 출력모든 append로 지정.
  .outputMode("append")
  .start()
```

<br>

<h2>7-4. flatMapGroupsWithState</h2>
<ul>
  <li>
    flatMapGroupsWithState 기능은 <strong>단일 키의 출력 결과가 여러 개</strong> 만들어지는 것을 제외하면 mapGroupsWithState와 매우 유사하다.
  </li>
  <li>
    mapGroupsWithState에 적용된 기본 구조를 갖으면서 다음과 같은 <strong>더 나은 유연성</strong>을 제공한다.
  </li>
</ul>

```scala
// < 예제: 세션화 >
// 1. 세 가지 클래스 정의: 입력 클래스, 상태 클래스, 출력 클래스(선택적 정의).

// 입력 데이터의 스키마를 표현하는 case class
case class InputRow(uid:String, timestamp:java.sql.Timestamp, x:Double,
  activity:String)

// 사용자별 세션 상태를 저장하는 case class
case class UserSession(val uid:String, var timestamp:java.sql.Timestamp,
  var activities: Array[String], var values: Array[Double])

// 세션 처리 후 출력되는 case class
case class UserSessionOutput(val uid:String, var activities: Array[String], var xAvg:Double)
```

```scala
// 2. key, event iterator 그리고 이전 상태를 기바능로 상태를 갱신하는 함수 작성.
def updateWithEvent(state:UserSession, input:InputRow):UserSession = {
  // 비정상적인 날짜를 제어
  if (Option(input.timestamp).isEmpty) {
    return state
  }

  // 상태의 timestamp를 최신 이벤트의 timestamp로 갱신
  state.timestamp = input.timestamp

  // 새로운 x값을 기존 values 배열에 추가 (누적)
  state.values = state.values ++ Array(input.x)

  // 만약 활동(activity)이 기존 activities 배열에 없으면 추가
  if (!state.activities.contains(input.activity)) {
    state.activities = state.activities ++ Array(input.activity)
  }

  // 갱신된 상태 반환
  state
}
```

```scala
// 3. 22.7.1절 '타임아웃'에서 설명한 타임아웃 파라미터
import org.apache.spark.sql.streaming.{GroupStateTimeout, OutputMode, GroupState}

// 상태를 기반으로 사용자 이벤트 스트리밍을 처리하는 함수
def updateAcrossEvents(uid:String,
  // 새로운 입력 이벤트들의 Iterator
  inputs: Iterator[InputRow],
  // 이전에 저장된 세션 상태
  oldState: GroupState[UserSession]):Iterator[UserSessionOutput] = {
  
  // 입력 이벤트들을 timestamp 순으로 정렬하여 처리
  inputs.toSeq.sortBy(_.timestamp.getTime).toIterator.flatMap { input =>
    
    // 기존 상태가 있으면 가져오고, 없으면 새로운 세션을 초기화
    val state = if (oldState.exists) oldState.get else UserSession(
    uid,
    // 임의의 먼 미래 시점 초기화
    new java.sql.Timestamp(6284160000000L),
    // 활동 배열 초기화
    Array(),
    // 값 배열 초기화
    Array()
    )

    // 현재 이벤트를 반영하여 새로운 상태 생성
    val newState = updateWithEvent(state, input)
    
    // 상태가 타임아웃된 경우
    if (oldState.hasTimedOut) {
      val state = oldState.get
      oldState.remove()

      // 타임아웃 시점까지의 활동과 평균값을 출력
      Iterator(UserSessionOutput(uid,
      state.activities,
      newState.values.sum / newState.values.length.toDouble))

      // 상태에 저장된 값이 1000개 이상이면 강제로 세션 종료
    } else if (state.values.length > 1000) {
      val state = oldState.get
      oldState.remove()
      Iterator(UserSessionOutput(uid,
      state.activities,
      newState.values.sum / newState.values.length.toDouble))

      // 아직 세션을 유지해야 하는 경우
    } else {
      // 새로운 상태를 저장
      oldState.update(newState)
      // 5초 늦게 도착한 이벤트까지만 처리하도록 설정.
      oldState.setTimeoutTImestamp(newState.timestamp.getTime(), "5 seconds")
      Iterator()
    }
  }
}
```

```scala
// 4. 스트림 실행
import org.apache.spark.sql.streaming.GroupStateTimeout

withEventTime
  // x 값이 null이 아닌 이벤트만 필터링
  .where("x is not null")
  // 컬럼 가공: user → uid, Creation_Time을 timestamp로 변환, 
  .selectExpr("user as uid",
    "cast(Creation_Time/1000000000 as timestamp) as timestamp",
    "x", "gt as activity")
  // Dataset[InputRow] 형식으로 변환 (스칼라 case class 매핑)
  .as[InputRow]
  // 워터마크 설정: 이벤트 타임 기준 최대 5초 지연 허용
  .withWatermark("timestamp", "5 seconds")
  // 사용자 ID(uid) 기준으로 그룹핑
  .groupByKey(_.uid)
  // 상태 기반 처리 적용
  //    - OutputMode.Append: 새로운 결과만 출력
  //    - GroupStateTimeout.EventTimeTimeout: 이벤트 타임 기반 타임아웃 설정
  //    - updateAcrossEvents: 사용자 정의 함수로 상태 관리 및 출력 생성
  .flatMapGroupsWithState(OutputMode.Append,
    GroupStateTimeout.EventTimeTimeout)(updateAcrossEvents)
  // 결과를 메모리 테이블로 출력
  .writeStream
  .queryName("count_based_device")
  .format("memory")
  .start()
```