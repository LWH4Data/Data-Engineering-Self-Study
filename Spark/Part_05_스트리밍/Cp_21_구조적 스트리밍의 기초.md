```bash
MSYS_NO_PATHCONV=1 MSYS2_ARG_CONV_EXCL="*" \
docker run -it --rm \
  -p 4040:4040 \
  -v /c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/workspace/Spark-The-Definitive-Guide \
  apache/spark:3.5.2 \
  /opt/spark/bin/pyspark
```

<h1>1. 구조적 스트리밍 기초</h1>
<ul>
  <li>
    구조적 스트리밍은 <strong>Spark SQL 엔진 기반</strong>의 스트림 처리 프레임워크로 <strong>Spark의 구조적 API</strong>를 사용한다.
  </li>
  <li>
    스트리밍 연산은 배치 연산과 동일하게 표현한다.
  </li>
    <ul>
      <li>
        스트림 처리용 코드와 목적지를 정의하면 구조적 스트리밍 엔진에서 신규 데이터에 대한 <strong>증분</strong> 및 <strong>연속형 쿼리</strong>를 실행한다.
      </li>
      <li>
        코드 생성, 쿼리 최적화 등의 기능을 지원하는 <strong>카탈리스트 엔진</strong>을 사용해 연산에 대한 논리적 명령을 처리한다.
      </li>
    </ul>
  <li>
    체크포인팅과 WAL(write-ahead log) 등 내고장성 기능도 제공한다.
  </li>
  <li>
    구조적 스트리밍의 핵심 아이디어는 스트림 데이터를 <strong>데이터가 계속해서 추가되는 테이블</strong>처럼 다루는 것이다.
  </li>
    <ul>
      <li>
        Streaming job은 계속해서 <strong>신규 입력 데이터를 확인 및 처리</strong>하며 필요한 경우 <strong>상태 저장소(state store)</strong>에 있는 <strong>일부 상태를 갱신해 결과를 변경</strong>한다.
      </li>
    </ul>
  <li>
    배치 처리나 스트림 처리와 관련된 구문을 변경할 필요 없이 배치나 스트리밍 중 하나를 <strong>쿼리 실행 유형</strong>으로 지정하면 된다.
  </li>
  <li>
    구조적 스트리밍은 내부적으로 사용자 쿼리를 어떻게 증분 처리할지 <strong>자동을 파악</strong>하기에 내고장성을 보장하면서 신규 데이터가 유입될 때마다 <strong>효율적으로 처리 결과를 갱신</strong>한다.
  </li>
  <li>
    구조적 스트리밍에서는 <strong>DataFrame</strong>도 <strong>스트리밍 방식</strong>으로 동작한다.
  </li>
  <li>
    Spark의 나머지 기능과 구조적 스트리밍을 통합하여 데이터에 실시간으로 반응하는 <strong>통합 빅데이터 처리 앱</strong>을 구현할 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 핵심 개념</h1>
<ul>
  <li>
    Spark는 <strong>복잡한 처리를 자동</strong>으로 제어하면서 스트림에 <strong>모든 Spark 연산을 사용</strong>할 수 있는 단순한 방법을 제공하는 목적을 갖는다.
  </li>
</ul>

<br>

<h2>2-1. 트랜스포메이션과 액션</h2>
<ul>
  <li>
    구조적 스트리밍은 transformation과 action의 개념을 갖는다.
  </li>
  <li>
    <strong>transfomration</strong>은 <strong>증분 처리를 할 수 없는 일부 쿼리 유형</strong>을 제외하면 2장에서 소개한 기능을 그대로 사용할 수 있다.
  </li>
  <li>
    <strong>action</strong>은 스트림 처리를 시작한 뒤 <strong>연속적으로 처리해 결과를 출력</strong>하는 단 한 가지 action만 존재한다.
  </li>
</ul>

<br>

<h2>2-2. 입력 소스</h2>
<ul>
  <li>
    구조적 스트리밍은 <strong>스트리밍 방식</strong>으로 데이터를 읽을 수 있는 몇 가지 <strong>입력 소스</strong>를 지원한다.
  </li>
    <ul>
      <li>
        아파치 카프카 0.10 버전
      </li>
      <li>
        HDFS나 S3 등 분산 파일 시스템의 파일
      </li>
      <li>
        테스트용 소켓 소스
      </li>
    </ul>
</ul>

<br>

<h2>2-3. 싱크</h2>
<ul>
  <li>
    <strong>싱크(sink)</strong>로 스트림의 결과를 저장할 <strong>목적지</strong>를 명시한다. 싱크와 실행 엔진은 데이터 처리의 진행 상황을 신뢰도 있고 정확하게 추적하는 역할을 한다.
  </li>
  <li>
    Spark 2.2 버전에서 지원하는 출력용 싱크는 다음과 같다.
  </li>
    <ul>
      <li>
        아파치 카프카 0.10
      </li>
      <li>
        거의 모든 파일 포맷
      </li>
      <li>
        출력 레코드에 임의 연산을 실행하는 foreach 싱크
      </li>
      <li>
        테스트용 콘솔 싱크
      </li>
      <li>
        디버깅용 메모리 싱크
      </li>
    </ul>
</ul>

<br>

<h2>2-4. 출력 모드</h2>
<ul>
  <li>
    출력 모드를 지정해야 하는 상황들은 다음과 같은 상황들이 있다.
  </li>
    <ul>
      <li>
        신규 정보만 추가하려는 경우
      </li>
      <li>
        바뀐 정보로 기존 로우를 갱신하려는 경우
      </li>
      <li>
        매번 전체 결과를 덮어 쓰려는 경우.
      </li>
    </ul>
  <li>
    <strong>정적인 (static) 형태</strong>의 구조적 API 처럼 <strong>출력 모드(output mode)</strong>를 지정해야 한다.
  </li>
    <ul>
      <li>
        append: sink에 <strong>신규 레코드</strong>만 추가
      </li>
      <li>
        update: 변경 대상 <strong>레코드 자체를 갱신</strong>
      </li>
      <li>
        complete: <strong>전체 출력 내용</strong> 작성하기.
      </li>
    </ul>
  <li>
    특정 쿼리와 싱크는 일부 출력 모드만 지원한다.
  </li>
  <li>
    신규 데이터가 매번 유입되어 무한정 커지는 경우 complete는 적절하지 않다.
  </li>
  <li>
    한정된 key를 사용해 집계하는 경우 시간에 따라 일부 key값을 갱신해야하며 update 모드가 적합하다.
  </li>
</ul>

<br>

<h2>2-5. 트리거</h2>
<ul>
  <li>
    트리거는 <strong>데이터 출력 시점</strong>을 정의한다. 정확히는 구조적 스트리밍에서 <strong>언제 신규 데이터를 확인</strong>하고 <strong>결과를 갱신</strong>할지 결정한다.
  </li>
  <li>
    구조적 스트리밍은 기본적으로 <strong>마지막 입력 데이터를 처리한 직후</strong>에 신규 입력 데이터를 조회해 <strong>최단 시간 내</strong>에 새로운 처리 결과를 만든다.
  </li>
    <ul>
      <li>
        단, 파일 싱크를 사용하는 경우 작은 파일이 여러 개 생길 수 있다. 이 경우 Spark의 <strong>처리 시간 기반</strong>의 트리거를 활용할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>2-6. 이벤트 시간 처리</h2>
<ul>
  <li>
    구조적 스트리밍은 무작위로 도착한 <strong>레코드에 기록된 타임스템프</strong>를 기준으로 하는 <strong>이벤트 시간 기준</strong>의 처리도 지원한다.
  </li>
</ul>

<h3>2-6-1. 이벤트 시간 데이터</h3>
<h4>2-6-1-1. 이벤트 시간(event-time)</h4>
<ul>
  <li>
    이벤트 시간(event-time)은 <strong>데이터에 기록된 시간 필드</strong>를 의미한다.
  </li>
  <li>
    Spark는 데이터가 <strong>데이터 생성 시간</strong>을 기준으로 처리를 하기에 업로드 혹은 네트워크 지연으로 인한 <strong>시간 차이와 무관</strong>하게 처리할 수 있다.
  </li>
  <li>
    시스템은 입력 데이터를 테이블로 인식하기에 <strong>이벤트 시간</strong>은 테이블에 있는 <strong>하나의 컬럼</strong>일 뿐이다.
  </li>
    <ul>
      <li>
        따라서 <strong>표준 SQL 연산자</strong>를 활용하여 그룹화, 집계 그리고 윈도우 처리를 할 수 있다.
      </li>
      <li>
        구조적 스트리밍이 내부적으로 이벤트 시간 필드를 인식하면 쿼리 실행 최적화나 타임 윈도우에서 상태 정보 제거 시점 결정 등 <strong>특별한 작업을 수행</strong>할 수 있다.
      </li>
    </ul>
  <li>
    위오 같은 작업을 제어할 때 <strong>워터마크</strong>를 사용한다.
  </li>
</ul>

<h4>2-6-1-2. 워터마크</h4>
<ul>
  <li>
    <strong>워터마크(watermark)</strong>는 <strong>시간 제한을 설정</strong>할 수 있는 스트리밍 시스템으로 <strong>늦게 들어온 데이터</strong>를 어디까지 처리할지 시간을 제한할 수 있다.
  </li>
  <li>
    Watermark는 과거 <strong>데이터의 보관 주기를 제한</strong>하거나 특정 이벤트 시간의 <strong>윈도우 결과를 출력하는 시점을 제어</strong> 할 때에도 사용한다.
  </li>
  <li>
    스트리밍 DataFrame Spark 앱에서 <strong>스트리밍 DataFrame을 생성</strong>한 후 <strong>transformation을 수행</strong>해 <strong>적합한 포맷의 데이터</strong>를 얻는다.
  </li>
  <li>
    구조적 스트리밍에서 스키마 추론 기능을 사용하고 싶다면 명시적으로 설정해야 한다. 단, 모르는 사이 데이터가 바뀔 수 있기 때문에 <strong>운영 환경에서는 스키마 추론을 사용하면 안 된다</strong>.
  </li>
  <li>
    <strong>스트리밍 DataFrame의 생성과 실행</strong>은 다른 Spark API처럼 <strong>지연 처리 방식</strong>으로 동작한다.
  </li>
    <ul>
      <li>
        따라서 action을 호출하기 전에 스트리밍 <strong>DataFrame에 대한 transformation</strong>을 지정할 수 있다.
      </li>
    </ul>
  <li>
    Spark는 각 스트림에 <strong>UUID</strong>를 부여하여 필요한 경우 실행 중인 스트림 목록을 조회해 이전에 만들었던 스트림을 <strong>다시 사용</strong>할 수 있다.
  </li>
</ul>

<br><br>

<h1>3. 구조적 스트리밍 활용</h1>

```python
# 1. 실습을 위한 데이터 불러오기
static = spark.read.json("/workspace/Spark-The-Definitive-Guide/data/activity-data/")
dataSchema = static.schema

# 스키마 확인
static.printSchema()

# 세 개의 row만 확인
static.show(3, truncate=False)
```

```python
# 2. 앞서 설정한 스키마를 기반으로 트리거를 설정하여 스트리밍 데이터 생성.
#   - 스트리밍은 파일을 차례로 읽는 방식으로 만든다.
streaming = spark.readStream.schema(dataSchema)\
    # 매번 trigger당 최대 1개의 파일만 읽도록 설정 (시뮬레이션용)
    .option("maxFilesPerTrigger", 1)\
    # JSON 파일 소스 지정 (activity-data 디렉토리에서 읽기)
    .json("/workspace/Spark-The-Definitive-Guide/data/activity-data")
```

```python
# 3. action을 통해 스트리밍 DataFrame을 생성하기 전에 transformation 설정.
activityCounts = streaming.groupBy("gt").count()
```

```python
# 4. 로컬 모드로 돌아가기에 셔플 파티션 수를 조정.
spark.conf.set("spark.sql.shuffle.partitions", 5)
```

```python
# 5. 싱크를 지정하는 과정에서 Spark가 데이터를 출력하는 방식을 complete로 출력 모드로 지
#    정.
activityQuery = activityCounts.writeStream\
    # 쿼리 이름을 지정 (Spark UI 등에서 모니터링할 때 표시됨)
    .queryName("activity_counts")\
    # 결과를 메모리 테이블에 저장 (SQL로 조회 가능)
    .format("memory")\
    # 출력 모드: 전체 집계 결과를 매번 완전히 덮어씀
    .outputMode("complete")\
    # 스트리밍 쿼리를 시작 (백그라운드에서 실행됨)
    # 독립 실행 스크립트에서는 프로그램이 종료되므로 awaitTermination() 필요
    .start()
```

```python
# 6. 실행 중인 스트림 목록 확인.
spark.streams.active
```

```python
# 7. 저장된 메모리 테이블을 조회하여 결과를 확인.
from time import sleep

for x in range(5):
    spark.sql("SELECT * FROM activity_counts").show()
    sleep(1)
```

<br><br>

<h1>4. 스트림 트랜스포메이션</h1>
<ul>
  <li>
    스트리밍 transformation은 2부에서 설명한 <strong>정적 DataFrame의 transformation</strong>을 대부분 포함한다. 추가로 <strong>DataFrame의 모든 함수</strong>와 <strong>개별 컬럼 처리</strong>도 지원한다.
  </li>
  <li>
    스트리밍 데이터에 맞지 않는 transformation에 제약이 있는데 버전마다 업데이트 될 수 있기에 공식문서를 확인하는 것이 좋다.
  </li>
</ul>

<br>

<h2>4-1. 선택과 필터링</h2>
<ul>
  <li>
    구조적 스트리밍은 DataFrame의 모든 함수와 개별 컬럼을 처리하는 선택과 필터링 그리고 단순 transformation을 지원한다.
  </li>
</ul>

```python
# 1. 선택과 필터링을 적용하고 append 출력 모드 사용.
from pyspark.sql.functions import expr

simpleTransform = streaming.withColumn("stairs", expr("gt like '%stairs%'"))\
    .where("stairs")\
    .where("gt is not null")\
    .select("gt", "model", "arrival_time", "creation_time")\
    .writeStream\
    .queryName("simple_transform")\
    .format("memory")\
    .outputMode("append")\
    .start()

# 쿼리 상태 확인.
spark.streams.active

# 메모리 테이블 조회
spark.sql("SELECT * FROM simple_transform").show(5, truncate=False)
```

<br>

<h2>4-2. 집계</h2>

```python
# 1. 구조적 스트리밍에서 집계 연산 활용.
deviceModelStats = streaming.cube("gt", "model").avg()\
    .drop("avg(Arrival_time)")\
    .drop("avg(Creation_Time)")\
    .drop("avg(Index)")\
    .writeStream.queryName("device_counts").format("memory")\
    .outputMode("complete")\
    .start()

# 메모리 테이블에서 결과 확인
spark.sql("SELECT * FROM device_counts").show(5, truncate=False)
```

<br>

<h2>4-3. 조인</h2>

```python
# 1. 스티리밍 DataFrame과 정적 DataFrame 조인
#   - 직접 종료하지 않으면 무한히 스트리밍된다.
historicalAgg = static.groupBy("gt", "model").avg()
deviceModelStats = streaming.drop("Arrival_Time", "Creation_Time", "Index")\
    .cube("gt", "model").avg()\
    .join(historicalAgg, ["gt", "model"])\
    .writeStream.queryName("device_counts_v2").format("memory")\
    .outputMode("complete")\
    .start()

# 메모리 테이블의 결과 조회
spark.sql("SELECT * FROM device_counts_v2").show(5, truncate=False)
```

<br><br>

<h1>5. 입력과 출력</h1>
<ul>
  <li>
    최근에 추가된 소스와 싱크는 공식 문서를 통해 확인할 수 있다.
  </li>
</ul>

<br>

<h2>5-1. 데이터를 읽고 쓰는 장소(소스와 싱크)</h2>
<h3>5-1-1. 파일 소스와 싱크</h3>
<ul>
  <li>
    가장 간단한 소스는 <strong>파일 소스</strong>이며 실전에서는 <strong>파케이, 텍스트, JSON 그리고 CVS 파일</strong>을 자주 사용한다.
  </li>
  <li>
    스트리밍에서 <strong>파일 소스/싱크</strong>를 사용할 때 정적 파일 소스와의 유일한 차이점은 maxFilesPerTrigger를 통해 트리거 시 <strong>읽을 파일 수</strong>를 결정한다는 것이다.
  </li>
  <li>
    모든 파일은 스트리밍 작업에서 바라보는 입력 디렉터리에 <strong>원자적으로 추가</strong> 되어야 한다.
  </li>
    <ul>
      <li>
        <strong>원자적</strong>: 파일이 <strong>완전히 쓰여진</strong> 상태에서만 스트리밍 입력 디렉터리에 나타나야 한다.
      </li>
      <li>
        원자적으로 추가하지 않으면 Spark에서 <strong>파일의 일부분</strong>만 처리한다.
      </li>
      <li>
        <strong>로컬 파일 시스템이나 HDFS</strong>에서는 <strong>부분 기록된 파일</strong>을 볼 수 있다. 반면 <strong>아마존 S3</strong>에서는 <strong>완전히 기록된 객체</strong>만 보인다.
      </li>
    </ul>
</ul>

<h3>5-1-2. 카프카 소스와 싱크</h3>
<ul>
  <li>
    Kafka는 <strong>분산형 버퍼</strong>로 생각할 수 있으며 <strong>레코드의 스트림</strong>은 <strong>토픽(topic)</strong>으로 불리는 카테고리에 저장한다.
  </li>
  <li>
    Kafka의 각 <strong>레코드</strong>는 <strong>key</strong>, <strong>value</strong> 그리고 <strong>tiemstamp</strong>로 구성된다.
  </li>
  <li>
    Topic은 <strong>순서를 바꿀 수 없는</strong> 레코드로 구성되며 레코드의 위치를 <strong>오프셋(offset)</strong>이라 한다.
  </li>
  <li>
    데이터를 읽는 동작은 <strong>구독(subscribe)</strong>, 쓰는 동작은 <strong>발행(publish)</strong>이라고 표현한다.
  </li>
  <li>
    Spark는 Kafka에 저장된 스트림을 배치와 스트리밍 방식으로 읽어 <strong>DataFrame을 생성</strong>할 수 있다.
  </li>
</ul>

<br>

<h2>5-2. 카프카 소스에서 메시지 읽기</h2>
<ul>
  <li>
    메시지를 읽기 위해 다음 세 가지 옵션 중 하나를 선택한다. Kafka에서 메시지를 읽을 때에는 아래 셋 중 하나의 옵션을 선택한다.
  </li>
    <ul>
      <li>
        assign
      </li>
      <li>
        subscribe
      </li>
      <li>
        subscribePattern
      </li>
    </ul>
  <li>
    <strong>assign</strong>은 <strong>토픽</strong> 뿐만 아니라 <strong>읽으려는 파티션</strong>까지 세밀하게 지정하는 옵션이다. 관련 정보는 <strong>JSON 문자열</strong> ({"topicA":[0,1],"topicB":[2,4]})로 표현한다.
  </li>
  <li>
    <strong>subscribe</strong>와 <strong>subscribePattern</strong>은 각각 <strong>토픽 목록</strong>과 <strong>패턴</strong>을 지정해 <strong>여러 토픽을 구독</strong>하는 옵션이다.
  </li>
  <li>
    두 번째로 할 일은 kafka.bootstrap.servers 값을 지정하는 것이다. 따라서 하단의 옵션을 축라로 설정해야 한다.
  </li>
    <ul>
      <li>
        <strong>startingOffsets 및 endingOffsets</strong>
      </li>
        <ul>
          <li>
            쿼리를 <strong>시작할 때 읽을 지점</strong>을 의미하며 <strong>새로운 스트리밍 쿼리</strong>가 시작될 때만 적용된다. (<strong>다시 시작한 쿼리</strong>는 쿼리가 남긴 <strong>오프셋</strong>을 을 사용한다.)
          </li>
          <li>
            옵션 값은 가장 작은 오프셋부터 읽는 <strong>earliest</strong> 혹은 가장 큰 오프셋부터 읽는 <strong>latest</strong> 지정한다.
          </li>
          <li>
            또 다른 방법은 TopicPartition에 대한 <strong>시작 오프셋을 명시한 JSON 문자열</strong>을 사용해 지정한다. JSON 오프셋을 -2로 지정하면 eariiest로, -1로 지정하면 latest로 동작한다.
          </li>
        </ul>
      <li>
        <strong>failOnDataLoss</strong>
      </li>
        <ul>
          <li>
            <strong>데이터 유실</strong>이 발생할 때 쿼리를 중단할 것인지 결정한다. 데이터 유실이란 <strong>토픽이 삭제</strong>되거나 <strong>오프셋이 범위를 벗어났을 때</strong>를 의미한다.
          </li>
          <li>
            기본값은 true이며 잘못된 정보를 전달할 경우 비활성화 할 수 있다.
          </li>
        </ul>
      <li>
        <strong>maxOffsetPerTrigger</strong>
      </li>
        <ul>
          <li>
            특정 트리거 시점에 읽을 <strong>오프셋의 전체 개수</strong>
          </li>
        </ul>
    </ul>
  <li>
    Kafka 소스의 각 row는 다음과 같은 스키마를 갖는다.
  </li>
    <ul>
      <li>
        키: binary
      </li>
      <li>
        값: binary
      </li>
      <li>
        토픽: string
      </li>
      <li>
        패턴: int
      </li>
      <li>
        오프셋: long
      </li>
      <li>
        타임스탬프: long
      </li>
    </ul>
  <li>
    Kafka의 메시지는 다양한 방식으로 <strong>직렬화</strong>될 수 있다.
  </li>
  <li>
    구조적 API에서 자제적으로 지원하는 <strong>Spark 함수</strong>나 <strong>사용자 정의 함수</strong>를 사용해 메시지를 분석하기 좋은 <strong>구조적인 포맷</strong>으로 변경할 수 있다.
  </li>
  <li>
    Kafka에서 데이터를 읽거나 쓸 때는 <strong>JSON</strong>이나 <strong>Avro</strong>를 자주 사용한다.
  </li>
</ul>

```python
# 1. kafka의 메시지를 읽는다.
#   - kafka 컨슈머의 타임아웃, 재시도 횟수 그리구 주기를 설정하는 옵션도 있다.

# topic1 수신
df1 = spark.readStream.format("kafka")\
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
    .option("subscribe", "topic1")\
    .load()

# 여러 개의 토픽 수신
df2 = spark.readStream.format("kafka")\
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
    .option("subscribe", "topic1,topic2")\
    .load()

# 패턴에 맞는 토픽 수신
df3 = spark.readStream.format("kafka")\
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
    .option("subscribePattern", "topic.*")\
    .load()
```

<br>

<h2>5-3. 카프카 싱크에 메시지 쓰기</h2>
<ul>
  <li>
    Kafka로 메시지를 발행하는 쿼리와 읽는 쿼리는 몇 가지 파라미터를 제외하고 매우 비슷하다.
  </li>
  <li>
    <strong>kafka.bootstrap.servers 속성</strong>을 명시하고 옵션으로 <strong>토픽 명세</strong>와 <strong>컬럼</strong>을 함께 지정한다.
  </li>
</ul>

```python
# 1. kafka 싱스케 메시지 쓰기
df1.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)")\
    .writeStream\
    .format("kafka")\
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
    .option("checkpointLocation", "/to/HDFS-compatible/dir")\
    .start()

df1.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")\
    .writeStream\
    .format("kafka")\
    .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
    .option("checkpointLocation", "/to/HDFS-compatible/dir")\
    .option("topic", "topic1")\
    .start()
```

<h3>5-3-1. foreach 싱크</h3>
<ul>
  <li>
    foreach 싱크는 각 파티션에서 임의의 연산을 <strong>병렬로 수행</strong>한다. (당시 Spark 버전에서 python에서는 사용 불가했음).
  </li>
  <li>
    foreach 싱크를 사용하려면 <strong>ForeachWriter 인터페이스</strong>를 구현해야한다. 인터페이스는 <strong>open</strong>, <strong>process</strong> 그리고 <strong>close</strong> 세 가지 메서드를 갖으며 <strong>트리거 후 출력을 생성</strong>할 때마다 호출된다.
  </li>
  <li>
    인터페이스 구현시 다음 사항들을 알아야 한다.
  </li>
    <ul>
      <li>
        writer는 UDF나 Dataset 맵 함수처럼 반드시 <strong>Serialiable 인터페이스</strong>를 구현해야 한다.
      </li>
      <li>
        세 가지 메서드(open, process, close)는 <strong>각 익스큐터</strong>에서 실행된다.
      </li>
      <li>
        writer는 연결을 맺거나 트랜잭션을 시작하는 등 모든 초기화 작업을 반드시 <strong>open 메서드</strong>에서 수행해야 한다. 흔한 문제는 open이 아닌 부분에서 초기화해 <strong>드라이버에서 초기화</strong>되는 경우이다.
      </li>
    </ul>
  <li>
    foreach 싱크는 임의의 사용자 코드를 실행하기에 반드시 <strong>내고장성</strong>을 고려해 사용해야 한다. 
  </li>
  <li>
    구조적 스트리밍 API는 <strong>정확히 한 번 처리</strong>할 수 있도록 몇 가지 파라미터를 제공한다.
  </li>
    <ul>
      <li>
        ForeachWriter의 open 메서드는 두 개의 파라미터를 사용하며 두 파라미터는 처리하려는 <strong>row를 식별</strong>할 수 있는 고윳값이다.
      </li>
        <ul>
          <li>
            <strong>version 파라미터</strong>는 트리거별로 <strong>단순하게 증가하는 ID</strong> 값이고, <strong>partitionId</strong>는 <strong>태스크의 출력 파티션 ID</strong>이다.
          </li>
          <li>
            open 메서드는 해당 row의 <strong>처리 여부</strong>를 반환환다. 처리할 row의 <strong>출력 상황</strong>은 <strong>외부에서 관리하는 싱크의 출력 정보</strong>로 확인할 수 있다.
          </li>
          <li>
            <strong>이미 출력</strong>된 경우 open 메서드에서 <strong>false를 반환</strong>하여 row가 처리되지 않도록 한다. (반대의 경우는 true를 반환).
          </li>
          <li>
            사용자가 작성한 ForeachWriter는 데이터를 쓰기 위해 <strong>각 트리거마다</strong> 다시 열린다.
          </li>
        </ul>
      <li>
        open 메서드가 <strong>true를 반환</strong>하면 process 메서드는 데이터의 <strong>레코드마다 호출</strong>된다. 해당 메서드는 데이터를 <strong>처리하거나 저장</strong>하는 용도로만 사용한다.
      </li>
      <li>
        open 메서드가 호출되면 장애가 발생하지 않는 한 close 메서드도 호출된다. <strong>장애가 발생</strong>하는 경우 <strong>close 메서드는 오류를 받게</strong> 되며 사용자는 close 메서드에서 <strong>열려 있는 자원을 해제</strong>해야 한다.
      </li>
    </ul>
</ul>

```scala
// 1. scala를 통한 foreach 싱크 구현
import org.apache.spark.sql.ForeachWriter

// Dataset<String> 데이터 타입으로 선언된 datasetDfString 변수를 사용할 때의 예제
datasetOfString.writeStream.foreach(new ForeachWriter[String]) {
  def open(partitionId: Long, version: Long): Boolean = {
    // 데이터베이스 연결 생성
  }
  def close(errorOrNull: Throwable): Unit = {
    // 데이터베이스 연결 종료
  }
}
```

<h3>5-3-2. 테스트용 소스와 싱크</h3>
<ul>
  <li>
    Spark는 스트리밍 쿼리의 프로토타입을 만들거나 디버깅 시 유용한 몇 가지 테스트용 소스와 싱크를 제공하여 테스트 해볼 수 있다. 단, 테스트 용도로만 활용해야 한다.
  </li>
  <li>
    운영 환경에서 대화형 SQL을 실행하기 위해 출력 데이터를 <strong>테이블에 반드시 저장</strong>해야 할 경우 <strong>파케이 파일 싱크</strong>를 이용해 <strong>분산 파일 시스템(e.g. S3)에 저장</strong>하는 방법을 권장한다.
  </li>
</ul>

```python
# 1. 소켓 소스
#   - 소켓 소스를 사용하여 스트림 데이터를 전송.
#   - 데이터를 읽기 위한 호스트와 포트를 지정.
#     - Spark는 해당 주소에서 데이터를 읽기 위해 새로운 TCP 연결 생성.
#   - 소켓은 드라이버에 있어 종단 간 내고장성을 보장하지 않기에 운영환경에서 X
socketDF = spark.readStream.format("socket") \
    .option("host", "host.docker.internal") \
    .option("port", 9999) \
    .load()
```

```python
# 1-1. 텍스트를 입력 데이터로 전송 (컨테이너로 실습 중이고 포트를 열지 않아 실행은 안됨).
#   - 소켓 소스는 입력 데이터 한 줄을 하나의 텍스트 문장렬 row로 구성한 테이블을 반환한다.
query = socketDF.writeStream.format("console") \
    .outputMode("append") \
    .start()
```

```python
# 2. 콘솔 싱크
#   - 스트리밍 쿼리의 처리 결과를 콘솔로 출력할 때 사용한다.
#   - 디버깅에 유용하지만 내고장성을 지원하지 않는다.
#   - 기본적으로 append와 complete 출력 모드를 지원한다.
activityCounts.writeStream.format("console").outputMode("complete").start()
```

```scala
// 3. 메모리 싱크
//   - 스트리밍 시스템을 테스트하는 데 사용한다.
//   - 드라이버에 데이터를 모은 후 대화형 쿼리가 가능한 메모리 테이블을 적재한다.
//   - 마찬가지로 내고장성을 제공하지 않는다.
//   - 개발 중 스트림을 테스트하고 쿼리할 때 사용한다.
//   - 기본적으로 append와 complete 출력 모드를 지원한다.
activityCounts.writeStream.format("memory").queryName("my_device_table")
```

<br>

<h2>5-4. 데이터 출력 방법(출력 모드)</h2>
<h3>5-4-1. append 모드</h3>
<ul>
  <li>
    기본 동작 방식이며 가장 이해하기 쉽다.
  </li>
  <li>
    새로운 row가 결과 <strong>테이블에 추가</strong>되면 사용자가 명시한 트리거에 맞춰 싱크로 출력 된다.
  </li>
  <li>
    내고장성을 보장하는 싱크를 사용한다는 가정하에 <strong>모든 row를 한 번만</strong> 출력한다.
  </li>
</ul>

<h3>5-4-2. complete 모드</h3>
<ul>
  <li>
    결과 테이블의 <strong>전체 상태</strong>를 싱크로 출력한다.
  </li>
  <li>
    모든 데이터가 계속해서 변경될 수 있는 일부 <strong>상태 기반 데이터(stateful data)</strong>를 다룰 때 유용하다.
  </li>
  <li>
    사용 중인 싱크가 <strong>저수준 업데이트</strong>를 지원하지 않을 때에도 유용하며 <strong>이전 배치가 실행되었을 시점</strong>의 스트림 상태인 것처럼 생각할 수 있다.
  </li>
</ul>

<h3>5-4-3. update 모드</h3>
<ul>
  <li>
    이전 출력 결과에서 <strong>변경된 row</strong>만 출력한다. 
  </li>
  <li>
    해당 모드를 지원하는 싱크는 반드시 <strong>저수준 업데이트를 지원</strong>해야 한다.
  </li>
  <li>
    쿼리에서 <strong>집계 연산</strong>을 하지 않는다면 append 모드와 동일하다.
  </li>
</ul>

<h3>5-4-4. 적절한 출력 모드 선택하기</h3>
<ul>
  <li>
    구조적 스트리밍에서는 <strong>쿼리 유형별</strong>로 사용할 수 있는 출력 모드가 정해져 있다. 이는 쿼리 유형에 따라 발생할 수 있는 <strong>자원 소비 현상</strong>을 방지한다.
  </li>
  <li>
    쿼리 유형과 출력 모드는 공식 문서를 통해 확인할 수 있다.
  </li>
</ul>

<br>

<h2>5-5. 데이터 출력 시점(트리거)</h2>
<ul>
  <li>
    데이터를 <strong>싱크로 출력하는 시점</strong>을 제어하려면 <strong>트리거(trigger)</strong>를 설정해야 한다.
  </li>
  <li>
    Trigger는 <strong>부하 발생</strong>과 <strong>파일 크기를 제어</strong>하는 용도로도 사용할 수 있다.
  </li>
  <li>
    책의 시점에서는 처리 시간 기반의 <strong>주기형 트리거(periodic trigger)</strong>와 처리 단계를 수동으로 한 번만 실행할 수 있는 <strong>일회성 트리거(once trigger)</strong>를 제공한다.
  </li>
</ul>

<h3>5-5-1. 처리 시간 기반 트리거</h3>
<ul>
  <li>
    처리 시간 기반 트리거는 처리 주기를 <strong>문자열</strong>(scala의 Duration 혹은 java의 TimeUnit도 사용가능)로 지정한다.
  </li>
  <li>
    ProcessingTrigger는 결과를 출력하기 위해 <strong>특정 주기만큼 여러 번 대기</strong>한다.
  </li>
</ul>

```python
# 1. 문자열 포맷 예
activityCounts.writeStream.trigger(processingTime='5 seconds')\
    .format("console")\
    .outputMode("complete")\
    .start()
```

<h3>5-5-2. 일회성 트리거</h3>
<ul>
  <li>
    <strong>개발 환경</strong>에서는 트리거에서 <strong>한 번에 처리</strong>할 수 있는 수준의 데이터로 <strong>앱을 테스트</strong> 할 수 있다.
  </li>
  <li>
    <strong>운영 환경</strong>에서는 자주 실행되지 않는 잡을 <strong>수동을 실행</strong>할 때 사용할 수 있다.
  </li>
  <li>
    일회성이지만 처리된 모든 입력 파일과 연산 과정의 <strong>상태 정보</strong>를 알 수 있기에 배치 작업에서 관련 정보를 추적하는 코드를 작성하는 방식보다 쉽고 <strong>연속형 처리에 필요한 자원을 절약</strong>할 수 있다.
  </li>
</ul>

```python
# 1. 일회성 트리거 작성.
activityCounts.writeStream.trigger(once=True)\
    .format("console")\
    .outputMode("complete")\
    .start()
```

<br><br>

<h1>6. 스트리밍 Dataset API</h1>
<ul>
  <li>
    스트림에 처리할 수 있는 API는 다양하며 필요에 따라 활용하면 된다.
  </li>
</ul>

```scala
// 1. 다른 API를 활용하여 스트리밍 처리
case class Flight(DEST_COUNTRY_NAME: String, ORIGIN_COUNTRY_NAME: String, count: BigInt)

val dataSchema = spark.read
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")
  .schema

val flightsDF = spark.readStream.schema(dataSchema)
  .parquet("/data/flight-data/parquet/2010-summary.parquet/")

val flights = flightDF.as[Flight]

def originIsDestination(flight_row: Flight): Boolean = {
  return flight_row.ORIGIN_COUNTRY_NAME == flight_row.DEST_COUNTRY_NAME
}

flights.filter(flight_row => originIsDestination(flight_row))
  .groupByKey(x => x.DEST_COUNTRY_NAME).count()
  .writeStream.qeuryName("device_counts").format("memory").outputMode("complete")
  .start()
```