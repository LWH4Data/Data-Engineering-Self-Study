```bash
MSYS_NO_PATHCONV=1 MSYS2_ARG_CONV_EXCL="*" \
docker run -it --rm \
  -p 4040:4040 \
  -v /c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/workspace/Spark-The-Definitive-Guide \
  apache/spark:3.5.2 \
  /opt/spark/bin/pyspark
```

<h1>1. 구조적 스트리밍 기초</h1>
<ul>
  <li>
    구조적 스트리밍은 <strong>Spark SQL 엔진 기반</strong>의 스트림 처리 프레임워크로 <strong>Spark의 구조적 API</strong>를 사용한다.
  </li>
  <li>
    스트리밍 연산은 배치 연산과 동일하게 표현한다.
  </li>
    <ul>
      <li>
        스트림 처리용 코드와 목적지를 정의하면 구조적 스트리밍 엔진에서 신규 데이터에 대한 <strong>증분</strong> 및 <strong>연속형 쿼리</strong>를 실행한다.
      </li>
      <li>
        코드 생성, 쿼리 최적화 등의 기능을 지원하는 <strong>카탈리스트 엔진</strong>을 사용해 연산에 대한 논리적 명령을 처리한다.
      </li>
    </ul>
  <li>
    체크포인팅과 WAL(write-ahead log) 등 내고장성 기능도 제공한다.
  </li>
  <li>
    구조적 스트리밍의 핵심 아이디어는 스트림 데이터를 <strong>데이터가 계속해서 추가되는 테이블</strong>처럼 다루는 것이다.
  </li>
    <ul>
      <li>
        Streaming job은 계속해서 <strong>신규 입력 데이터를 확인 및 처리</strong>하며 필요한 경우 <strong>상태 저장소(state store)</strong>에 있는 <strong>일부 상태를 갱신해 결과를 변경</strong>한다.
      </li>
    </ul>
  <li>
    배치 처리나 스트림 처리와 관련된 구문을 변경할 필요 없이 배치나 스트리밍 중 하나를 <strong>쿼리 실행 유형</strong>으로 지정하면 된다.
  </li>
  <li>
    구조적 스트리밍은 내부적으로 사용자 쿼리를 어떻게 증분 처리할지 <strong>자동을 파악</strong>하기에 내고장성을 보장하면서 신규 데이터가 유입될 때마다 <strong>효율적으로 처리 결과를 갱신</strong>한다.
  </li>
  <li>
    구조적 스트리밍에서는 <strong>DataFrame</strong>도 <strong>스트리밍 방식</strong>으로 동작한다.
  </li>
  <li>
    Spark의 나머지 기능과 구조적 스트리밍을 통합하여 데이터에 실시간으로 반응하는 <strong>통합 빅데이터 처리 앱</strong>을 구현할 수 있다.
  </li>
</ul>

<br><br>

<h1>2. 핵심 개념</h1>
<ul>
  <li>
    Spark는 <strong>복잡한 처리를 자동</strong>으로 제어하면서 스트림에 <strong>모든 Spark 연산을 사용</strong>할 수 있는 단순한 방법을 제공하는 목적을 갖는다.
  </li>
</ul>

<br>

<h2>2-1. 트랜스포메이션과 액션</h2>
<ul>
  <li>
    구조적 스트리밍은 transformation과 action의 개념을 갖는다.
  </li>
  <li>
    <strong>transfomration</strong>은 <strong>증분 처리를 할 수 없는 일부 쿼리 유형</strong>을 제외하면 2장에서 소개한 기능을 그대로 사용할 수 있다.
  </li>
  <li>
    <strong>action</strong>은 스트림 처리를 시작한 뒤 <strong>연속적으로 처리해 결과를 출력</strong>하는 단 한 가지 action만 존재한다.
  </li>
</ul>

<br>

<h2>2-2. 입력 소스</h2>
<ul>
  <li>
    구조적 스트리밍은 <strong>스트리밍 방식</strong>으로 데이터를 읽을 수 있는 몇 가지 <strong>입력 소스</strong>를 지원한다.
  </li>
    <ul>
      <li>
        아파치 카프카 0.10 버전
      </li>
      <li>
        HDFS나 S3 등 분산 파일 시스템의 파일
      </li>
      <li>
        테스트용 소켓 소스
      </li>
    </ul>
</ul>

<br>

<h2>2-3. 싱크</h2>
<ul>
  <li>
    <strong>싱크(sink)</strong>로 스트림의 결과를 저장할 <strong>목적지</strong>를 명시한다. 싱크와 실행 엔진은 데이터 처리의 진행 상황을 신뢰도 있고 정확하게 추적하는 역할을 한다.
  </li>
  <li>
    Spark 2.2 버전에서 지원하는 출력용 싱크는 다음과 같다.
  </li>
    <ul>
      <li>
        아파치 카프카 0.10
      </li>
      <li>
        거의 모든 파일 포맷
      </li>
      <li>
        출력 레코드에 임의 연산을 실행하는 foreach 싱크
      </li>
      <li>
        테스트용 콘솔 싱크
      </li>
      <li>
        디버깅용 메모리 싱크
      </li>
    </ul>
</ul>

<br>

<h2>2-4. 출력 모드</h2>
<ul>
  <li>
    출력 모드를 지정해야 하는 상황들은 다음과 같은 상황들이 있다.
  </li>
    <ul>
      <li>
        신규 정보만 추가하려는 경우
      </li>
      <li>
        바뀐 정보로 기존 로우를 갱신하려는 경우
      </li>
      <li>
        매번 전체 결과를 덮어 쓰려는 경우.
      </li>
    </ul>
  <li>
    <strong>정적인 (static) 형태</strong>의 구조적 API 처럼 <strong>출력 모드(output mode)</strong>를 지정해야 한다.
  </li>
    <ul>
      <li>
        append: sink에 <strong>신규 레코드</strong>만 추가
      </li>
      <li>
        update: 변경 대상 <strong>레코드 자체를 갱신</strong>
      </li>
      <li>
        complete: <strong>전체 출력 내용</strong> 작성하기.
      </li>
    </ul>
  <li>
    특정 쿼리와 싱크는 일부 출력 모드만 지원한다.
  </li>
  <li>
    신규 데이터가 매번 유입되어 무한정 커지는 경우 complete는 적절하지 않다.
  </li>
  <li>
    한정된 key를 사용해 집계하는 경우 시간에 따라 일부 key값을 갱신해야하며 update 모드가 적합하다.
  </li>
</ul>

<br>

<h2>2-5. 트리거</h2>
<ul>
  <li>
    트리거는 <strong>데이터 출력 시점</strong>을 정의한다. 정확히는 구조적 스트리밍에서 <strong>언제 신규 데이터를 확인</strong>하고 <strong>결과를 갱신</strong>할지 결정한다.
  </li>
  <li>
    구조적 스트리밍은 기본적으로 <strong>마지막 입력 데이터를 처리한 직후</strong>에 신규 입력 데이터를 조회해 <strong>최단 시간 내</strong>에 새로운 처리 결과를 만든다.
  </li>
    <ul>
      <li>
        단, 파일 싱크를 사용하는 경우 작은 파일이 여러 개 생길 수 있다. 이 경우 Spark의 <strong>처리 시간 기반</strong>의 트리거를 활용할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>2-6. 이벤트 시간 처리</h2>
<ul>
  <li>
    구조적 스트리밍은 무작위로 도착한 <strong>레코드에 기록된 타임스템프</strong>를 기준으로 하는 <strong>이벤트 시간 기준</strong>의 처리도 지원한다.
  </li>
</ul>

<h3>2-6-1. 이벤트 시간 데이터</h3>
<h4>2-6-1-1. 이벤트 시간(event-time)</h4>
<ul>
  <li>
    이벤트 시간(event-time)은 <strong>데이터에 기록된 시간 필드</strong>를 의미한다.
  </li>
  <li>
    Spark는 데이터가 <strong>데이터 생성 시간</strong>을 기준으로 처리를 하기에 업로드 혹은 네트워크 지연으로 인한 <strong>시간 차이와 무관</strong>하게 처리할 수 있다.
  </li>
  <li>
    시스템은 입력 데이터를 테이블로 인식하기에 <strong>이벤트 시간</strong>은 테이블에 있는 <strong>하나의 컬럼</strong>일 뿐이다.
  </li>
    <ul>
      <li>
        따라서 <strong>표준 SQL 연산자</strong>를 활용하여 그룹화, 집계 그리고 윈도우 처리를 할 수 있다.
      </li>
      <li>
        구조적 스트리밍이 내부적으로 이벤트 시간 필드를 인식하면 쿼리 실행 최적화나 타임 윈도우에서 상태 정보 제거 시점 결정 등 <strong>특별한 작업을 수행</strong>할 수 있다.
      </li>
    </ul>
  <li>
    위오 같은 작업을 제어할 때 <strong>워터마크</strong>를 사용한다.
  </li>
</ul>

<h4>2-6-1-2. 워터마크</h4>
<ul>
  <li>
    <strong>워터마크(watermark)</strong>는 <strong>시간 제한을 설정</strong>할 수 있는 스트리밍 시스템으로 <strong>늦게 들어온 데이터</strong>를 어디까지 처리할지 시간을 제한할 수 있다.
  </li>
  <li>
    Watermark는 과거 <strong>데이터의 보관 주기를 제한</strong>하거나 특정 이벤트 시간의 <strong>윈도우 결과를 출력하는 시점을 제어</strong> 할 때에도 사용한다.
  </li>
  <li>
    스트리밍 DataFrame Spark 앱에서 <strong>스트리밍 DataFrame을 생성</strong>한 후 <strong>transformation을 수행</strong>해 <strong>적합한 포맷의 데이터</strong>를 얻는다.
  </li>
  <li>
    구조적 스트리밍에서 스키마 추론 기능을 사용하고 싶다면 명시적으로 설정해야 한다. 단, 모르는 사이 데이터가 바뀔 수 있기 때문에 <strong>운영 환경에서는 스키마 추론을 사용하면 안 된다</strong>.
  </li>
  <li>
    <strong>스트리밍 DataFrame의 생성과 실행</strong>은 다른 Spark API처럼 <strong>지연 처리 방식</strong>으로 동작한다.
  </li>
    <ul>
      <li>
        따라서 action을 호출하기 전에 스트리밍 <strong>DataFrame에 대한 transformation</strong>을 지정할 수 있다.
      </li>
    </ul>
  <li>
    Spark는 각 스트림에 <strong>UUID</strong>를 부여하여 필요한 경우 실행 중인 스트림 목록을 조회해 이전에 만들었던 스트림을 <strong>다시 사용</strong>할 수 있다.
  </li>
</ul>

<br><br>

<h1>3. 구조적 스트리밍 활용</h1>

```python
# 1. 실습을 위한 데이터 불러오기
static = spark.read.json("/workspace/Spark-The-Definitive-Guide/data/activity-data/")
dataSchema = static.schema

# 스키마 확인
static.printSchema()

# 세 개의 row만 확인
static.show(3, truncate=False)
```

```python
# 2. 앞서 설정한 스키마를 기반으로 트리거를 설정하여 스트리밍 데이터 생성.
#   - 스트리밍은 파일을 차례로 읽는 방식으로 만든다.
streaming = spark.readStream.schema(dataSchema)\
    # 매번 trigger당 최대 1개의 파일만 읽도록 설정 (시뮬레이션용)
    .option("maxFilesPerTrigger", 1)\
    # JSON 파일 소스 지정 (activity-data 디렉토리에서 읽기)
    .json("/workspace/Spark-The-Definitive-Guide/data/activity-data")
```

```python
# 3. action을 통해 스트리밍 DataFrame을 생성하기 전에 transformation 설정.
activityCounts = streaming.groupBy("gt").count()
```

```python
# 4. 로컬 모드로 돌아가기에 셔플 파티션 수를 조정.
spark.conf.set("spark.sql.shuffle.partitions", 5)
```

```python
# 5. 싱크를 지정하는 과정에서 Spark가 데이터를 출력하는 방식을 complete로 출력 모드로 지
#    정.
activityQuery = activityCounts.writeStream\
    # 쿼리 이름을 지정 (Spark UI 등에서 모니터링할 때 표시됨)
    .queryName("activity_counts")\
    # 결과를 메모리 테이블에 저장 (SQL로 조회 가능)
    .format("memory")\
    # 출력 모드: 전체 집계 결과를 매번 완전히 덮어씀
    .outputMode("complete")\
    # 스트리밍 쿼리를 시작 (백그라운드에서 실행됨)
    # 독립 실행 스크립트에서는 프로그램이 종료되므로 awaitTermination() 필요
    .start()
```

```python
# 6. 실행 중인 스트림 목록 확인.
spark.streams.active
```

```python
# 7. 저장된 메모리 테이블을 조회하여 결과를 확인.
from time import sleep

for x in range(5):
    spark.sql("SELECT * FROM activity_counts").show()
    sleep(1)
```

<br><br>

<h1>4. 스트림 트랜스포메이션</h1>
<ul>
  <li>
    스트리밍 transformation은 2부에서 설명한 <strong>정적 DataFrame의 transformation</strong>을 대부분 포함한다. 추가로 <strong>DataFrame의 모든 함수</strong>와 <strong>개별 컬럼 처리</strong>도 지원한다.
  </li>
  <li>
    스트리밍 데이터에 맞지 않는 transformation에 제약이 있는데 버전마다 업데이트 될 수 있기에 공식문서를 확인하는 것이 좋다.
  </li>
</ul>

<br>

<h2>4-1. 선택과 필터링</h2>
<ul>
  <li>
    구조적 스트리밍은 DataFrame의 모든 함수와 개별 컬럼을 처리하는 선택과 필터링 그리고 단순 transformation을 지원한다.
  </li>
</ul>

```python
# 1. 선택과 필터링을 적용하고 append 출력 모드 사용.
from pyspark.sql.functions import expr

simpleTransform = streaming.withColumn("stairs", expr("gt like '%stairs%'"))\
    .where("stairs")\
    .where("gt is not null")\
    .select("gt", "model", "arrival_time", "creation_time")\
    .writeStream\
    .queryName("simple_transform")\
    .format("memory")\
    .outputMode("append")\
    .start()

# 쿼리 상태 확인.
spark.streams.active

# 메모리 테이블 조회
spark.sql("SELECT * FROM simple_transform").show(5, truncate=False)
```

<br>

<h2>4-2. 집계</h2>

```python
# 1. 구조적 스트리밍에서 집계 연산 활용.
deviceModelStats = streaming.cube("gt", "model").avg()\
    .drop("avg(Arrival_time)")\
    .drop("avg(Creation_Time)")\
    .drop("avg(Index)")\
    .writeStream.queryName("device_counts").format("memory")\
    .outputMode("complete")\
    .start()

# 메모리 테이블에서 결과 확인
spark.sql("SELECT * FROM device_counts").show(5, truncate=False)
```

<br>

<h2>4-3. 조인</h2>

```python
# 1. 스티리밍 DataFrame과 정적 DataFrame 조인
#   - 직접 종료하지 않으면 무한히 스트리밍된다.
historicalAgg = static.groupBy("gt", "model").avg()
deviceModelStats = streaming.drop("Arrival_Time", "Creation_Time", "Index")\
    .cube("gt", "model").avg()\
    .join(historicalAgg, ["gt", "model"])\
    .writeStream.queryName("device_counts_v2").format("memory")\
    .outputMode("complete")\
    .start()

# 메모리 테이블의 결과 조회
spark.sql("SELECT * FROM device_counts_v2").show(5, truncate=False)
```

<br><br>

<h1>5. 입력과 출력</h1>
<ul>
  <li>
    최근에 추가된 소스와 싱크는 공식 문서를 통해 확인할 수 있다.
  </li>
</ul>

<br>

<h2>5-1. 데이터를 읽고 쓰는 장소(소스와 싱크)</h2>
<h3>5-1-1. 파일 소스와 싱크</h3>