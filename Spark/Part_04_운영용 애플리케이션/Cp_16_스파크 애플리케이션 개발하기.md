<h1>1. 스파크 애플리케이션 작성하기</h1>
<ul>
  <li>
    Spark 앱은 <strong>Spark 클러스터</strong>와 <strong>사용자 코드</strong> 두 가지 조합으로 구성된다.
  </li>
  <li>
    실습에서는 클러스터 모드를 <strong>로컬 모드</strong>로 설정하고 사전 정의된 앱을 사용자 코드로 사용한다.
  </li>
</ul>

<br>

<h2>1-1. 간단한 스칼라 기반 앱</h2>
<ul>
  <li>
    Scala는 Spark의 기본 언어이기에 앱을 개발하는데 가장 적합하다. (방법은 일반 scala 코드와 다르지 않다).
  </li>
  <li>
    Spark 앱은 두 가지 자바 가상 머신 기반의 빌드인 <strong>sbt</strong> 혹은 <strong>maven</strong>으로 빌드할 수 있다.
  </li>
  <li>
    책에서는 sbt를 사용하여 빌드하며 관련 링크를 통해 sbt에 대해 알아볼 수 있다.
  </li>
  <li>
    <strong>spark-submit 명령</strong>을 사용해 작성한 코드를 클러스터에 제출한다. 실행 명령 중 main 클래스를 지정한 부분을 눈여겨 보자.
  </li>
  <li>
    sbt assemble을 통해 빌드할 수 있지만 라이브러리 의존성 충돌이 발생할 수도 있다. 가장 쉬운 방법은 sbt package 명령어를 사용하는 것이다.
  </li>
    <ul>
      <li>
        관련 라이브러리 모두를 <strong>target 폴더</strong>에 모을 수는 있지만 관련 라이브러리들을 모두 갖고 있는 <strong>하나의 거대한 JAR</strong> 파일을 만들어내지는 않는다.
      </li>
    </ul>
</ul>

```scala
// 1. sbt를 통한 Spark 앱 빌드 (build.sbt 정의).
name := "example"
organization := "com.databricks"
version := "0.1-SNAPSHOT"
scalaVersion := "2.11.8"

// 스파크 관련 정보
val sparkVersion = "2.2.0"

// 스파크 패키지 포함
resolvers += "bintray-spark-packages" at
    "https://dl.bintray.com/spark-packages/maven/"

resolvers += "MavenRepository" at
    "http://mvnrepository.com/"

libraryDependencies ++= Seq(
  // 스파크 코어
  "org.apache.spark" %% "spark-core" % sparkVersion,
  "org.apache.spark" %% "spark-sql" % sparkVersion,

// 나머지 생략
)
```

```scala
// 2. 프로젝트 코드 작성.
//   - 스칼라와 자바 디렉터리에 소스 코드 작성.
src/
  main/
    resources/
      <JAR 파일에 포함할 파일들>
    scala/
      <메인 스칼라 소스 파일>
    java/
      <메인 자바 소스 파일>
  test/
    resources
      <테스트 JAR에 포함될 파일들>
    scala/
      <테스트용 스칼라 소스 파일>
    java/
      <테스트용 자바 소스 파일>
```

```scala 
// 3. 소스 코드 작성. 
//   - SparkSession을 초기화하고 앱 실행 후 종료.
object DataFrameExample extends Serializable {
  def main(args: Array[String]) = {

    val pathToDataFolder = args(0)

    // 명시적으로 몇 가지 설정값을 지정한 다음 SparkSession을 시작.
    val spark = SparkSession.builder().appName("Spark Example")
      .cocfig("spark.sql.warehouse.dir", "/user/hive/warehouse")
      .getOrCreate()

    // UDF를 등록한다.
    spark.udf.register("myUDF", someUDF(_:String):String)

    val df = spark.read.json(pathToDataFolder + "data.json")
    val manipulated = df.groupBy(expr("myUDF(group)")).sum().collect()
      .foreach(x => println(x))
  }
}
```

<h3>1-1-1. 애플리케이션 실행하기</h3>
<ul>
  <li>
    target 폴더에는 spark-submit에서 인수로 사용할 jar 파일이 들어있다.
  </li>
</ul>

```scala
// 1. 빌드 수행.
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.DataFrameExample \
  --master local \
  target/scala-2.11/example_2.11-0.1-SNAPSHOT.jar "hello"
```

<br>

<h2>1-2. 파이썬 애플리케이션 작성하기</h2>
<ul>
  <li>
    PySpark를 통해 앱을 작성하는 방법은 일반 파이썬 앱과 거의 유사하다.
  </li>
  <li>
    Spark에는 빌드 개념이 없으며 PySpark 앱은 <strong>파이썬 스크립트</strong>에 지나지 않는다. 따라서 앱을 실행하려면 클러스터에서 <strong>스크립트를 실행</strong>하면 된다.
  </li>
  <li>
    spark-submit의 <strong>--py-files 인수</strong>로 .py, .zip, .egg 파일을 지정하여 앱과 함께 배포하면 <strong>코드를 재사용</strong>할 수 있다.
  </li>
  <li>
    모든 파이썬 클래스에서 SparkSession 객체를 생성하는 것보다 <strong>런타임 환경</strong>에서 변수를 생성해 파이썬 클래스에 전달하는 방식이 더 좋다.
  </li>
  <li>
    파이썬으로 Spark 앱을 개발할 때에는 <strong>PySpark를 라이브러리 의존성으로 정의</strong>하기 위해 <strong>pip을 사용</strong>할 수 있다.
  </li>
  <li>
    PySpark는 <strong>pip install pyspark</strong> 명령으로 설치하며, 설치 후 다른 파이썬 패키지와 같은 방식을 의존성을 정의한다.
  </li>
</ul>

```python
# 1. python을 통해 SparkSession을 실행 가능한 스크립트 파일로 만든다.
from __future__ import print_function

if __name__ == '__main__':
    from pyspark.sql import SparkSession
    spark = SparkSession.builder \
        .master("local") \
        .appName("Word Count") \
        .config("spark.some.config.option", "some-value") \
        .getOrCreate()

print(spark.range(5000).where("id > 500").selectExpr("sum(id)").collect())
```

<h3>1-2-1. 애플리케이션 실행하기</h3>

```bash
# 1. spark-submit을 통해 코드를 클러스터에 전달한다.
$SPARK_HOME/bin/spark-submit --master local pyspark_template/main.py
```

<br>

<h2>1-3. 자바 애플리케이션 작성하기</h2>
<ul>
  <li>
    Java는 Scala와 Spark 앱을 작성하는 방법은 거의 유사하나 <strong>라이브러리 의존성</strong>을 지정하는 방식만 다르다.
  </li>
  <li>
    디렉터리 구조는 scala 프로젝트와 동일하다.
  </li>
</ul>

```xml
// 1. maven을 이용해 라이브러리 의존성 지정.
<dependencies>
  <dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.11</artifactId>
    <version>2.1.0</version>
  </dependency>
  <dependency>
    <groupId>graphframes</groupId>
    <artifactId>graphframes</artifactId>
    <version>0.4.0-spark2.1-s_2.11</version>
  </dependency>
</dependencies>
<repositories>
    <!-- 저장소 목록을 나열 -->
    <repository>
      <id>SparkPackagesRepo</id>
      <url>http://dl.bintray.com/spark-packages/maven</url>
    </repository>
</repositories>
```

```java
// 2. java 코드 실행을 위한 main 함수 작성.
import org.apache.spark.sql.SparkSession

public class SimpleExample {
  public static void main(String[] args) {
    SparkSession spark = SparkSession
        .builder()
        .getOrCreate();
    spark.range(1, 2000).count()
  }
}

// 이제 mvn package 명령으로 패키지를 만들 수 있다.
```

<h3>1-3-1. 애플리케이션 실행하기</h3>

```bash
$SPARK_HOME/bin/spark-submit \
  --class com.databricks.example.SimpleExample \
  --master local \
  target/spark-example-0.1-SNAPSHOT.jar "hello"
```

<br><br>

<h1>2. 스파크 애플리케이션 테스트</h1>
<h2>2-1. 전략적 원칙</h2>
<h3>2-1-1. 입력 데이터에 대한 유연성</h3>
<ul>
  <li>
    데이터 파이프라인은 <strong>다양한 유형의 입력 데이터</strong>에 유연하게 대처할 수 있어야 한다. (오류 상황을 적절히 제어할 수 있어야 함).
  </li>
  <li>
    따라서 입력 데이터로 인해 발생할 수 있는 <strong>다양한 예외 상황</strong>을 테스트하는 코드를 작성해야 한다.
  </li>
</ul>

<h3>2-1-2. 비즈니스 로직 변경에 대한 유연성</h3>
<ul>
  <li>
    파이프라인의 내부 <strong>비즈니스 로직</strong> 변경에도 대처할 수 있어야 한다. (실제와 유사한 데이터를 통해 로직을 꼼꼼히 테스트).
  </li>
  <li>
    해다 유형의 테스트에서는 Spark <strong>단위 테스트</strong>를 작성하지 않도록 조심해야 한다.
  </li>
  <li>
    <strong>비즈니스 로직을 테스트</strong>해 복잡한 비즈니스 파이프라인이 의도 대로 동작하는지 반드시 확인.
  </li>
</ul>

<h3>2-1-3. 결과의 유연성과 원자성</h3>
<ul>
  <li>
    입력 데이터와 비즈니스 로직 확인 절차까지 되었다면 결과 데이터가 스키마에 맞는 <strong>적절한 형태로 반환</strong>될 수 있도록 제어해야 한다.
  </li>
  <li>
    대부분의 Spark 파이프라인은 <strong>다른 Spark 파이프라인의 입력</strong>으로 사용된다. 따라서 <strong>데이터의 상태(갱신 주기, 데이터 완벽성, 데이터 변질 등)</strong>를 고려하여 파이프라인을 작성해야 한다.
  </li>
</ul>

<br>

<h2>2-2. 테스트 코드 작성 시 고려사항</h2>
<h3>2-2-1. SparkSession 관리하기</h3>
<ul>
  <li>
    <strong>Spark 로컬 모드</strong>를 사용하면 JUnit 혹은 ScalaTest 등의 단위 테스트용 프레임워크로 비교적 쉽게 Spark 코드를 테스트 할 수 있다.
  </li>
  <li>
    <strong>테스트 하네스(test haness)</strong>의 일부로 로컬 모드의 SparkSession을 만들어 사용하면 된다.
  </li>
  <li>
    SparkSession을 <strong>한 번만 초기화</strong>하고<strong> 런타임 환경</strong>에서 함수와 클래스에 전달하는 방식을 사용하면 테스트 중에 SparkSession을 <strong>쉽게 교체</strong>할 수 있다.
  </li>
    <ul>
      <li>
        이 방법으로 단위 테스트를 수행할 때 <strong>테스트용 SparkSession</strong>으로 개별 함수를 더욱 쉽게 테스트할 수 있다.
      </li>
    </ul>
</ul>

<h3>2-2-2. 테스트 코드용 스파크 API 선정하기</h3>
<ul>
  <li>
    API 유형과 상관없이 각 함수의 입력과 출력 타입을 <strong>문서</strong>로 만들고 테스트해야 한다.
  </li>
  <li>
    <strong>저수준 RDD API</strong>는 정적 데이터 타입을 사용하지만 Dataset API에는 없는 파티셔닝 같은 <strong>저수준 API의 기능이 필요할 때에만</strong> 사용한다.
  </li>
  <li>
    Dataset API를 사용하면 성능을 최적화할 수 있으며 앞으로도 많은 성능 최적화 방식을 제공할 가능성이 높다.
  </li>
  <li>
    API와 앱에 사용할 언어는 <strong>회사와 팀</strong>에 맞추어 결정해야 한다.
  </li>
</ul>

<br>

<h2>2-3. 단위 테스트 프레임워크에 연결하기</h2>
<ul>
  <li>
    코드를 단위 테스트하려면 각 언어의 <strong>표준 프레임워크(e.g. JUnit 혹은 ScalaTest)를 사용</strong>하고, 테스트 하네스에서 <strong>테스트마다</strong> SparkSession 생성하고 제거하도록 설정하는 것이 좋다.
  </li>
  <ul>
    각 프레임워크는 SparkSession 생성과 제거를 수행할 수 있는 메커니즘을 제공한다.
  </ul>
</ul>

<br>

<h2>2-4. 데이터소스 연결하기</h2>
<ul>
  <li>
    가능하면 테스트 코드로 운영 환경의 <strong>데이터소스</strong>에 접속하지 말아야한다. 이를 통해 데이터소스가 변경되더라도 <strong>독립된 환경</strong>에서 개발자가 쉽게 테스트가 가능하다.
  </li>
    <ul>
      <li>
        비즈니스 로직을 갖는 함수가 데이터소스에 직접 접근하지 않고 <strong>DataFrame</strong>이나 <strong>Dataset</strong>을 넘겨 받게 만든다.
      </li>
      <li>
        Spark의 <strong>구조적 API</strong>를 사용하는 경우 <strong>이름이 지정된 테이블(named table)</strong>을 이용해 문제를 해결할 수도 있다.
      </li>
        <ul>
          <li>
            작은 <strong>더미 데이터셋(텍스트 파일 혹은 인메모리 객체로 만든 데이터셋)</strong>에<strong>이름</strong>을 붙여 테이블로 등록하고 사용하는 것.
          </li>
        </ul>
    </ul>
</ul>

<br><br>

<h1>3. 개발 프로세스</h1>
<ul>
  <li>
    대화형 노트북이나 유사한 환경에 <strong>초기화된 작업 공간</strong> 마련<br>→ <strong>핵심 컴포넌트</strong>와 <strong>알고리즘</strong>을 만든다.<br>→ 만든 코드를 <strong>영구적인 영역</strong>으로 옮긴다.
  </li>
  <li>
    대부분의 shell은 대화형 앱을 개발할 때 사용하지만 <strong>spark-submit 명령</strong>은 Spark 클러스터에서 <strong>운영용 앱을 실행</strong>하기위해 사용한다.
  </li>
    <ul>
      <li>
        spark-shell(scala, java), PySpark, Spark SQL 그리고 SparkR이 속한다.
      </li>
    </ul>
</ul>

<br><br>

<h1>4. 애플리케이션 시작하기 (p401)</h1>
<ul>
  <li>
    Spark job을 제출할 때에는 클라이언트 모드와 클러스터 모드 중 하나를 선택하는데 지연 시간을 줄이기 위해 <strong>클러스터 모드를 권장</strong>한다.
  </li>
  <li>
    <strong>파이썬 앱</strong>을 제출할 경우 <strong>.jar 파일 위치</strong>에 .py 파일을 지정하고 파이썬 .zip, .egg, .py 파일을 --py-files에 추가한다.
  </li>
  <li>
    표에는 특정 클러스터 매니저에서 사용할 수 있는 옵션과 spark-submit 명령에 사용할 수 있는 모든 옵션이 있기에 필요에따라 참고하면 된다.
  </li>
    <ul>
      <li>
        spark-submit --help 명령을 실행하면 전체 옵션을 조회할 수 있다.
      </li>
    </ul>
</ul>

```bash
# 1. spark-submit 명령에 옵션과 앱 JAR 파일 또는 스크립트 관련 인수를 전달하여 사용.
./bin/spark-submit \
  --class <메인 클래스> \
  --master <스파크 마스터 URL> \
  --deploy-mode <배포 모드> \
  --conf <키>=<값> \
  ... # 다른 옵션
  <애플리케이션 JAR 또는 스크립트>
  [애플리케이션의 인수]
```

<br>

<h2>4-1. 애플리케이션 시작 예제</h2>
<ul>
  <li>

  </li>
</ul>

```bash
# 1. 특정 파라미터의 활용 방법을 모를 때에는 SparkPi 클래스를 메인 클래스로 사용해 테스트해
#    볼 수 있다.
./bin/spark-submit \
  --class org.apache.spark.examples.SparkPi \
  --master spark://207.184.161.138:7077 \
  --executor-memory 20G \
  --total-executor-cores 100 \
  # 로컬에서 사용하는 스칼라와 스파크 버전에 맞게 컴파일한 파일로 변경해야 할 수도 있다.
  replace/with/path/to/examples.jar \
  1000
```

```bash
# 2. 파이썬 버전
./bin/spark-submit \
  # master 옵션을 local이나 local[*]로 변경하면 앱을 로컬 모드로 실행할 수 있다.
  --master spark://207.184.161.138:7077 \
  examples/src/main/python/pi.py \
  1000
```

<br><br>

<h1>5. 애플리케이션 환경 설정하기</h1>
<ul>
  <li>
    환경 설정에 대해 대략적으로 알아본다. 대부분의 설정은 다음과 같이 분류된다.
  </li>
    <ul>
      <li>
        애플리케이션 속성
      </li>
      <li>
        런타임 환경
      </li>
      <li>
        셔플 동작 방식
      </li>
      <li>
        스파크 UI
      </li>
      <li>
        압축과 직렬화
      </li>
      <li>
        메모리 관리
      </li>
      <li>
        처리 방식
      </li>
      <li>
        네트워크 설정
      </li>
      <li>
        스케줄링
      </li>
      <li>
        동적 할당
      </li>
      <li>
        보안
      </li>
      <li>
        암호화
      </li>
      <li>
        스파크 SQL
      </li>
      <li>
        스파크 스트리밍
      </li>
      <li>
        SparkR
      </li>
    </ul>
  <li>
    Spark에서는 다음과 같은 방식으로 시스템을 설정할 수 있다.
  </li>
    <ul>
      <li>
        <strong>Spark 속성</strong>은 대부분의 <strong>앱 파라미터</strong>를 제어하며 <strong>SparkConf 객체</strong>를 사용해 Spark 속성을 설정할 수 있다.
      </li>
      <li>
        자바 시스템 속성
      </li>
      <li>
        하드코딩된 환경 설정 파일
      </li>
    </ul>
  <li>
    내려받은 Spark의 <strong>/conf 디렉터리</strong>에서 사용 가능한 여러 종류의 <strong>템플릿 파일</strong>을 찾아볼 수 있다.
  </li>
  <li>
    <strong>IP 주소</strong>와 같은 환경 변수는 Spark 클러스터 노드의 <strong>conf/spark-env.sh 스크립트</strong>에서 머신별로 지정할 수 있다.
  </li>
  <li>
    <strong>log4j.properties 파일</strong>을 사용해 <strong>로그</strong>와 관련된 설정을 할 수 있다.
  </li>
</ul>

<br>

<h2>5-1. SparkConf</h2>
<ul>
  <li>
    <strong>SparkConf</strong>는 앱의 <strong>모든 설정</strong>을 관리한다.
  </li>
  <li>
    <strong>SparkConf</strong> 객체는 Spark 앱에 대한 <strong>Spark 속성값을 구성</strong>하는 용도로 사용한다.
  </li>
    <ul>
      <li>
        Spark 속성값은 Spark 앱의 동작 방식과 클러스터 구성 방식을 제어한다.
      </li>
    </ul>
  <li>
    시간 주기 형태의 속성값은 다음 포맷을 사용한다.
  </li>
    <ul>
      <li>
        25ms (밀리세컨드)
      </li>
      <li>
        5s (초)
      </li>
      <li>
        10m 또는 10min (분)
      </li>
      <li>
        3h (시간)
      </li>
      <li>
        5d (일)
      </li>
      <li>
        1y (년)
      </li>
    </ul>
</ul>

```python
# 1. SparkConf를 import하여 객체를 생성.
#   - 생성된 객체는 불변성이다.
from pyspark import SparkConf

conf = SparkConf() \
          # 로컬 클러스터에 두 개의 스레드 생성
          .setMaster("local[2]") \
          # Spark UI에 표시한 앱 이름 지정
          .setAppName("DefinitiveGuide")\
          .set("some.conf", "to.save.value")
```

```bash
# 2. conf가 아닌 명령행으로 인수를 전달하여 런타임에 구성.
./bin/spark-submit --name "DefinitiveGuide" --master local[4] ...
```

<br>

<h2>5-2. 애플리케이션 속성 (p406)</h2>
<ul>
  <li>
    앱 속성은 <strong>spark-submit 명령</strong>이나 Spark 앱을 <strong>개발</strong>할 때 설정할 수 있다.
  </li>
  <li>
    앱의 속성은 기본 앱 <strong>메타데이터 일부</strong>를 실행 특성을 정의한다.
  </li>
  <li>
    드라이버가 실행된 호스트의 <strong>4040 포트</strong>로 접속하여 <strong>Spark UI의 Environment</strong> 탭을 통해 값이 올바르게 설정되었는지 확인할 수 있다.
  </li>
</ul>

<br>

<h2>5-3. 런타임 속성</h2>
<ul>
  <li>
    드물에 Spark 런타임 환경을 설정해야 할 수도 있다. 필요한 경우 <strong>Spark 공식 문서의 환경 설정 표</strong>에서 확인할 수 있다.
  </li>
  <li>
    관련 속성을 사용하여 드라이버와 익스큐터를 위한 추가 클래스와 파이썬패스, 파이썬 워커 설정 그리고 다양한 로그 관련 속성을 정의할 수 있다.
  </li>
</ul>

<br>

<h2>5-4. 실행 속성</h2>
<ul>
  <li>
    실행 속성과 관련된 설정값은 <strong>실제 처리를 더욱 세밀하게 제어</strong>할 수 있어 자주 사용된다.
  </li>
  <li>
    실행 속성 또한 <strong>Spark 공식 문서에서 처리 방식 표</strong>를 참고한다. 
  </li>
  <li>
    자주 사용하는 속성으로는 spark.executor.cores(익스큐터 코어 수)와 spark.files.maxPartitionBytes(파일 읽기 시 파티션의 최대 크기)가 있다.
  </li>
</ul>

<br>

<h2>5-5. 메모리 관리 설정</h2>
<ul>
  <li>
    최적화를 위해 메모리 옵션을 수동으로 관리해야 할 때가 있는데 대부분의 옵션은 <strong>메모리 자동 관리 기능</strong> 추가 이후 Spark 2.x부터 제거되었다.
  </li>
  <li>
    <strong>스파크 공식 문서에서 메모리 관리 표</strong>를 참고하면된다.
  </li>
</ul>

<br>

<h2>5-6. 셔플 동작방식 설명</h2>
<ul>
  <li>
    <strong>Shuffle</strong>이 <strong>병목 구간</strong>이 되어 <strong>네트워크 부하</strong>가 발생할 수 있음을 배웠다. 
  </li>
  <li>
    <strong>Spark 공식 문서의 shuffle 동작 방식 표</strong>를 참고.
  </li>
</ul>

<br>

<h2>5-7. 환경변수</h2>
<ul>
  <li>
    Spark가 설치된 디렉터리의 <strong>conf/spark-env.sh 파일 (윈도우에서는 conf/spark-env.cmd 파일)</strong>에서 읽은 <strong>환경변수</strong>로 특정 Spark 설정을 구성할 수 있다.
  </li>
    <ul>
      <li>
        스탠드얼론과 메소스 모드에서는 해당 파일로 <strong>머신에 특화된 정보(호스트명 등)</strong>를 제공할 수 있다.
      </li>
      <li>
        <strong>로컬 앱</strong>이나 <strong>제출용 스크립트</strong>를 실행할 때 <strong>함께 적용</strong>된다.
      </li>
    </ul>
  <li>
    Spark를 설치한다해서 conf/spark-env.sh 파일이 기본적으로 존재하는 것은 아니며 <strong>conf/spark-env.sh.template파일을 복사</strong>해 생성할 수 있다.
  </li>
  <li>
    spark-env.sh 스크립트의 예시 변수들
  </li>
    <ul>
      <li>
        <strong>JAVA_HOME</strong>
      </li>
        <ul>
          <li>
            Java가 설치된 경로를 지정. (기본 PATH에 자바 경로가 없는 경우).
          </li>
        </ul>
      <li>
        <strong>PYSPARK_PYTHON</strong>
      </li>
        <ul>
          <li>
            PySpark의 <strong>드라이버와 워커</strong> 모두에서 사용할 <strong>파이썬 바이너리 실행 파일을 지정</strong>. (python 버전 확인 필요).
          </li>
          <li>
            <strong>spark.pyspark.python 속성</strong>은 PYSPARK_PYTHON 보다 우선권을 갖는다.
          </li>
        </ul>
      <li>
        <strong>PYSPARK_DRIVER_PYTHON</strong>
      </li>
        <ul>
          <li>
            <strong>드라이버</strong>에서 PySpark를 사용하기 위해 가능한 <strong>파이썬 바이너리를 지정</strong>. (기본은 PYSPARK_PYTHON).
          </li>
          <li>
            <strong>spark.pyspark.driver.python 속성</strong>이 PYSPARK_DRIVER_PYTHON보다 우선권을 갖는다.
          </li>
        </ul>
      <li>
        <strong>SPARKR_DRIVER_R</strong>
      </li>
        <ul>
          <li>
            <strong>SparkR shell</strong>에서 사용할 R 바이너리 실행 명령을 지정. (기본값은 R).
          </li>
          <li>
            <strong>spark.r.shell.command 속성</strong>이 SPARKR_DRIVER_R 보다 우선권을 갖는다.
          </li>
        </ul>
      <li>
        <strong>SPARK_LOCAL_IP</strong>
      </li>
        <ul>
          <li>
            <strong>머신의 IP 주소</strong>를 지정한다.
          </li>
        </ul>
      <li>
        <strong>SPARK_PUBLIC_DNS</strong>
      </li>
        <ul>
          <li>
            Spark 프로그램이 다른 머신에 <strong>알려줄 호스트명</strong>이다.
          </li>
        </ul>
    </ul>
</ul>

<br>

<h2>5-8. 애플리케이션에서 잡 스케줄링</h2>
<ul>
  <li>
    Spark 애플리케이션에서 <strong>별도의 스레드</strong>를 사용해 <strong>여러 job</strong>을 <strong>동시에 실행</strong>할 수 있다.
  </li>
    <ul>
      <li>
        <strong>스레드</strong>: <strong>CPU</strong>가 실제로 명령어를 실행하는 <strong>최소 단위</strong>.
      </li>
    </ul>
  <li>
    이번 절에서 <strong>Job</strong>은 해당 action을 수행하기 위해 <strong>실행되어야 할 모든 task</strong>와 <strong>action</strong>을 의미한다.
  </li>
  <li>
    <strong>Spark의 스케줄러</strong>는 <strong>스레드의 안정성</strong>을 충분히 보장하며 여러 요청을 동시에 처리할 수 있는 앱을 만들 수 있다.
  </li>
  <li>
    기본적으로 Spark의 스케줄러는 <strong>FIFO 방식</strong>으로 동작한다. 
  </li>
    <ul>
      <li>
        Queue의 전단(head)에 있는 <strong>job이</strong> 클러스터의 <strong>전체 자원을 사용하지 않으면</strong> 이후 job을 <strong>바로 실행</strong>할 수 있다.
      </li>
      <li>
        Queue의 전단에 있는 <strong>job이 너무 크면 아주 늦게 실행</strong>된다.
      </li>
    </ul>
  <li>
    <strong>라운드 로빈(round-robin)</strong>을 사용하여 여러 Spark의 job이 자원을 <strong>공평하게 나눠 쓰도록</strong> 구성할 수도 있다.
  </li>
    <ul>
      <li>
        <strong>장시간 수행되는 spark job</strong>이 처리되는 도중 <strong>짧게 끝난 Spark job</strong>이 제출되면 즉시 장시간 Spark job의 자원을 할당받아 처리한다.<br>→ 장시간 수행되는 Spark job의 종료를 <strong>기다릴 필요</strong>가 없다.
      </li>
      <li>
        책에서는 왜 round-robin이라 한지 모르겠지만 <strong>선점형(Preemptive) 스케줄링</strong>에 더 가깝지 않나 생각한다.
      </li>
    </ul>
  <li>
    SparkContext를 설정할 때 <strong>spark.scheduler.mode 속성을 FAIR</strong>로 지정하여 <strong>페어 스케줄러(fair scheduler)</strong>를 사용할 수 있다.
  </li>
    <ul>
      <li>
        Fair scheduler는 여러 개의 잡을 <strong>pool로 그룹화</strong>하는 방식도 지원한다.
      </li>
      <li>
        개별 풀에는 <strong>다른 스케줄링 옵션</strong>이나 <strong>가중치</strong>를 설정할 수 있다. 따라서 <strong>더 중요한 Spark job</strong>을 설정하여 <strong>우선순위가 높은 풀</strong>을 만들 수 있다.
      </li>
      <li>
        각 사용자의 Spark job을 그룹화하여 <strong>모든 사용자가 같은 양의 자원</strong>을 사용하도록 설정할 수 있다.
      </li>
      <li>
        사용자가 명시적으로 pool을 지정하지 않으면 Spark는 새로운 잡을 <strong>default pool</strong>에 할당한다.
      </li>
      <li>
        Job을 제출하는 스레드에서 SparkContext의 로컬 속성인 <strong>spark.scheduler.pool 속성</strong>을 지정해 <strong>풀을 지정</strong>할 수 있다.
      </li>
    </ul>
</ul>

```bash
# 1. sc가 SparkContext의 변수라고 가정하면 다음과 같이 풀을 지정한다.
sc.setLocalProperty("spark.scheduler.pool", "pool1")

# 풀 지정 후에는 해당 스레드에서 제출되는 모든 잡은 이 풀을  사용한다.
# 사용자를 대신해 스레드가 여러 잡을 쉽게 실행할 수 있도록 스레드별로 지정 가능하다.
# 스레드에 연결된 풀을 조기화하고 싶다면 spark.scheduler.pool 속성 값을 null로 지정한다.
```

