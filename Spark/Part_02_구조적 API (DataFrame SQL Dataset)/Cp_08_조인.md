<ul>
  <li>
    Spark가 지원하는 join 타입, 실제 Spark가 클러스터에서 어떻게 join을 수행하는지 배운다.
  </li>
  <li>
    이번 장의 내용은 <strong>메모리 부족 상황을 회피</strong>하는 방법과 이전에 풀지 못했던 문제를 해결하는데 도움이 된다.
  </li>
</ul>

<br>

<h1>1. 조인 표현식</h1>
<ul>
  <li>
    Spark의 join 연산은 다음과 같이 진행된다.
  </li>
    <ul>
      <li>
        왼쪽과 오른쪽 데이터셋에 있는 <strong>하나 이상의 키값을 비</strong><br>→ 왼쪽 데이터셋과 오른쪽 데이터셋의 결합 여부를 결정하는 <strong>조인 표현식(join expression)평가</strong><br>→ 평가 결과에 따라 <strong>두 개의 데이터셋을 join</strong>
      </li>
      <li>
        가장 많이 사용하는 조인 표현식은 왼쪽과 오른쪽 데이터셋에 지정된 키가 <strong>동일한지 비교하는 동등 조인(equi-join)</strong>이다.
      </li>
    </ul>
</ul>

<br><br>

<h1>2. 조인 타입</h1>
<ul>
  <li>
    <strong>조인 표현식</strong>은 <strong>두 row의 조인 여부를 결정</strong>하는 반면, <strong>조인 타입</strong>은 결과 데이터셋에 <strong>어떤 데이터</strong>가 있어야 하는지 결정한다.
  </li>
  <li>
    조인 타입 종륲
  </li>
    <ul>
      <li>
        <strong>내부 조인(inner join)</strong>: 왼쪽과 오른쪽 데이터셋에 <strong>키가 있는 row</strong>를 유지.
      </li>
      <li>
        <strong>외부 조인(outer join)</strong>: <strong>왼쪽이나 오른쪽 데이터셋</strong>에 키가 있는 row를 유지
      </li>
      <li>
        <strong>왼쪽 외부 조인(left outer join)</strong>: <strong>왼쪽 데이터셋에 키가 있는 row</strong>를 유지
      </li>
      <li>
        <strong>오른쪽 외부 조인(right outer join)</strong>: <strong>오른쪽 데이터셋에 키가 있는 row</strong>를 유지
      </li>
      <li>
        <strong>왼쪽 세미 조인(left semi join)</strong>: 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 <strong>키가 일치하는 왼쪽 데이터셋</strong>만 유지.
      </li>
      <li>
        <strong>왼쪽 안티 조인(left anti join)</strong>: 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 <strong>키가 일치하지 않는 왼쪽 데이터셋</strong>만 유지
      </li>
      <li>
        <strong>자연 조인(natural join)</strong>: 두 데이터셋에서 <strong>동일한 이름을 갖는 컬럼</strong>을 <strong>암시적(implicit)으로 결합</strong>하는 조인을 수행.
      </li>
      <li>
        <strong>교차 조인(cross join)</strong> 혹은 <strong>카테시안 조인(Cartesian join)</strong>: 왼쪽 데이터셋의 <strong>모든 row</strong>와 오른쪽 데이터 셋의 모든 row를 조합.
      </li>
    </ul>
</ul>

```python
# 1. 실습을 위한 데이터셋 생성.
person = spark.createDataFrame([
    (0, "Bill Chambers", 0, [100]),
    (1, "Matei Zaharia", 1, [500, 250, 100]),
    (2, "Micael Armbrust", 1, [250, 100])])\
    .toDF("id", "name", "graduate_program", "spark_status")

graduateProgram = spark.createDataFrame([
    (0, "Masters", "School of Infrmation", "UC Berkeley"),
    (2, "Masters", "EECS", "UC Berkeley"),
    (1, "Ph.D.", "EECS", "UC Berkeley")])\
    .toDF("id", "degree", "department", "school")

sparkStatus = spark.createDataFrame([
    (500, "Vice President"),
    (250, "PMC Member"),
    (100, "Contributor")])\
    .toDF("id", "status")
```

```python
# 2. 생성한 데이터셋ㅇ르 이장 전체 예제에 사용하기 위해 테이블로 등록.
person.createOrReplaceTempView("person")
graduateProgram.createOrReplaceTempView("graduateProgram")
sparkStatus.createOrReplaceTempView("sparkStatus")
```

<br><br>

<h1>3. 내부 조인</h1>
<ul>
  <li>
    내부 조인은 DataFrame이나 테이블에 존재하는 키를 평가하고, <strong>참(true)으로 평가되는 row만 결합</strong>한다.
  </li>
</ul>

```python
# 1. graduateProgram과 person DataFrame을 join해 새로운 DataFrame 생성.
joinExpression = person["graduate_program"] == graduateProgram['id']
```

```python
# 2. 두 DataFrame 모두에 키가 존재하지 않으면 빈 DataFrame을 반환받는다.
wrongJoinExpression = person["name"] == graduateProgram["School"]
```

```python
# 3. 내부 조인 시행
person.join(graduateProgram, joinExpression).show()
```

```python
# 3. 조인 메서드의 세 번째 파라미터로 조인 타입을 명시하여 조인.
joinType = "inner"

person.join(graduateProgram, joinExpression, joinType).show()
```

<br><br>

<h1>4. 외부 조인</h1>
<ul>
  <li>
    외부 조인은 DataFrame이나 테이블에 존재하는 키를 평가하여 참(true) 혹은 <strong>거짓(false)로 평가한 row를 포함</strong>한다.
  </li>
  <li>
    왼쪽이나 오른쪽 DataFrame에 일치하는 row가 없다면 <strong>null을 삽입</strong>한다.
  </li>
</ul>

```python
# 1. 외부 조인 수행
joinType = "outer"

person.join(graduateProgram, joinExpression, joinType).show()
```

<br><br>

<h1>5. 왼쪽 외부 조인</h1>
<ul>
  <li>
    왼쪽 외부 조인은 키를 평가한 후 <strong>왼쪽 DataFrame의 모든 row</strong>와 왼쪽 DataFrame과 <strong>일치하는 오른쪽 DataFrame의 row를 포함</strong>한다.
  </li>
  <li>
    일차히를 row가 없다면 Spark는 해당 위치에 <strong>null을 삽입</strong>한다.
  </li>
</ul>

```python
# 1. 왼쪽 외부 조인 수행
joinType = "left_outer"

graduateProgram.join(person, joinExpression, joinType).show()
```

<br><br>

<h1>6. 오른쪽 외부 조인</h1>
<ul>
  <li>
    오른쪽 외부 조인은 키 평가 후 <strong>오른쪽 DataFrame의 모든 row</strong>와 오른쪽 DataFrame과 <strong>일치하는 왼쪽 DataFrame의 row를 반환</strong>한다.
  </li>
  <li>
    왼쪽 DataFrame에 일치하는 row가 없다면 Spark는 <strong>null을 삽입</strong>한다.
  </li>
</ul>

```python
# 1. 오른쪽 외부 조인 수행.
joinType = "right_outer"

person.join(graduateProgram, joinExpression, joinType).show()
```

<br><br>

<h1>7. 왼쪽 세미 조인</h1>
<ul>
  <li>
    세미 조인은 오른쪽 DataFrame의 어떤 값도 <strong>포함하지 않기에</strong> 목적이 다르다.
  </li>
  <li>
    두 번째 DataFrame은 <strong>값이 존재하는지 확인</strong>하기 위한 용도로만 사용된다.
  </li>
  <li>
    <strong>DataFrame 필터</strong>정도로 볼 수 있다.
  </li>
</ul>

```python
# 1. 왼쪽 세미 조인 실행.
joinType = "left_semi"

graduateProgram.join(person, joinExpression, joinType).show()

# 파이썬 코드
gradProgram2 = graduateProgram.union(spark.createDataFrame([
    (0, "Masters", "Duplicated Row", "Dulicated School")]))

gradProgram2.createOrReplaceTempView("gradProgram2")

gradProgram2.join(person, joinExpression, joinType).show()
```

<br><br>

<h1>8. 왼쪽 안티 조인</h1>
<ul>
  <li>
    왼쪽 안티 조인은 왼쪽 세미 조인과 반대 개념이다.
  </li>
  <li>
    기본적으로 왼쪽 세미 조인과 동일하지만 두 번째 DataFrame에서 <strong>관련된 키를 찾을 수 없는 row</strong>만 결과에 포함한다.
  </li>
  <li>
    SQL의 <strong>NOT IN</strong>과 같은 스타일의 필터로 볼 수 있다.
  </li>
</ul>

```python
# 1. 왼쪽 안티 조인 수행
joinType = "left_anti"
graduateProgram.join(person, joinExpression, joinType).show()
```

<br><Br>

<h1>9. 자연 조인</h1>
<ul>
  <li>
    자연 조인은 조인하려는 컬럼을 명시적으로 전달하지 않고 <strong>암시적으로 추정</strong>한다.
  </li>
  <li>
    왼쪽과 오른쪽 그리고 외부 자연 조인을 사용할 수 있다.
  </li>
  <li>
    암시적인 처리는 언제나 <strong>위험</strong>하다. 두 테이블이 같은 컬럼명을 갖더라도 각 DataFrame에서 <strong>내포하는 의미는 다를 수 있기 때문</strong>이다.
  </li>
</ul>

<br><br>

<h1>10. 교차 조인(카테지안 조인)</h1>
<ul>
  <li>
    교차 조인은 <strong>조건절을 기술하지 않은 내부 조인</strong>을 의미한다.
  </li>
  <li>
    <strong>왼쪽 DataFrame의 모든 row</strong>를 <strong>오른쪽 DataFrame의 모든 row</strong>와 결합한다.
  </li>
  <li>
    모든 row를 결합하기에 <strong>엄청난 수의 row를 갖는 DataFrame</strong> 생성될 수 있다. 따라서 반드시 <strong>keyword</strong>를 이용해 교차 조인을 수행해야 한다.
  </li>
  <li>
    사용자는 정말 <strong>필요한 경우</strong>에만 교차 조인을 사용하고, 또 <strong>명시적인 교차 조인</strong>을 정의해야 한다.
  </li>
    <ul>
      <li>
        <strong>spark.sql.crossJoin.enable 속성값을 true</strong>로 설정해 교차 조인 시 발생하는 <strong>경고 로그를 제거</strong>하거나 혹은 Spark가 교차 조인을 <strong>다른 조인 방식</strong>으로 처리하지 않도록 할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. 교차 조인 수행.
joinType = "cross"
graduateProgram.join(person, joinExpression, joinType).show()

# 명시적으로 메서드를 호출하는 방식
person.crossJoin(graduateProgram).show()
```

<br><br>

<h1>11. 조인 사용 시 문제점</h1>
<h2>11-1. 복합 데이터 타입의 조인</h2>
<ul>
  <li>
    복합 데이터를 처리한다는 점에서 어렵게 느껴질 수 있지만 <strong>불리언</strong>을 반환하는 모든 표현식은 조인 표현식으로 간주한다는 점만 기억하면 된다.
  </li>
</ul>

```python
from pyspark.sql.functions import expr

person.withColumnRenamed("id", "personId")\
.join(sparkStatus, expr("array_contains(spark_status, id)")).show()
```

<br>

<h2>11-2. 중복 컬럼명 처리</h2>
<ul>
  <li>
    DataFrame의 각 컬럼은 Spark SQL 엔진인 카탈리스트 내에 <strong>고유 ID</strong>가 있다. 하지만 고유 ID는 <strong>직접 참조가 불가</strong>하기 때문에 중복된 컬럼명을 처리할 필요가 있다.
  </li>
  <li>
    중복된 컬럼명이 문제를 일으키는 경우는 크게 두 가지 이다.
  </li>
    <ul>
      <li>
        Join에 사용할 DataFrame의 <strong>특정 키가 동일한 이름</strong>을 갖으며, <strong>key가 제거되지 않도록</strong> 조인 표현식에 명시하느 경우.
      </li>
      <li>
        <strong>조인 대상이 아닌 두 개의 컬럼</strong>이 동일한 이름을 갖는 경우.
      </li>
    </ul>
</ul>

```python
# 1. 실습을 진행하기 위한 잘못된 데이터셋 생성.
from pyspark.sql.functions import col

# id -> graduate_program 로 컬럼명 변경
gradProgramDupe = graduateProgram.withColumnRenamed("id", "graduate_program")

# 조인 표현식
joinExpr = gradProgramDupe["graduate_program"] == person["graduate_program"]
```

```python
# 2. graduate_program 컬럼을 키로하여 조인 수행.
person.join(gradProgramDupe, joinExpr).show()

# 조인을 수행했음에도 두 개의 graduate_program 컬럼이 존재.
```

```python
# 3. 중복된 두 개의 graduate_program 컬럼 중 하나를 참조할 때 문제 발생.
person.join(gradProgramDupe, joinExpr).select("graduate_program").show()

'''
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/pyspark/sql/dataframe.py", line 3229, in select
    jdf = self._jdf.select(self._jcols(*cols))
  File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/spark/python/pyspark/errors/exceptions/captured.py", line 185, in deco
    raise converted from None
pyspark.errors.exceptions.captured.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `graduate_program` is ambiguous, could be: [`graduate_program`, `graduate_program`].
'''
```

<h3>11-2-1. 해결 방법 1: 다른 조인 표현식 사용</h3>
<ul>
  <li>
    불리언 형태의 조인 표현식을 <strong>문자열</strong>이나 <strong>시퀀스 형태</strong>로 변환하면 조인을 할 때 두 컬럼 중 하나가 <strong>자동으로 제거</strong>된다.
  </li>
</ul>

```python
# 1. 표현식이 아닌 문자열로 조인 표현식을 전달.
person.join(gradProgramDupe, "graduate_program").select("graduate_program").show()
```

<h3>11-2-2. 해결 방법 2: 조인 후 컬럼 제거</h3>
<ul>
  <li>
    조인 시 동일한 키 이름을 사용하거나 원본 DataFrame에 동일한 컬럼명이 존재하는 경우 사용할 수 있다.
  </li>
  <li>
    원본 DataFrame을 사용해 컬럼명을 참조하여 제거한다.
  </li>
  <li>
    col 메서드를 사용하여 컬럼의 고유 ID로 해당 컬럼을 암시적으로 지정하고 이를 활용하기에 Spark 코드 분석 단계를 통과한다.
  </li>
</ul>

```python
# 2. 중복 컬럼 제거 방식으로 조인.
from pyspark.sql.functions import col

# 1) person 과 gradProgramDupe 조인
#    - 조인 키: graduate_program
#    - 조인 후 person 쪽 graduate_program 컬럼을 drop
#    - gradProgramDupe 쪽 graduate_program 컬럼만 남겨 select 후 show
person.join(
    gradProgramDupe,
    # joinExpr
    person["graduate_program"] == gradProgramDupe["graduate_program"], "inner")\
# person의 graduate_program 제거
.drop(person["graduate_program"])\
# gradProgramDupe의 graduate_program만 선택
.select("graduate_program")\
.show()

# 2) person 과 graduateProgram 조인
#    - 조인 키: person.graduate_program == graduateProgram.id
#    - 조인 후 graduateProgram 쪽 id 컬럼을 drop
#    - 전체 결과 show
person.join(
    graduateProgram,
    person["graduate_program"] == graduateProgram["id"], "inner")\
# graduateProgram의 id 컬럼 제거
.drop(graduateProgram["id"])\
.show()
```

<h3>11-2-3. 해결 방법 3: 조인 전 컬럼명 변경</h3>
<ul>
  <li>
    조인 전에 <strong>컬럼명을 변경</strong>하면 문제를 완전히 회피할 수 있다.
  </li>
</ul>

```python
# 1. 컬럼명을 변경하고 조인 수행.
from pyspark.sql.functions import col

# graduateProgram의 id 컬럼명을 grad_id로 변경
gradProgram3 = graduateProgram.withColumnRenamed("id", "grad_id")

# person.graduate_program == gradProgram3.grad_id 조건으로 조인 후 show
person.join(
    gradProgram3,
    person["graduate_program"] == gradProgram3["grad_id"], "inner")\
.show()
```

<br><br>

<h1>12. 스파크의 조인 수행 방식</h1>
<ul>
  <li>
    Spark가 조인을 수행하는 방식을 이해하기 위해서는 다음 두 가지 핵심을 이해해야 한다.
  </li>
    <ul>
      <li>
        노드간 네트워크 통신 전략
      </li>
      <li>
        노드별 연산 전략
      </li>
    </ul>
  <li>
    Spark 조인 수행 방식을 이해하면 <strong>빠르게 완료되는 작업</strong>과 <strong>절대 완료되지 않는 작업</strong> 간의 차이를 알 수 있다.
  </li>
</ul>

<br>

<h2>12-1. 네트워크 통신 전략</h2>
<ul>
  <li>
    Spark는 조인 시에 전체 노드 간 통신을 유발하는 <strong>셔플 조인(shuffle join)</strong>과 그렇지 않은 <strong>브로드캐스트 조인(broadcast join)</strong> 둘 중 하나를 활용한다.
  </li>
  <li>
    위와 같은 내부 최적화 기술은 시간이 지나 비용 기반 옵티마이저(cost-based optimizer, CBO)가 개선되고 더 나은 총신 전략이 도입되는 경우 바뀔 수 있다.
  </li>
</ul>

<h3>12-1-1. 큰 테이블과 큰 테이블 조인</h3>
<ul>
  <li>
    셔플 조인은 <strong>전체 노드 간 통신</strong> 발생<br>→ 조인에 사용한 <strong>특정 키나 키 집합을 갖는 노드</strong>와 데이터를 공유<br>→ 데이터가 잘 나눠져 있다면 <strong>더 많은 노드</strong>들이 참여<br>→ <strong>네트워크가 복잡</strong>해지고 <strong>많은 자원을 소모</strong>하게 된다.
  </li>
  <li>
    큰 테이블과 큰 테이블 간의 조인에서는 <strong>모든 워커 노드</strong>에서 통신이 발생한다.
  </li>
</ul>

<h3>12-1-2. 큰 테이블과 작은 테이블 조인</h3>
<ul>
  <li>
    테이블이 <strong>단일 워커 노드의 메모리 크기</strong>에 적합할 정도로 충분히 작은 경우 조인 연산을 <strong>최적화</strong>할 수 있다.
  </li>
  <li>
    브로드캐스트 조인은 <strong>작은 DataFrame</strong>을 클러스터의 <strong>전체 워커 노드에 복제</strong>하는 것을 의미한다.
  </li>
  <li>
    전체 노드로의 복제이기에 자원을 많이 낭비하는듯 보이지만 실제로는 조인 <strong>프로세스 내내 전체 노드가 통신하는 현상을 방지</strong>할 수 있다.
  </li>
    <ul>
      <li>
        시작 시 전체로 <strong>단 일 회 복제</strong>가 수행되고, 이후로는 <strong>개별 워커가 다른 워커 노드를 기다리는 일이 없다</strong>.
      </li>
      <li>
        즉, 일 회 대규모 노드 간 통신만 존재.
      </li>
      <li>
        각 Executor가 <strong>복제본</strong>을 활용해 <strong>로컬에서 직렬 연산</strong>을 하기 때문에 <strong>CPU 사용량이 병목 요인</strong>이 될 수 있다.”
      </li>
    </ul>
  <li>
    <strong>너무 큰 테이블</strong>을 브로드캐스트하면 <strong>고비용의 수집 연산</strong>이 발생하기에 <strong>드라이버 노드가 비정상적으로 종료</strong>될 수 있다.
  </li>
</ul>

```python
# 1. 브로드캐스트 조인 수행.
# (현재 버전에서는 SortMergeJoin이 수행된다)
from pyspark.sql.functions import col

join_expr = person["graduate_program"] == graduateProgram["id"]

person.join(graduateProgram, join_expr, "inner")\
    .explain(mode="formatted")
```

```python
# 2. 명시적으로 브로드캐스트 조인 수행
from pyspark.sql.functions import broadcast

person.join(broadcast(graduateProgram),\
    person["graduate_program"] == graduateProgram["id"],"inner")\
.explain(mode="formatted")
```

<h3>12-1-3. 아주 작은 테이블 사이의 조인</h3>
<ul>
  <li>
    <strong>아주 작은 테이블 사이의 조인</strong>을 할 때에는 <strong>Spark가 조인 방식을 결정</strong>하도록 하는 것이 좋다.
  </li>
</ul>