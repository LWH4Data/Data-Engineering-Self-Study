```bash
docker run --rm -it \
  -v /mnt/c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/opt/spark-data \
  apache/spark:3.5.2 \
  /opt/spark/bin/pyspark

```

<ul>
  <li>
    <strong>집계(aggregation)</strong>는 무언가를 <strong>모으는 행위</strong>이며 빅데이터의 초석이다.
  </li>
  <li>
    집계를 수행할 때에는 <strong>key</strong>와 <strong>group</strong>을 지정하고, <strong>하나 이상의 컬럼을 변환</strong>하는 방법을 지정하는 <strong>집계 함수</strong>를 사용한다.
  </li>
  <li>
    집계 함수는 여러 입력값이 주어지면 <strong>그룹별로 결과</strong>를 생성한다.
  </li>
  <li>
    Spark가 생성할 수 있는 그룹화 데이터 타입을 이번 장에서 배운다.
  </li>
  <li>
    지정된 집계 함수에 따라 그룹화된 결과는 <strong>RelationalGroupedDataset</strong>을 반환한다.
  </li>
  <li>
    중요한 점은 어떤 결과를 만들어야할지 <strong>정확히 파악</strong>해야 한다는 것이다.
  </li>
    <ul>
      <li>
        빅데이터를 사용한여 연산을 수행하는 작업은 연산, 네트워크, 저장소 등 <strong>많은 비용을 소모</strong>한다.
      </li>
    </ul>
  <li>
    수용 가능한 <strong>근사치 계산용 함수</strong>를 활용하면 Spark job의 실행과 속도를 개선할 수 있다.
  </li>
</ul>

```python
# 1. 실습 데이터 정의
#   - 구매 이력 데이터 활용
#   - partition을 훨씬 적은 수로 분할할 수 있도록 repartitioning (데이터 수 대비 
#     partition 수가 많을 수 있기 때문).
#   - 빠르게 접근할 수 있도록 캐싱
df = spark.read.format("csv")\
.option("header", "true")\
.option("inferSchema", "true")\
.load("/opt/spark-data/data/retail-data/all/*.csv")
.coalesce(5)

# 캐시 설정
df.cache()
df.createOrReplaceTempView("dfTable")
```

```python
# 2. count 메서드를 활용한 간단한 집계
#   - 캐시를 활용하면 count 메서드를 DataFrame의 캐싱 작업을 수행하는 용도로 활용할 수 있
#     다.
#   - count()는 액션으로 모든 row를 캐시에 올린다.
df.count()
```

<br><br>

<h1>1. 집계 함수</h1>
<ul>
  <li>
    모든 집계는 DataFrame의 .stat 속성을 이용하는 경우를 제외하면 <strong>함수</strong>를 사용한다.
  </li>
  <li>
    집계 함수는 <strong>org.apache.spark.sql.functions 패키지</strong>에서 찾아볼 수 있다.
  </li>
  <li>
    Scala와 python에서 임포트할 수 있는 함수와 SQL에서 사용 가능한 함수 사이에는 <strong>배포 버전</strong>마다 약간의 차이가 있다.
  </li>
</ul>

<br>

<h2>1-1. count</h2>
<ul>
  <li>
    이번 예제의 count 함수는 action이 아니라 <strong>transformation</strong>으로 동작한다.
  </li>
  <li>
    count 함수는 두 가지로 활용할 수 있다. 하나는 count 함수를 <strong>특정 컬럼</strong>에 지정하는 방식이고, 다른 하나는 <strong>count(*) 혹은 count(1)을 사용</strong>하는 방식이다.
  </li>
  <li>
    null 값이 포함되는 경우 <strong>count(*)</strong>는 <strong>null인 row를 포함</strong>하는 반면, <strong>컬럼을 지정</strong>하는 경우 <strong>null을 포함하지 않는다</strong>.
  </li>
</ul>

```python
# 1. count 함수 활용 예시
#   - select 문 내부에서 사용하기에 transformation으로 적용된다.
from pyspark.sql.functions import count

df.select(count("StockCode")).show()
```

<br>

<h2>1-2. countDistinct</h2>
<ul>
  <li>
    전체 record의 수가 아닌 <strong>고유 record의 수</strong>를 구할 때에는 <strong>countDistinct 함수</strong>를 사용한다.
  </li>
  <li>
    <strong>개별 컬럼</strong>을 처리하는 데 더 적합하다.
  </li>
</ul>

```python
# 1. countDistint 함수를 통한 고유 record 수 반환.
from pyspark.sql.functions import countDistinct

df.select(countDistinct("StockCode")).show()
```

<br>

<h2>1-3. approx_count_distinct</h2>
<ul>
  <li>
    정확한 고유 record 수가 아닌 <strong>근사치</strong>를 활용할 때에는 <strong>approx_count_distinct 함수</strong>를 사용한다.
  </li>
  <li>
    approx_count_disctinct 함수는 <strong>최대 추정 오류율(maximum estimation error)</strong>이라는 한 가지 파라미터를 사용한다. 최대 추정 오류율을 조정하여 <strong>속도와 정확도의 trade-off를 조정</strong>할 수 있다.
  </li>
</ul>

```python
# 1. approx_count_distinct를 통해 근사치 고유 record 수 반환.
from pyspark.sql.functions import approx_count_distinct

df.select(approx_count_distinct("StockCode", 0.1)).show()
```

<br>

<h2>1-4. first와 last</h2>
<ul>
  <li>
    <strong>first</strong>는 DataFrame의 <strong>첫 번째 값</strong>을, <strong>last</strong>는 <strong>마지막 값</strong>을 반환한다.
  </li>
  <li>
    first와 last 두 함수는 DataFrame이 아닌 <strong>row를 기반</strong>으로 동작한다.
  </li>
</ul>

```python
# 1. first와 last를 통해 DataFrame의 첫 행과 마지막 행 반환.
from pyspark.sql.functions import first, last

df.select(first("StockCode"), last("StockCode")).show()
```

<br>

<h2>1-5. min과 max</h2>
<ul>
  <li>
    <strong>min</strong>을 활용하여 DataFrame에서 <strong>최솟값</strong>을, <strong>max</strong>를 활용하여 <strong>최댓값</strong>을 추출할 수 있다.
  </li>
</ul>

```python
# 1. min과 max 함수를 통한 최소, 최대값 반환
from pyspark.sql.functions import min, max

df.select(min("Quantity"), max("Quantity")).show()
```

<br>

<h2>1-6. sum</h2>
<ul>
  <li>
    DataFrame에서 <strong>특정 컬럼의 모든 값을 합산</strong>할 때에는 <strong>sum 함수</strong>를 사용한다.
  </li>
</ul>

```python
# 1. sum을 통한 특정 컬럼 합산.
from pyspark.sql.functions import sum

df.select(sum("Quantity")).show()
```

<br>

<h2>1-7. sumDistinct</h2>
<ul>
  <li>
    <strong>sumDistinct 함수</strong>를 사용해 <strong>고윳값을 합산</strong>할 수 있다.
  </li>
</ul>

```python
# 1. sumDistinct 함수를 사용하여 고윳값의 합산을 출력.
from pyspark.sql.functions import sumDistinct

df.select(sumDistinct("Quantity")).show()
```

<br>

<h2>1-8. avg</h2>
<ul>
  <li>
    Spark의 <strong>avg 함수</strong> 혹은 <strong>mean 함수</strong>를 통해 <strong>평균값</strong>을 구할 수 있다.
  </li>
  <li>
    <strong>distinct 함수</strong>를 통해 <strong>고윳값의 평균</strong>을 구할 수도 있다. 또한 대부분의 집계 함수는 <strong>고윳값을 통해 집계</strong>를 수행하는 방식을 지원한다.
  </li>
</ul>

```python
# 1. avg와 mean을 통한 평균 계산.
from pyspark.sql.functions import sum, count, avg, expr

df.select(
    count("Quantity").alias("total_transactions"),
    sum("Quantity").alias("total_purchases"),
    avg("Quantity").alias("avg_purchases"),
    expr("mean(Quantity)").alias("mean_purchases"))\
    .selectExpr(

        # count와 sum을 통한 직접 평균 계산
        "total_purchases/total_transactions",

        # avg 함수를 통한 평균 계산
        "avg_purchases",

        # mean 함수를 통한 평균 계산
        "mean_purchases").show()
```

<br>

<h2>1-9. 분산과 표준편차</h2>
<ul>
  <li>
    Spark는 <strong>표본표준편차(sample standard deviation)</strong>과 <strong>모표준편차(population standard deviation)</strong> 둘 다 제공하기에 주의가 필요하다.
  </li>
  <li>
    <strong>var_samp</strong> 혹은 <strong>stddev_samp 함수</strong>를 통해 <strong>표본표준분산</strong>과 <strong>표본표준편차</strong>를 구할 수 있다.
  </li>
  <li>
    <strong>var_pop</strong> 혹은 <strong>stddev_pop</strong> 함수를 통해 <strong>모표준분산</strong>과 <strong>모표준편차</strong>를 구할 수 있다.
  </li>
</ul>

```python
# 1. var_sam, stddev_sam, var_pop, stddev_pop을 통해 표준표본분산, 표준포본편차, 모표
#    준분산, 모표준편차 계산.
from pyspark.sql.functions import var_pop, stddev_pop
from pyspark.sql.functions import var_samp, stddev_samp

df.select(var_pop("Quantity"), var_samp("Quantity"),
stddev_pop("Quantity"), stddev_samp("Quantity")).show()
```

<br>

<h2>1-10. 비대칭도와 첨도</h2>
<ul>
  <li>
    <strong>비대칭도(skewness)</strong>와 <strong>첨도(kurtosis)</strong> 모두 데이터의 <strong>변곡점(extreme point)을 측정</strong>하는 방법이다.
  </li>
  <li>
    <strong>비대칭도</strong>는 데이터 <strong>평균의 비대칭 정도</strong>를 측정하고, <strong>첨도</strong>는 <strong>데이터 끝 부분을 측정</strong>한다.
  </li>
  <li>
    비대충도와 첨도는 <strong>확률변수(random variable)의 확률분포(probability distribution)</strong>로 데이터를 모델링할 때 특히 중요하다.
  </li>
</ul>

```python
# 1. 함수를 사용해 비대칭도와 첨도 계산.
from pyspark.sql.functions import skewness, kurtosis

df.select(skewness("Quantity"), kurtosis("Quantity")).show()
```

<br>

<h2>1-11. 공분산과 상관관계</h2>
<ul>
  <li>
    <strong>cov</strong>와 <strong>corr 함수</strong>를 사용해 <strong>공분산(covariance)</strong>과 <strong>상관관계(correlation)</strong>를 계산할 수 있다.
  </li>
  <li>
    <strong>공분산</strong>은 데이터 <strong>입력값</strong>에 따라 <strong>다른 범위</strong>를 갖는다.
  </li>
  <li>
    상관관계는 <strong>피어슨 상관계수(Pearson correlation coefficient)</strong>를 측정하며 <strong>-1과 1사이의 값</strong>을 갖는다.
  </li>
  <li>
    <strong>covar_samp 함수</strong>와 <strong>covar_pop 함수</strong>를 활용해 <strong>표본공분산(sample covariance)</strong> 혹은 <strong>모공분산(population covariance)</strong>를 계산할 수도 있다.
  </li>
  <li>
    상관관계는 <strong>모집단이나 표본에 대한 계산 개념이 없다</strong>.
  </li>
</ul>

```python
# 1. covar_samp, covar_pop 그리고 corr을 통해 공분산과 상관관계 계산.
from pyspark.sql.functions import corr, covar_pop, covar_samp

df.select(corr("InvoiceNo", "Quantity"), covar_samp("InvoiceNo", "Quantity"), covar_pop("InvoiceNo", "Quantity")).show()
```

<br>

<h2>1-12. 복합 데이터 타입의 집계</h2>
<ul>
  <li>
    Spark는 특정 컬럼의 값을 <strong>list로 수집</strong>하거나 <strong>set 데이터 타입</strong>으로 고윳값만 수집하는 등 <strong>복합 데이터 타입을 집계에 활용</strong>할 수 있다.
  </li>
</ul>

```python
# 1. collect _set과 collect_list를 활용하여 복합 데이터 집계 실행.
from pyspark.sql.functions import collect_set, collect_list

df.agg(collect_set("Country"), collect_list("Country")).show()
```

<br><br>

<h1>2. 그룹화</h1>
<ul>
  <li>
    데이터 그룹 기반의 집계는 <strong>단일 컬럼 데이터를 그룹화</strong>하고, 해당 그룹의 <strong>다른 컬럼들을 사용하여 계산</strong>하기 위해 <strong>카테고리형 데이터(categorical data)</strong>를 활용한다.
  </li>
  <li>
    집계 연산은 <strong>하나 이상의 컬럼을 그룹화</strong>하는 단계와 <strong>집계 연산을 수행</strong>하는 두 단계로 이루어진다.
  </li>
    <ul>
      <li>
        첫 번째 단계에서는 <strong>RelationalGroupedDataset</strong>이 반환되고, 두 번째 단계에서는 <strong>DataFrame</strong>이 반환된다.
      </li>
    </ul>
  <li>
    그룹이 되는 컬럼은 <strong>여러 개</strong>를 지정할 수 있다.
  </li>
</ul>

```python
# 1. InvoiceNo를 기준으로 그룹을 만들고 CustomerId를 count한다.
df.groupby("InvoiceNo", "CustomerId").count().show()
```

<br>

<h2>2-1. 표현식을 이용한 그룹화</h2>
<ul>
  <li>
    메서드 대신 <strong>count 함수</strong>를 사용하는 것이 좋다.
  </li>
  <li>
    count 함수를 <strong>select 구문에 표현식</strong>으로 지정하는 경우 대신 <strong>agg 메서드</strong>를 사용하는 것이 좋다.
  </li>
    <ul>
      <li>
        agg 메서드는 <strong>여러 집계를 한 번에 지정</strong>할 수 있고, <strong>집계에 표현식</strong>을 사용할 수 있다.
      </li>
    </ul>
  <li>
    <strong>transformation이 완료된 컬럼</strong>에 <strong>alias 메서드</strong> 사용할 수 있다.
  </li>
</ul>

```python
# 1. 표현식을 활용한 그룹화
from pyspark.sql.functions import count

df.groupBy("InvoiceNo").agg(
    count("Quantity").alias("quan"),
    expr("count(Quantity)")).show()
```

<br>

<h2>2-2. 맵을 이용한 그룹화</h2>
<ul>
  <li>
    <strong>컬럼을 key</strong>로, <strong>수행할 집계 함수의 문자열을 value</strong>로 하는 <strong>map 타입</strong>을 사용해 <strong>transformation을 정의</strong>할 수 있다.
  </li>
  <li>
    수행할 집계 함수를 한 줄로 작성하면 <strong>여러 컬럼명을 재사용</strong>할 수 있다.
  </li>
</ul>

```python
# 1. map을 활용한 그룹화
df.groupBy("InvoiceNo").agg(expr("avg(Quantity)"), expr("stddev_pop(Quantity)"))\
.show()
```

<br><br>

<h1>3. 윈도우 함수</h1>
<ul>
  <li>
    <strong>윈도우 함수</strong>를 <strong>집계</strong>에 사용할 수 있다.
  </li>
  <li>
    윈도우 함수는 데이터의 <strong>특정 윈도우(window)</strong>를 대상으로 <strong>고유의 집계 연산을 수행</strong>한다.
  </li>
  <li>
    데이터의 윈도우는 현재 데이터에 대한 <strong>참조(reference)를 사용해 정의</strong>한다.
  </li>
  <li>
    <strong>윈도우 명세(window specification)</strong>는 함수에 전달될 <strong>row를 결정</strong>한다.
  </li>
  <li>
    <strong>group-by 함수</strong>는 모든 row record가 <strong>단일 그룹</strong>으로만 이동한다. 반면 <strong>window function</strong>은 <strong>frame에 입력되는 모든 row</strong>에 대해 결과를 계산한다.
  </li>
    <ul>
      <li>
        <strong>frame</strong>은 <strong>row 기반의 table</strong>을 의미한다.
      </li>
      <li>
        각 row는 <strong>하나 이상의 frame에 할당</strong>될 수 있다.
      </li>
    </ul>
  <li>
    가장 흔하게 사용되는 방법 중 하나는 <strong>하루</strong>를 나타내는 값의 <strong>롤링 평균(rolling average)</strong>를 구하는 것이다.
  </li>
    <ul>
      <li>
        작업을 완료하기 위해서는 <strong>개별 row</strong>가 <strong>7 개의 다른 frame</strong>으로 구성돼야 한다.
      </li>
    </ul>
  <li>
    Spark에서 제공하는 세 가지 <strong>window function</strong>
  </li>
    <ul>
      <li>
        <strong>랭크 함수(ranking function)</strong>
      </li>
      <li>
        <strong>분석 함수(analyric function)</strong>
      </li>
      <li>
        <strong>집계 함수(aggregate function)</strong>
      </li>
    </ul>
  <li>
    직관적으로 이야기하면 <strong>group-by</strong>의 경우 <strong>row가 1회</strong> 사용 가능하지만, <strong>window를 사용</strong>하면 <strong>여러번 사용이 가능</strong>하다.
  </li>
</ul>

```python
# 1. 주문 일자(InvoiceDate) 컬럼을 변환해 date 컬럼 생성.
from pyspark.sql.functions import col, to_date

dfWithDate = df.withColumn("date", to_date(col("InvoiceDate"), "MM/d/yyyy H:mm"))
dfWithDate.createOrReplaceTempView("dfWithDate")
```

```python
# 2. 윈도우 명세 작성.
from pyspark.sql.window import Window
from pyspark.sql.functions import desc

windowSpec = Window\

# 그룹을 어떻게 나눌지 결정하는 것과 유사
.partitionBy("CustomerId", "date")\

# partition의 정렬 방식을 정의
.orderBy(desc("Quantity"))\

# 프레임 명세: 입력된 row의 참조를 기반으로 frame에 row가 포함되 수 있는지 결정.
# 첫 row부터 현재 row까지 확인.
.rowsBetween(Window.unboundedPreceding, Window.currentRow)
```

```python
# 3. 2번에서 작성한 윈도우 명세를 활용하여 함수 활용.
#   - partitionBy로 CustomerId와 date 별로 그룹이 묶이기에 시간대별 계산이 가능하다.
from pyspark.sql.functions import max

# over를 통해 윈도우 명세도 함께 사용.
maxPurchaseQuantity = max(col("Quantity")).over(windowSpec)
```

```python
# 4. 집계 함수를 통해 구매량 순위 생성.
from pyspark.sql.functions import dense_rank, rank

# select 구문을 사용할 수 있는 컬럼 반환. 
purchaseDenseRank = dense_rank().over(windowSpec)
purchaseRank = rank().over(windowSpec)
```

```python
# 5. 4번의 결과와 select 구문을 활용하여 계산된 윈도우값 확인. 
from pyspark.sql.functions import col

dfWithDate.where("CustomerId IS NOT NULL").orderBy("CustomerId")\
.select(
  col("CustomerId"),
  col("date"),
  col("Quantity"),
  purchaseRank.alias("quantityRank"),
  purchaseDenseRank.alias("quantityDenseRank"),
  maxPurchaseQuantity.alias("maxPurchaseQuantity")).show()
```

<br><br>

<h1>4. 그룹화 셋</h1>
<ul>
  <li>
    <strong>여러 그룹</strong>에 걸쳐 집계를 진행할 때에는 <strong>그룹화 셋</strong>을 사용한다.
  </li>
  <li>
    그룹화 셋은 <strong>여러 집계를 결합</strong>하는 저수준 기능으로 <strong>group-by 구문</strong>에서 원하는 형태로 집계를 생성할 수 있다.
  </li>
  <li>
    그룹화 셋은 <strong>null 값</strong>에 따라 <strong>집계 수준</strong>이 달라진다. 따라서 <strong>null 값을 제거</strong>해 주어야 한다.
  </li>
  <li>
    그룹화 셋은 <strong>SQL</strong>의 <strong>GROUPING SETS</strong>를 통해서만 가능하다. <strong>DataFrame</strong>에서는 <strong>rollup 메서드</strong>와 <strong>cube 메서드</strong>를 활용한다. (뒤에서 배운다).
  </li>
</ul>

```python
# 1. 제고 코드(stockCode)와 고객(CustomerId)별 총 수량 집계를 위해 결측치를 제거한 테이
#    블 생성.
dfNoNull = dfWithDate.na.drop()

dfNoNull.createOrReplaceTempView("dfNoNull")

# 집계
from pyspark.sql.functions import sum as _sum

result = (dfNoNull
    .groupBy("CustomerId", "StockCode")
    .agg(_sum("Quantity").alias("totalQuantity"))
    .orderBy(["CustomerId", "StockCode"], ascending=[False, False])
)

result.show()
```

```python
# 2. 1번의 집계를 그룹화 셋을 활용한 방식으로 리팩토링
dfNoNull\
    .cube("CustomerId", "StockCode")\
    .agg(_sum("Quantity").alias("totalQuantity"))\
    .orderBy(["CustomerId", "StockCode"], ascending=[False, False])\
    .show()
```

```python
# 3. CustomerId-StockCode 별이 아닌 전체 합계를 GROUPING SETS로 집계 
#   - 그룹화 셋은 SQL에서만 가능하기에 리팩토링 X
SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull
GROUP BY customerId, stockCode, GROUPING SETS((customerId, stockCode), ())
ORDER BY sum(Quantity) DESC, CustomerId DESC, stockCode DESC
```

<br>

<h2>4-1. 롤업</h2>
<ul>
  <li>
    <strong>롤업</strong>은 group-by 스타일의 다양한 연산을 수행할 수 있는 <strong>다차원 집계 기능</strong>이다.
  </li>
</ul>

```python
# 1. 시간(신규 Date 컬럼)과 공간(Country 컬럼)을 축으로하는 롤업을 생성.
rolledUpDF = dfNoNull.rollup("Date", "Country").agg(sum("Quantity"))\
.selectExpr("Date", "Country", "`sum(Quantity)` as total_quantity")\
.orderBy("Date")
rolledUpDF.show()

# null 값을 갖는 row에서 전체 날짜의 합계를 확인할 수 있다.
# (두 개의 컬럼값이 모두 null인 row는 두 컬럼에 속한 record의 전체 합계를 나타낸다.)

```

```python
# 2. 두 컬럼의 값이 모두 null인 row를 통해 전체 합산 확인.
rolledUpDF.where("Country IS NULL").show()

rolledUpDF.where("Date IS NULL").show()
```

<br>

<h2>4-2. 큐브</h2>
<ul>
  <li>
    <strong>큐브(cube)</strong>는 <strong>롤업을 고차원적</strong>으로 사용할 수 있게 한다.
  </li>
  <li>
    cube는 요소들을 계층적으로 다루는 대신 <strong>모든 차원에 대해 동일한 작업</strong>을 수행한다. 예를 들면 하단의 작업들이 해당한다.
  </li>
    <ul>
      <li>
        전체 날짜와 모든 국가에 대한 합계
      </li>
      <li>
        모든 국가의 날짜별 합계
      </li>
      <li>
        날짜별 국가별 합계
      </li>
      <li>
        전체 날짜의 국가별 합계
      </li>
    </ul>
  <li>
    메서드 호출 방식은 rollup 메서드 대신 <strong>cube 메서드</strong>를 호출한다.
  </li>
  <li>
    쉽게 말해 cube는 계층적 구조가 아닌 <strong>모든 경우의 수</strong>에대한 집계를 반환한다.
  </li>
</ul>

```python
# 1. cube를 통한 집계
from pyspark.sql.functions import sum

dfNoNull.cube("Date", "Country").agg(sum(col("Quantity")))\
.select("Date", "Country", "sum(Quantity)").orderBy("Date").show()
```

<br>

<h2>4-3. 그룹화 메타데이터</h2>
<ul>
  <li>
    rollup과 cube를 사용할 때 집계 수준에 따라 쉽게 필터링을 하기위해서는 <strong>grouping_id</strong>를 활용한다.
  </li>
  <li>
    <strong>grouping_id</strong>는 결과 데이터셋의 <strong>집계 수준을 명시하는 컬럼</strong>을 제공한다.
  </li>
</ul>

```python
# 1. 네 개의 그룹화 ID 확인.
#   - 3: 가장 높은 계층의 집계 결과.
#   - 2: custoemrId와 무관한 stockCode 별 총 수량.
#   - 1: stockCode와 무관한 customerId 기반 총 수량.
#   - 0: customerId와 stockCOde 조합에 따른 총 수량.
from pyspark.sql.functions import grouping_id, sum as _sum, col

dfNoNull\
    .cube("CustomerId", "StockCode")\
    .agg(grouping_id().alias("grouping_id"), _sum("Quantity").alias("sum_Quantity"))\
    .orderBy(col("grouping_id").desc())\
    .show()
```

<br>

<h2>4-4. 피벗</h2>
<ul>
  <li>
    <strong>pivot</strong>을 사용해 <strong>row를 column으로 변환</strong>할 수 있다.
  </li>
  <li>
    특정 컬럼의 <strong>cardinality</strong>가 낮다면 schema와 query 대상을 확인할 수 있도록 pivot을 활용해 <strong>다수의 컬럼으로 변환</strong>하는 것이 좋다.
  </li>
</ul>

```python
# 1. pivot을 통해 그룹화한 Country를 컬럼으로 변환.
pivoted = dfWithDate.groupBy("date").pivot("Country").sum()

# 결과 조회
pivoted.where("date > '2011-12-05'").select("date", "`USA_sum(Quantity)`").show()
```

<br><br>

<h1>5. 사용자 정의 집계 함수</h1>
<ul>
  <li>
    책 출판 당시에는 Scala와 Java로만 UDAF가 가능했으나 최신 버전에서는 <strong>pandas UDF</strong>를 사용하여 정의할 수 있다.
  </li>
  <li>
    <strong>사용자 정의 집계 함수(user-defined aggregation function, UDAF)</strong>는 직접 제작한 함수나 비즈니스 규칙에 기반을 둔 <strong>자체 집계 함수를 정의</strong>하는 방법이다.
  </li>
  <li>
    Spark는 입력 받는 데이터의 모든 그룹의 <strong>중간 결과</strong>를 <strong>단일 AggregationBuffer</strong>에 저장해 관리한다.
  </li>
  <li>
    UDAF를 생성할 때에는 기본 클래스인 <strong>UserDefineAggregateFunction</strong>을 상속 받은 후 다음과 같은 메서드를 정의한다.
  </li>
    <ul>
      <li>
        <strong>inputSchema</strong>: UDAF <strong>입력 파라미터의 스키마</strong>를 StructType으로 정의
      </li>
      <li>
        <strong>bufferSchema</strong>: UDAF <strong>중간 결과의 스키마</strong>를 StructType으로 정의
      </li>
      <li>
        <strong>dataType</strong>: <strong>반환될 값</strong>의 DataType을 정의
      </li>
      <li>
        <strong>deterministic</strong>: UDAF가 <strong>동일한 입력값</strong>에 대해 항상 <strong>동일한 결과</strong>를 반환하는지 불리언값으로 정의
      </li>
      <li>
        <strong>initialize</strong>: 집계용 버퍼의 값을 <strong>초기화</strong>하는 로직을 정의
      </li>
      <li>
        <strong>update</strong>: 입력받은 row를 기반으로 <strong>내부 buffer를 업데이트</strong>하는 로직을 정의
      </li>
      <li>
        <strong>merge</strong>: 두 개의 집계용 buffer를 <strong>병합</strong>하는 로직을 정의
      </li>
      <li>
        <strong>evaluate</strong>: 집계의 <strong>최종 결과를 생성</strong>하는 로직을 정의.
      </li>
    </ul>
</ul>