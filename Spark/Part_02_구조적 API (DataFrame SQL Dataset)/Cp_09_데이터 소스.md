<ul>
  <li>
    Spark의 여섯 가지 핵심 데이터 소스와 커뮤니티에서 만든 일부 데이터 소스를 다룬다.
  </li>
  <li>
    여섯 가지 핵심 데이터 소스
  </li>
    <ul>
      <li>
        CSV
      </li>
      <li>
        JSON
      </li>
      <li>
        파케이
      </li>
      <li>
        ORC
      </li>
      <li>
        JDBC/ODBC 연결
      </li>
      <li>
        일반 텍스트 파일
      </li>
    </ul>
  <li>
    커뮤니티의 일부 데이터 소스
  </li>
    <ul>
      <li>
        카산드라
      </li>
      <li>
        HBase
      </li>
      <li>
        몽고디비
      </li>
      <li>
        AWS Redshift
      </li>
      <li>
        XML
      </li>
      <li>
        기타
      </li>
    </ul>
  <li>
    데이터소스를 이용해 데이터를 읽고 쓰는 방법과 서드파티 데이터소스와 Spark를 연동할 때 고려할 점을 다룬다.
  </li>
</ul>

<br><br>

<h1>1. 데이터소스 API의 구조</h1>
<h2>1-1. 읽기 API 구조</h2>

```plaintext
DataFrameReader.format(...).option("key", "value").schema(...).load()
```

<ul>
  <li>
    모든 데이터 소스를 읽을 때에는 위와 같은 형식을 사용한다.
  </li>
  <li>
    <strong>format 메서드</strong>는 <strong>선택적</strong>으로 사용할 수 있으며 <strong>기본값은 파케이 포맷</strong>이다.
  </li>
  <li>
    <strong>option 메서드</strong>를 사용해 <strong>데이터를 읽는 방법</strong>에 대한 파라미터를 <strong>key-value 쌍</strong>으로 설정할 수 있다.
  </li>
  <li>
    <strong>schema 메서드</strong>를 사용해 데이터소스에 <strong>schema를 제공</strong>하거나 <strong>schema 추론 기능</strong>을 사용하는 경우 <strong>선택적으로 사용</strong>할 수 있다.
  </li>
</ul>

<br>

<h2>1-2. 데이터 읽기의 기초</h2>
<ul>
  <li>
    Spark에서 데이터를 읽을 때에는 기본적으로 <strong>DataFrameReader</strong>를 사용하며 SparkSession의 <strong>read 속성(spark.read)</strong>으로 접근할 수 있다.
  </li>
  <li>
    DataFrameReader에는 <strong>포맷, 스키마, 읽기 모드, 옵션과 같은 값</strong>을 지정해야 한다.
  </li>
    <ul>
      <li>
        읽기 모드를 제외한 <strong>세 가지 항목(포맷, 스키마, 옵션)</strong>은 필요한 경우에만 <strong>선택적으로 지정</strong>할 수 있으며 <strong>transformation을 추가로 정의</strong>할 수 있는 <strong>DataFrameReader를 반환</strong>한다.
      </li>
    </ul>
  <li>
    사용자는 반드시 <strong>데이터를 읽을 경로</strong>를 지정해야 한다.
  </li>
</ul>

```scala
// 1. 전반적인 코드 구성
spark.read.format("csv")
.option("mode", "FAILFAST")
.option("inferSchema", "true")
.option("path", "path/to/file(s)")
.schema(someSchema)
.load()
```

<h3>1-2-1. 읽기 모드</h3>
<ul>
  <li>
    읽기 모드는 Spark가 <strong>형식에 맞지 않는 데이터</strong>를 만났을 때의 동작 방식을 지정하는 옵션이다. (기본값은 permissive 이다).
  </li>
    <ul>
      <li>
        <strong>permissive</strong>: 오류 record의 모든 필드를 <strong>null로 설정</strong>하고 모든 오류 레코드를 <strong>_corrup_record</strong>라는 문자열 컬럼에 기록한다.
      </li>
      <li>
        <strong>dropMalformed</strong>: 형식에 맞지 않는 레코드가 포함된 로우를 <strong>제거</strong>한다.
      </li>
      <li>
        <strong>failfast</strong>: 형식에 맞지 않는 레코드를 만나면 <strong>즉시 종료</strong>한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-3. 쓰기 API 구조</h2>

```plaintext
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
```

<ul>
  <li>
    모든 데이터소스에 데이터를 쓸 때에는 위와 같은 형식을 사용한다.
  </li>
  <li>
    <strong>format 메서드</strong>는 선택적으로 사용아 가능하며 <strong>기본값은 파케이 포맷</strong>이다.
  </li>
  <li>
    <strong>option 메서드</strong>를 사용하여 <strong>데이터 쓰기 방법</strong>을 설정할 수 있다.
  </li>
  <li>
    <strong>pertitionBy, buckerBy, sortBy 메서드</strong>는 <strong>파일 기반의 데이터소스</strong>에서만 동작한다. 최종 파일 <strong>배치 형태(layout)</strong>를 제어할 수 있다.
  </li>
</ul>

<br>

<h2>1-4. 데이터 쓰기의 기초</h2>
<ul>
  <li>
    데이터 쓰기는 <strong>DataFrameWriter를 사용</strong>한다.
  </li>
  <li>
    DataFrame의 <strong>write 속성</strong>을 이용해 <strong>DataFrame 별로 DataFrameWriter에 접근(dataFrame.write)</strong>해야 한다.
  </li>
</ul>

```scala
// 1. 데이터 쓰기의 일반적인 형식
dataframe.write.format("csv")
.option("mode", "OVERWRITE")
.option("dateFormat", "yyyy-MM-dd")
.option("path", "path/to/file(s)")
.save()
```

<h3>1-4-1. 저장 모드</h3>
<ul>
  <li>
    저장 모드는 Spark가 지정한 위치에 <strong>동일한 파일</strong>이 존재할 때의 동작 방식을 지정하는 옵션이다.
  </li>
    <ul>
        <li>
        <strong>append</strong>: 해당 경로에 이미 존해하는 파일 목록에 <strong>결과 파일을 추가</strong>한다.
        </li>
        <li>
        <strong>overwrite</strong>: 이미 존재하는 데이터를 <strong>완전히 덮어쓴다</strong>.
        </li>
        <li>
        <strong>errorIfExists</strong>: <strong>오류</strong>를 발생시키고 <strong>쓰기 작업을 실패</strong>시킨다.
        </li>
        <li>
        <strong>ignore</strong>: <strong>아무런 처리도하지 않는다</strong>.
        </li>
    </ul>
  <li>
    기본값은 errorIfExists 이다.
  </li>
</ul>

<br><br>

<h1>2. CSV 파일</h1>
<ul>
  <li>
    <strong>CSV(comma-separated values)</strong>는 <strong>콤마(,)</strong>로 구분된 값을 의미한다.
  </li>
  <li>
    CSV는 <strong>각 줄이 단일 record</strong>가 되고 record의 각 <strong>필드를 콤파로 구분</strong>하는 일반적인 텍스트 파일 포맷이다.
  </li>
  <li>
    CSV는 운영 환경에서 어떤 내용이 있는지, 어떤 구조로 되어 있는지 다양한 전제를 만들어 낼 수 없기에 <strong>많은 수의 옵션</strong>을 제공하며 <strong>다루기 어려운 형식</strong>에 속한다.
  </li>
</ul>

<br>

<h2>2-1. CSV 옵션 (p2150)</h2>
<ul>
  <li>
    CSV reader에서 사용할 수 있는 옵션들이 정리되어 있으며 필요할 때 참고하면 된다.
  </li>
</ul>

<br>

<h2>2-2. CSV 파일 읽기</h2>

```python
# 1. CSV용 DataFrameReader 생성.
spark.read.format("csv")
```

```python
# 2. 스키마 읽기 모드 지정
df = (spark.read.format("csv")\
    # 첫 줄을 header로 사용.
    .option("header", "true")\
    # 잘못된 레코드가 있으면 즉시 실패
    .option("mode", "FAILFAST")\
    # 데이터 타입 자동 추론
    .option("inferSchema", "true")\
    # CSV 파일 경로 (임시)
    .load("some/path/to/file.csv"))
df.show()
```

```python
from pyspark.sql.types import StructType, StructField, StringType, LongType

# 수동으로 스키마 정의
myManualSchema = StructType([
    StructField("DEST_COUNTRY_NAME", StringType(), True),
    StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
    StructField("count", LongType(), False)
])

# CSV 읽기 - FAILFAST 모드 (비정상 레코드 발견 시 즉시 실패)
df = (spark.read.format("csv")
    .option("header", "true")
    .option("mode", "FAILFAST")
    .schema(myManualSchema)
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
)
df.show(5)

# 잘 조회되기 때문에 문제가 없음을 알 수 있다.
```

```python
from pyspark.sql.types import StructType, StructField, LongType

# 3. 타입을 잘못 지정해서 오류 발생하도록 스키마 정의 
# (문자열 컬럼을 StringType → LongType으로 설정하여 오류 유도)
myManualSchema = StructType([
    StructField("DEST_COUNTRY_NAME", LongType(), True),
    StructField("ORIGIN_COUNTRY_NAME", LongType(), True),
    StructField("count", LongType(), False)
])

# FAILFAST 모드이므로 스키마 불일치 시 즉시 오류 발생
df = (spark.read.format("csv")
    .option("header", "true")
    .option("mode", "FAILFAST")
    .schema(myManualSchema)
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
)

df.take(5)

# 예상대로 오류가 발생하고 종료된다.
```

<br>

<h2>2-3. CSV 파일 쓰기</h2>
<ul>
  <li>
    maXColumns와 inferSchema 등 몇 가지를 제외하면 읽기와 동일한 옵션을 제공한다.
  </li>
  <li>
    쓰기는 <strong>명령 실행 시점의 DataFrame의 파이션 수</strong>를 반영한다. 따라서 <strong>결과도 파티션 수 만큼</strong> 내보내게 된다.
  </li>
</ul>

```python
# 1. CSV 파일 작성.
csvFile = spark.read.format("csv")\
    .option("header", "true")\
    .option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
```

```python
# 2. CSV 파일을 읽고 TSV 파일로 내보내기.
csvFile.write.format("csv").mode("overwrite").option("sep", "\t")\
    .save("/tmp/my-tsv-file.tsv")
```

<br><br>

<h1>3. JSON 파일</h1>
<ul>
  <li>
    자바스크립트에서 나온 파일 형식들은 <strong>자바스크립트 객체 표기법</strong>, 즉 JSON(JavaScript Object Notation)으로 더 친숙하게 알려져 있다.
  </li>
  <li>
    Spark는 JSON 파일을 사용할 때 <strong>줄로 구분된 JSON</strong>을 기본적으로 사용한다.
  </li>
  <li>
    <strong>multiLine 옵션</strong>을 통해 <strong>줄</strong>로 구분된 방식과 <strong>여러 줄</strong>로 구성된 방식을 선택적으로 사용할 수 있다. (<strong>true</strong>로 지정할 경우 전체 파일을 <strong>하나의 JSON 객체</strong>로 읽을 수 있다).
  </li>
  <li>
    Spark는 JSON 파일을 <strong>파싱</strong>하여 <strong>DataFrame을 생성</strong>한다.
  </li>
  <li>
    줄로 구분된 JSON은 전체 파일을 읽어 들인 다음 저장하는 방식이 아니기에 <strong>새로운 record를 추가</strong>할 수 있다.
  </li>
  <li>
    다른 포맷에 비해 훨씬 더 <strong>안정적인 포맷</strong>이다.
  </li>
  <li>
    <strong>구조화</strong>가 되어있고, <strong>최소한의 기본 데이터 타입이 존재</strong>한다는 장점이 있으며 <strong>Spark는 적합한 데이터 타입을 추정</strong>할 수 있어 원활하게 처리가 가능하다.
  </li>
  <li>
    JSON은 객체이기에 CSV보다 <strong>옵션 수가 적다</strong>.
  </li>
</ul>

<br>

<h2>3-1. JSON 옵션 (p255)</h2>
<ul>
  <li>
    JSON 객체를 다룰 때 사용할 수 있는 옵션들의 모음으로 필요할 때 참고하면 된다.
  </li>
  <li>
    줄로 구분된 JSON 파일을 읽는 방법은 <strong>데이터 포맷 설정</strong>과 <strong>옵션 지정 방식</strong>만 다르다.
  </li>
</ul>

```python
spark.read.format("json")
```

<br>

<h2>3-2. JSON 파일 읽기</h2>

```python
# 1. JSON 파일을 읽는 방법과 옵션 비교 예제
spark.read.format("json").option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("/opt/spark-data/data/flight-data/json/2010-summary.json").show(5)
```

<br>

<h2>3-3. JSON 파일 쓰기</h2>
<ul>
  <li>
    <strong>데이터소스와 관계없이</strong> JSON 파일에 저장할 수 있다.
  </li>
  <li>
    이전에 만들어 둔 CSV DataFrame을 <strong>JSON 파일의 소스로 재사용</strong>할 수 있다.
  </li>
  <li>
    <strong>파티션당 하나의 파일</strong>을 만들며 DataFrame을 <strong>단일 폴더</strong>에 저장한다.
  </li>
  <li>
    JSON 객체는 <strong>한 줄에 하나씩</strong> 기록된다.
  </li>
</ul>

```python
# 1. JSON 파일 쓰기
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
```

<br><br>

<h1>4. 파케이 파일</h1>
<ul>
  <li>
    파케이는 <strong>다양한 스토리지 최적화 기술</strong>을 제공하는 <strong>오픈소스 컬럼</strong> 기반의 데이터 저장 방식이며 <strong>분석 워크로드</strong>에 최적화 되어 있다.
  </li>
  <li>
    전체 파일을 읽는 대신 <strong>개별 컬럼</strong>을 읽을 수 있으며, <strong>컬럼 기반의 압축 기능</strong>을 제공한다.
  </li>
  <li>
    Apache Spark와 잘 호환되며 Spark의 <strong>기본 파일 포맷</strong>이다.
  </li>
  <li>
    파케이 파일은 읽기 연산 시 JSON이나 CSV보다 효울적이기에 <strong>저장용 데이터</strong>는 파케이 포맷으로 저장하는 것이 좋다.
  </li>
  <li>
    <strong>복합 데이터 타입</strong>을 지원하며 컬럼은 배열, 맵, 구조체 데이터 타입도 문제 없기 가능하다.
  </li>
  <li>
    단, CSV에서는 <strong>배열</strong>을 사용할 수 없다.
  </li>
</ul>

```python
# 1. 파케이 포맷 지정 방법.
spark.read.format("parquet")
```

<br>

<h2>4-1. 파케이 파일 읽기</h2>
<ul>
  <li>
    파케이는 데이터를 저장할 때 <strong>자체 스키마</strong>를 사용해 데이터를 저장하기 때문에 <strong>옵션이 거의 없으며</strong> 포맷을 지정하는 것만으로도 충분하다.
  </li>
  <li>
    Schema는 <strong>DataFrame을 표현</strong>하기 위해 정확한 schema가 필요한 경우에만 설정한다.
  </li>
    <ul>
      <li>
        단, <strong>읽는 시점</strong>에 스키마를 알 수 있기에 사실상 <strong>잘 사용하지 않는다</strong>.
      </li>
      <li>
        파케이 파일은 스키마가 <strong>파일 자체에 내장</strong>되어 있어 <strong>추정</strong> 또한 필요없다.
      </li>
    </ul>
</ul>

```python
# 1. 파케이 파일 읽기.
spark.read.format("parquet")

spark.read.format("parquet")\
    .load("/opt/spark-data/data/flight-data/parquet/2010-summary.parquet").show(5)
```

<h3>4-1-1. 파케이 옵션 (p259 참고)</h3>
<ul>
  <li>
    파케이는 옵션이 거의 없으며 <strong>단 두 개의 옵션</strong>이 존재한다.
  </li>
  <li>
    단 두 개의 옵션만 존재할 정도로 완벽하지만 호환되지 않는 <strong>파케이 파일을 다룰 때</strong> 문제가 발생할 수 있다. 따라서 <strong>Spark 버전</strong>을 신경써야 한다.
  </li>
  <li>
    두 가지 옵션은 <strong>compression(codec)</strong>와 <strong>mergeSchema</strong>이다.
  </li>
</ul>

<br>

<h2>4-2. 파케이 파일 쓰기</h2>
<ul>
  <li>
    파케이 파일을 쓸 때에는 <strong>파일의 경로</strong>만 명시하면 된다. <strong>분할 규칙</strong>은 <strong>다른 포맷과 동일하게 적용</strong>된다.
  </li>
</ul>

```python
# 1. 파케이 쓰기
csvFile.write.format("parquet").mode("overwrite")\
.save("/tmp/my-parquet-file.parquet")
```

<br><br>

<h1>5. ORC 파일</h1>
<ul>
  <li>
    <strong>ORC</strong>는 <strong>하둡 워크로드</strong>를 위해 설계된 자기 기술적(self-describin)이며 데이터 타입을 인식할 수 있는 컬럼 기반의 파잇 포맷이다.
  </li>
  <li>
    <strong>대규모 스트리밍 읽기</strong>에 최적화 되어 있으며 <strong>필요한 row를 신속하게 찾아낼</strong> 수 있는 기능들이 통합 되어 있다.
  </li>
  <li>
    Spark는 ORC 파일 포맷을 효율적으로 사용할 수 있으며 별도의 <strong>옵션 지정 없이</strong> 데이터를 읽을 수 있다.
  </li>
  <li>
    <strong>파케이</strong>는 <strong>Spark</strong>에 최적화된 반면 <strong>ORC</strong>는 <strong>하이브</strong>에 최적화 되어있다는 차이가 있다.
  </li>
</ul>

<br>

<h2>5-1. ORC 파일 읽기</h2>

```python
spark.read.format("orc").load("/opt/spark-data/data/flight-data/orc/2010-summary.orc").show(5)
```

<br>

<h2>5-2. ORC 파일 쓰기</h2>

```python
# 2. ORC 파일 쓰기 또한 지금까지의 다른 형식들과 동일하다.
csvFile.write.format("orc").mode("overwrite").save("/tmp/my-orc-file.orc")
```

<br><br>

<h1>6. SQL 데이터베이스</h1>
<ul>
  <li>
    데이터베이스는 원시 파일 형태가 아니기 때문에 <strong>고려해야할 옵션이 더 많다</strong>.
  </li>
    <ul>
      <li>
        예를 들면 데이터베이스 인증 정보 혹은 접속과 관련된 옵셔들과 네트워크 상태 등이 해당한다.
      </li>
    </ul>
  <li>
    책은 단일 머신에서 실습을 진행하기에 <strong>SQLite</strong>를 사용한다.
  </li>
    <ul>
      <li>
        SQLite는 파일에 데이터를 저장하는 데이터베이스로 공유된 <strong>실습 파일에 포함</strong>되어 있어 그냥 따라 진행하면 된다.
      </li>
      <li>
        실습은 SQLite를 활용하지만 PostgreSQL 등에서도 잘 작동한다.
      </li>
    </ul>
  <li>
    데이터베이스의 데이터를 읽고 쓰기 위해서는 <strong>Spark 클래스패스(classpath)</strong>에 데이터베이스의 <strong>JDBC(Java DataBase Connectivity) 드라이버를 추가</strong>하고, 적절한 JDBC 드라이버 <strong>jar 파일을 제공</strong>해야 한다.
  </li>
  <li>
    p262에는 JDBC 데이터베이스를 사용할 때 설정할 수 있는 모든 옵션 정보가 제공되어 있으며 필요할 때 읽으면 된다.
  </li>
</ul>

<br>

<h2>6-1. SQL 데이터베이스 읽기</h2>

```bash
# 0. 실습을 위한 컨테이너 띄우기
docker run --rm -it \
  -v /mnt/c/Users/SSAFY/Desktop/spark-prac/Spark-The-Definitive-Guide:/opt/spark-data \
  apache/spark:3.5.2 \
  /opt/spark/bin/pyspark \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.xerial:sqlite-jdbc:3.45.3.0
```

```python
# 1. 데이터베이스를 읽는 법도 다른 데이터소스와 동일하다. 옵션을 지정하고 데이터를 읽는다.
driver = "org.sqlite.JDBC"
path = "/opt/spark-data/data/flight-data/jdbc/my-sqlite.db"
url = "jdbc:sqlite:" + path
tablename = "flight_info"
```

```python
# 2. SQL 테이블을 읽어 DataFrame을 만들기.
dbDataFrame = spark.read.format("jdbc").option("url", url)\
.option("dbtable", tablename).option("driver", driver).load()
```

```python
# 3. PostgreSQL 설정 코드
pgDF = spark.read.format("jdbc")\
    .option("driver", "org.postgresql.Driver")\
    .option("url", "jdbc:postgresql://database_server")\
    .option("dbtable", "schema.tablename")\
    .option("user", "username").option("password", "my-secret-password").load()
```

```python
# 4. 생성한 dbDataFrame 조회
dbDataFrame.select("DEST_COUNTRY_NAME").distinct().show(5)
```

<br>

<h2>6-2. 쿼리 푸시다운</h2>
<ul>
  <li>
    Spark는 DataFrame을 생성하기 전에 <strong>데이터베이스 자체</strong>에서 데이터를 필터링하도록 만들 수 있다.
  </li>
  <li>
    Spark는 DataFrame에 필터를 명시하면 해당 필터에 대한 처리를 <strong>데이터베이스로 위임(push down)</strong>하는 등 특정 유형의 쿼리를 더 나은 방식으로 처리할 수 있다.
  </li>
  <li>
    Spark는 모든 Spark 함수를 SQL 데이터베이스에 맞게 변환하지는 못하기 때문에 때로는 <strong>전체 쿼리</strong>를 데이터베이스에 전달하여 결과를 DataFrame으로 받아야 하는 경우도 존재한다.
  </li>
</ul>

```python
# 1. 이전 dbDataFrame 조회 쿼리 실행계획을 확인하면 테이블의 여러 컬럼 중 관련 있는 컬럼만
#    선택한다.
dbDataFrame.select("DEST_COUNTRY_NAME").distinct().explain()
```

```python
# 2. 실행 계획의 PushedFilters를 확인.
dbDataFrame.filter("DEST_COUNTRY_NAME in ('Anguilla', 'Sweden')").explain()
```

```python
# 3. 전체 쿼리를 SQL로 전달.
#   - 테이블명 대신 SQL 쿼리를 명시한다.
#   - 괄호로 쿼리를 묶고 이름을 변경한다.
pushdownQuery = """(SELECT DISTINCT(DEST_COUNTRY_NAME) FROM flight_info) AS\
flight_info"""

# pushdownQuery에 명시한 쿼리를 통해 수행.
dbDataFrame = spark.read.format("jdbc")\
    .option("url", url).option("dbtable", pushdownQuery)\
    .option("driver", driver).load()

# 실행계획 확인
dbDataFrame.explain()
```

<h3>6-2-1. 데이터베이스 병렬로 읽기</h3>
<ul>
  <li>
    Spark는 파일 크기, 파일 유형 그리고 압축 방식에 따른 <strong>분할 가능성</strong>에 따라 <strong>여러 파일을 읽어 하나의 파티션</strong>으로 만들거나 <strong>여러 파티션을 하나의 파일</strong>로 만드는 기본 알고리즘을 갖고 있다.
  </li>
  <li>
    Spark의 파일처리가 갖는 유연성은 데이터베이스에도 존재하지만 <strong>수동 설정</strong>이 필요하다.
  </li>
  <li>
    명시적으로 <strong>조건절을 SQL 데이터베이스에 위임</strong>할 수 있다. 조건절을 명시함으로써 <strong>특정 파티션</strong>이 <strong>특정 데이터의 물리적 위치</strong>를 제어할 수 있다.
  </li>
</ul>

```python
# 1. numPartitions 옵션을 사용해 읽기 및 쓰기용 동시 작업 수를 제한할 수 있는 최대 파티션 
#    수 설정.
dbDataFrame = spark.read.format("jdbc")\
    .option("url", url).option("dbtable", tablename).option("driver", driver)\
    .option("numPartitions", 10).load()
```

```python
# 2. 데이터 조회
dbDataFrame.select("DEST_COUNTRY_NAME").distinct().show()
```

```python
# 3. 전체 데이터 중 Anguilla와 Sweden 두 국가의 데이터만 필터링.
#   - 필터링을 데이터베이스에 위임해 처리된 결과를 반환 받는다.
#     → Spark 자체 파티션에 결과 데이터를 저장하여 더 많은 처리를 할 수 있다.
#     → 데이터소스 생성 시 조건절 목록을 정의해 Spark 자체 파티션에 결과 데이터를 저장.
props = {"driver":"org.sqlite.JDBC"}
predicates = [
    "DEST_COUNTRY_NAME = 'Sweden' OR ORIGIN_COUNTRY_NAME = 'Sweden'",
    "DEST_COUNTRY_NAME = 'Anguilla' OR ORIGIN_COUNTRY_NAME = 'Anguilla'"]
spark.read.jdbc(url, tablename, predicates=predicates, properties=props).show()
spark.read.jdbc(url, tablename, predicates=predicates, properties=props)\
    .rdd.getNumPartitions()
```

```python
# 4. 연관성이 없는 조건절을 정의하면 중복 row가 많이 발생한다.
props = {"driver":"org.sqlite.JDBC"}
predicates = [
    "DEST_COUNTRY_NAME != 'Sweden' OR ORIGIN_COUNTRY_NAME != 'Sweden'",
    "DEST_COUNTRY_NAME != 'Anguilla' OR ORIGIN_COUNTRY_NAME != 'Anguilla'"]
spark.read.jdbc(url, tablename, predicates=predicates, properties=props).count()
```

<h3>6-2-2. 슬라이딩 윈도우 기반의 파티셔닝</h3>
<ul>
  <li>
    조건절을 기준으로 분할할 수 있는 방법.
  </li>
  <li>
    수치형 count 컬럼을 기준으로 분할<br>→ 처음과 마지막 파티션 사이의 최솟값과 최댓값 활용.<br>→ 범위 밖의 모든 값은 첫 번째 또는 마지막 파티션에 속한다.<br>→ 전체 파티션 수 설정. (병렬 처리 수준을 의미).<br>→ Spark는 데이터베이스에 병렬로 쿼리를 요청하고 numPartitions에 설정된 값만큼 파티션을 반환 받는다.<br>→ 파티션에 값을 할당하기 위해 상한값과 하한값 수정.<br>→ 필터링은 발생하지 않으며 최젓값과 최곳값까지 동일 분배.
  </li>
</ul>

```python
# 1. 실습을 수행하기 위한 변수들 정의.
colName = "count"
lowerBound = 0
upperBound = 348113
numPartitions = 10
```

```python
# 2. 실습 수행.
spark.read.jdbc(url, tablename, column=colName, properties=props,
                lowerBound=lowerBound, upperBound=upperBound,
                numPartitions=numPartitions).count()
```

<br>

<h2>6-3. 데이터베이스 쓰기</h2>
<ul>
  <li>
    <strong>URI를 지정</strong>하고 지정한 <strong>쓰기 모드</strong>에 따라 데이터를 쓴다.
  </li>
</ul>

```python
# 1. 데이터베이스 쓰기
newPath = "jdbc:sqlite://tmp/my-sqlite.db"
csvFile.write.jdbc(newPath, tablename, mode="overwrite", properties=props)

# 실행 결과 확인
spark.read.jdbc(newPath, tablename, preperties=props).count()
```

```python
# 2. 새로운 테이블에 데이터 추가.
csvFile.write.jdbc(newPath, tablename, mode="append", properties=props)

# record 수 증가 확인.
spark.read.jdbc(newPath, tablename, properties=props).count()
```

<br><br>

<h1>7. 텍스트 파일</h1>
<ul>
  <li>
    텍스트 파일의 <strong>각 줄</strong>은 <strong>DataFrame의 record</strong>가 된다. 따라서 변환하는 것도 마음대로 가능하다.
  </li>
  <li>
    텍스트 파일은 기본 데이터 타입의 유연성을 활용하기에 <strong>Dataset API</strong>에서 사용하기 매우 좋은 포맷이다.
  </li>
</ul>

<br>

<h2>7-1. 텍스트 파일 읽기</h2>
<ul>
  <li>
    텍스트 파일을 읽는 것은 <strong>textFile 메서드</strong>를 활용한다.
  </li>
  <li>
    textFile 메서드는 파티션 수행 결과로 만들어진 <strong>디렉터리명을 무시</strong>한다.<br>→ 따라서 <strong>파티션된 텍스트 파일</strong>을 읽거나 쓰려면 읽기 및 쓰기 시 파티션 수행 결과로 만들어진 디렉터리를 인식할 수 있도록 <strong>text 메서드</strong>를 사용해야 한다.
  </li>
</ul>

```python
# 1. textFile 메서드를 통해 텍스트 파일 읽기.
# (pyspark로 리팩토링)
from pyspark.sql.functions import split, col

spark.read.text("/opt/spark-data/data/flight-data/csv/2010-summary.csv")\
     .select(split(col("value"), ",").alias("rows"))\
     .show()
```

<br>

<h2>7-2. 텍스트 파일 쓰기</h2>
<ul>
  <li>
    텍스트 파일을 쓸 때에는 <strong>문자열 컬럼</strong>이 <strong>하나</strong>만 존재해야 한다.
  </li>
  <li>
    텍스트 파일을 저장할 때 <strong>파티셔닝 작업</strong>을 수행하면 <strong>더 많은 컬럼</strong>을 저장할 수 있다. 단, 모든 파일에 컬럼을 추가하는 것이 아닌 <strong>텍스트 파일이 저장되는 디렉터리</strong>에 <strong>폴더별</strong>로 컬럼을 저장한다.
  </li>
</ul>

```python
# 1. 텍스트 파일 쓰기.
csvFile.select("DEST_COUNTRY_NAME").write.text("/tmp/simple-text-file.txt")
```

```python
# 2. 파티셔닝 작업을 통해 텍스트 파일 쓰기.
csvFile.limit(10).select("DEST_COUNTRY_NAME", "count")\
    .write.partitionBy("count").text("/tmp/five-csv-files2py.csv")
```

<br><br>

<h1>8. 고급 I/O 개념</h1>
<ul>
  <li>
    쓰기 작업 전에 <strong>파티션 수를 조절</strong>함으로써 <strong>병렬로 처리할 파일 수</strong>를 제어할 수 있다.
  </li>
  <li>
    <strong>버켓팅</strong>과 <strong>파티셔닝</strong>을 조절함으로써 데이터의 저장 구조를 제어할 수 있다.
  </li>
</ul>

<br>

<h2>8-1. 분할 가능한 파일 타입과 압축 방식</h2>
<ul>
  <li>
    Spark는 <strong>분할</strong>을 통해 <strong>쿼리에 필요한 부분</strong>만 읽을 수 있어 성능 향상에 도움이 된다.
  </li>
  <li>
    <strong>HDFS</strong>와 같은 시스템을 사용하면 <strong>분할된 파일</strong>을 <strong>여러 블록으로 나누어 분산 저장</strong>하기 때문에 최적화가 가능하다.
  </li>
  <li>
    <strong>압축 방식</strong>도 관리가 필요하다. 모든 압출 방식이 분할 압축을 지원하지는 않기 때문이다. 추천하는 <strong>파일 포맷</strong>과 <strong>압축 방식</strong>은 <strong>파케이 파일 포맷</strong>과 <strong>GZIP 압축 방식</strong>이다.
  </li>
</ul>

<br>

<h2>8-2. 병렬로 데이터 읽기</h2>
<ul>
  <li>
    여러 익스큐터가 같은 파일을 동시에 읽을 수는 없지만 <strong>여러 파일을 동시에</strong> 읽을 수 있다.
  </li>
  <li>
    다수의 파일이 존재하는 폴더를 읽을 때 <strong>폴더의 개별 파일</strong>은 <strong>DataFrame의 파티션</strong>이 된다.<br>→ 사용 가능한 <strong>익스큐터</strong>를 이용해 <strong>병렬</strong>로 파일을 읽는다.
  </li>
    <ul>
      <li>
        만약 <strong>'익스큐터 수 < 파일의 수'</strong>라면 처리 중인 파일이 완료 될 때까지 <strong>대기</strong>한다.
      </li>
    </ul>
</ul>

<br>

<h2>8-3. 병렬로 데이터 쓰기</h2>
<ul>
  <li>
    <strong>파일이나 데이터 수</strong>는 데이터를 쓰는 시점에 <strong>DataFrame이 갖는 파티션 수</strong>에 따라 달라질 수 있다.
  </li>
</ul>

```python
# 1. 폴더 안에 5 개의 파일을 생성.
# repartition을 통해 5 개의 파일을 생성하도록 설정한다.
csvFile.repartition(5).write.format("csv").save("/tmp/multiple.csv")
```

<br>

<h3>8-3-1. 파티셔닝</h3>
<ul>
  <li>
    <strong>Partitioning</strong>은 <strong>어떤 데이터를 어디에 저장</strong>할 것인지 제어할 수 있는 기능이다.
  </li>
  <li>
    Partitioning된 디렉터리 또는 테이블에 파일을 쓸 때 <strong>디렉터리별</strong>로 <strong>컬럼 데이터를 인코딩</strong>해 저장한다.<br>→ 데이터를 읽을 때 전체 데이터셋을 스캔하지 않고 <strong>필요한 컬럼</strong>만 읽을 수 있다.
  </li>
  <li>
    Partitioning 기능은 <strong>모든 파일 기반의 데이터소스</strong>에서 지원한다.
  </li>
  <li>
    Partitioning은 <strong>필터링</strong>을 자주 사용하는 테이블을 갖는 경우 사용할 수 있는 <strong>가장 손쉬운 최적화 방식</strong>이다.
  </li>
    <ul>
      <li>
        예를 들어 <strong>지난주</strong> 데이터만을 참고한다면 <strong>날짜 기준</strong>으로 파티션을 생성할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. 파티셔닝을 통해 데이터 저장 설정.
#   - DEST_COUNTRY_NAME 별로 파티셔닝 했기에 국가별로 디렉터리가 생성된다.
csvFile.limit(10).write.mode("overwrite").partitionBy("DEST_COUNTRY_NAME")\
    .save("/tmp/partitioned.files.parquet")
```

<h3>8-3-2. 버켓팅</h3>
<ul>
  <li>
    <strong>버케팅(bucketing)</strong>은 각 파일에 저장된 데이터를 제어할 수 있는 또 다른 <strong>파일 조직화 기법</strong>이다.
  </li>
  <li>
    <strong>동일한 bucket ID</strong>를 갖는 데이터가 <strong>하나의 물리적 파티션</strong>에 모두 모여 있기 때문에 <strong>suffle을 피할 수 있다</strong>.
  </li>
    <ul>
      <li>
        데이터가 이후 사용 방식에 맞춰 <strong>사전에 파티셔닝</strong>되어 있기에 조인이나 집계 시 발생 <strong>비용이 최저</strong>가 된다.
      </li>
    </ul>
  <li>
    특정 컬럼을 파티셔닝하면 <strong>수억 개의 디렉터리</strong>가 생성되는 경우 bucketing할 수 있는 방법을 찾아야 한다.
  </li>
  <li>
    Bucketing은 <strong>Spark 관리 테이블</strong>에서만 사용할 수 있다. 자세한 내용은 <strong>스파크 서밋 2017의 발표 내용</strong> 참고.
  </li>
</ul>

```python
# 1. Bucket 단위로 데이터를 모아 일정 수의 파일로 저장하는 예
numberBuckets = 10
columnToBucketBy = "count"

csvFile.write\
    .format("parquet")\
    .mode("overwrite")\
    .bucketBy(numberBuckets, columnToBucketBy)\
    .saveAsTable("bucketFiles")
```

<br>

<h2>8-4. 복합 데이터 유형 쓰기</h2>
<ul>
  <li>
    Spark는 자체 데이터 타입을 제공하지만 모든 데이터 파일 포맷에 적합한 것은 아니기에 적절하게 고려할 필요가 있다.
  </li>
</ul>

<br>

<h2>8-5. 파일 크기 관리</h2>
<ul>
  <li>
    파일 크기는 데이터를 저장할 때는 중요한 요소가 아니지만 데이터를 <strong>읽을 때</strong>에는 중요한 요소이다.
  </li>
  <li>
    작은 파일 문제
  </li>
    <ul>
      <li>
        작은 파일을 많이 생성하면 <strong>메타데이터</strong>에 부하가 발생한다.
      </li>
      <li>
        HDFS와 같은 파일 시스템은 작은 크기의 파일을 잘 다루지 못한다.
      </li>
    </ul>
  <li>
    반면 너무 큰 파일도 좋지 못하기에 적절한 크기로 관리하는 것이 좋다.
  </li>
  <li>
    Spark 2.2 버전(도서 출판)에는 <strong>자동으로 파일 크기를 최적</strong>으로 제한 해준다.
  </li>
    <ul>
      <li>
        기능을 사용하기 위해서는 <strong>maxRecordsPerFile 옵션</strong>에 <strong>파일당 레코드 수</strong>를 지정해야 한다.
      </li>
      <li>
        각 파일에 기록될 레코드 수를 조절할 수 있기에 파일 크기를 더 효과적으로 관리할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. 파일당 최대 5,000 개의 row를 포함하도록 보장하여 설정.
df.write.option("maxRecordsFile", 5000)
```