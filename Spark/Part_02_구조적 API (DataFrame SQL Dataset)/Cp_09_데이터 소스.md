<ul>
  <li>
    Spark의 여섯 가지 핵심 데이터 소스와 커뮤니티에서 만든 일부 데이터 소스를 다룬다.
  </li>
  <li>
    여섯 가지 핵심 데이터 소스
  </li>
    <ul>
      <li>
        CSV
      </li>
      <li>
        JSON
      </li>
      <li>
        파케이
      </li>
      <li>
        ORC
      </li>
      <li>
        JDBC/ODBC 연결
      </li>
      <li>
        일반 텍스트 파일
      </li>
    </ul>
  <li>
    커뮤니티의 일부 데이터 소스
  </li>
    <ul>
      <li>
        카산드라
      </li>
      <li>
        HBase
      </li>
      <li>
        몽고디비
      </li>
      <li>
        AWS Redshift
      </li>
      <li>
        XML
      </li>
      <li>
        기타
      </li>
    </ul>
  <li>
    데이터소스를 이용해 데이터를 읽고 쓰는 방법과 서드파티 데이터소스와 Spark를 연동할 때 고려할 점을 다룬다.
  </li>
</ul>

<br><br>

<h1>1. 데이터소스 API의 구조</h1>
<h2>1-1. 읽기 API 구조</h2>

```plaintext
DataFrameReader.format(...).option("key", "value").schema(...).load()
```

<ul>
  <li>
    모든 데이터 소스를 읽을 때에는 위와 같은 형식을 사용한다.
  </li>
  <li>
    <strong>format 메서드</strong>는 <strong>선택적</strong>으로 사용할 수 있으며 <strong>기본값은 파케이 포맷</strong>이다.
  </li>
  <li>
    <strong>option 메서드</strong>를 사용해 <strong>데이터를 읽는 방법</strong>에 대한 파라미터를 <strong>key-value 쌍</strong>으로 설정할 수 있다.
  </li>
  <li>
    <strong>schema 메서드</strong>를 사용해 데이터소스에 <strong>schema를 제공</strong>하거나 <strong>schema 추론 기능</strong>을 사용하는 경우 <strong>선택적으로 사용</strong>할 수 있다.
  </li>
</ul>

<br>

<h2>1-2. 데이터 읽기의 기초</h2>
<ul>
  <li>
    Spark에서 데이터를 읽을 때에는 기본적으로 <strong>DataFrameReader</strong>를 사용하며 SparkSession의 <strong>read 속성(spark.read)</strong>으로 접근할 수 있다.
  </li>
  <li>
    DataFrameReader에는 <strong>포맷, 스키마, 읽기 모드, 옵션과 같은 값</strong>을 지정해야 한다.
  </li>
    <ul>
      <li>
        읽기 모드를 제외한 <strong>세 가지 항목(포맷, 스키마, 옵션)</strong>은 필요한 경우에만 <strong>선택적으로 지정</strong>할 수 있으며 <strong>transformation을 추가로 정의</strong>할 수 있는 <strong>DataFrameReader를 반환</strong>한다.
      </li>
    </ul>
  <li>
    사용자는 반드시 <strong>데이터를 읽을 경로</strong>를 지정해야 한다.
  </li>
</ul>

```scala
// 1. 전반적인 코드 구성
spark.read.format("csv")
.option("mode", "FAILFAST")
.option("inferSchema", "true")
.option("path", "path/to/file(s)")
.schema(someSchema)
.load()
```

<h3>1-2-1. 읽기 모드</h3>
<ul>
  <li>
    읽기 모드는 Spark가 <strong>형식에 맞지 않는 데이터</strong>를 만났을 때의 동작 방식을 지정하는 옵션이다. (기본값은 permissive 이다).
  </li>
    <ul>
      <li>
        <strong>permissive</strong>: 오류 record의 모든 필드를 <strong>null로 설정</strong>하고 모든 오류 레코드를 <strong>_corrup_record</strong>라는 문자열 컬럼에 기록한다.
      </li>
      <li>
        <strong>dropMalformed</strong>: 형식에 맞지 않는 레코드가 포함된 로우를 <strong>제거</strong>한다.
      </li>
      <li>
        <strong>failfast</strong>: 형식에 맞지 않는 레코드를 만나면 <strong>즉시 종료</strong>한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-3. 쓰기 API 구조</h2>

```plaintext
DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()
```

<ul>
  <li>
    모든 데이터소스에 데이터를 쓸 때에는 위와 같은 형식을 사용한다.
  </li>
  <li>
    <strong>format 메서드</strong>는 선택적으로 사용아 가능하며 <strong>기본값은 파케이 포맷</strong>이다.
  </li>
  <li>
    <strong>option 메서드</strong>를 사용하여 <strong>데이터 쓰기 방법</strong>을 설정할 수 있다.
  </li>
  <li>
    <strong>pertitionBy, buckerBy, sortBy 메서드</strong>는 <strong>파일 기반의 데이터소스</strong>에서만 동작한다. 최종 파일 <strong>배치 형태(layout)</strong>를 제어할 수 있다.
  </li>
</ul>

<br>

<h2>1-4. 데이터 쓰기의 기초</h2>
<ul>
  <li>
    데이터 쓰기는 <strong>DataFrameWriter를 사용</strong>한다.
  </li>
  <li>
    DataFrame의 <strong>write 속성</strong>을 이용해 <strong>DataFrame 별로 DataFrameWriter에 접근(dataFrame.write)</strong>해야 한다.
  </li>
</ul>

```scala
// 1. 데이터 쓰기의 일반적인 형식
dataframe.write.format("csv")
.option("mode", "OVERWRITE")
.option("dateFormat", "yyyy-MM-dd")
.option("path", "path/to/file(s)")
.save()
```

<h3>1-4-1. 저장 모드</h3>
<ul>
  <li>
    저장 모드는 Spark가 지정한 위치에 <strong>동일한 파일</strong>이 존재할 때의 동작 방식을 지정하는 옵션이다.
  </li>
    <ul>
        <li>
        <strong>append</strong>: 해당 경로에 이미 존해하는 파일 목록에 <strong>결과 파일을 추가</strong>한다.
        </li>
        <li>
        <strong>overwrite</strong>: 이미 존재하는 데이터를 <strong>완전히 덮어쓴다</strong>.
        </li>
        <li>
        <strong>errorIfExists</strong>: <strong>오류</strong>를 발생시키고 <strong>쓰기 작업을 실패</strong>시킨다.
        </li>
        <li>
        <strong>ignore</strong>: <strong>아무런 처리도하지 않는다</strong>.
        </li>
    </ul>
  <li>
    기본값은 errorIfExists 이다.
  </li>
</ul>

<br><br>

<h1>2. CSV 파일</h1>
<ul>
  <li>
    <strong>CSV(comma-separated values)</strong>는 <strong>콤마(,)</strong>로 구분된 값을 의미한다.
  </li>
  <li>
    CSV는 <strong>각 줄이 단일 record</strong>가 되고 record의 각 <strong>필드를 콤파로 구분</strong>하는 일반적인 텍스트 파일 포맷이다.
  </li>
  <li>
    CSV는 운영 환경에서 어떤 내용이 있는지, 어떤 구조로 되어 있는지 다양한 전제를 만들어 낼 수 없기에 <strong>많은 수의 옵션</strong>을 제공하며 <strong>다루기 어려운 형식</strong>에 속한다.
  </li>
</ul>

<br>

<h2>2-1. CSV 옵션 (p2150)</h2>
<ul>
  <li>
    CSV reader에서 사용할 수 있는 옵션들이 정리되어 있으며 필요할 때 참고하면 된다.
  </li>
</ul>

<br>

<h2>2-2. CSV 파일 읽기</h2>

```python
# 1. CSV용 DataFrameReader 생성.
spark.read.format("csv")
```

```python
# 2. 스키마 읽기 모드 지정
df = (spark.read.format("csv")\
    # 첫 줄을 header로 사용.
    .option("header", "true")\
    # 잘못된 레코드가 있으면 즉시 실패
    .option("mode", "FAILFAST")\
    # 데이터 타입 자동 추론
    .option("inferSchema", "true")\
    # CSV 파일 경로 (임시)
    .load("some/path/to/file.csv"))
df.show()
```

```python
from pyspark.sql.types import StructType, StructField, StringType, LongType

# 수동으로 스키마 정의
myManualSchema = StructType([
    StructField("DEST_COUNTRY_NAME", StringType(), True),
    StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
    StructField("count", LongType(), False)
])

# CSV 읽기 - FAILFAST 모드 (비정상 레코드 발견 시 즉시 실패)
df = (spark.read.format("csv")
    .option("header", "true")
    .option("mode", "FAILFAST")
    .schema(myManualSchema)
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
)
df.show(5)

# 잘 조회되기 때문에 문제가 없음을 알 수 있다.
```

```python
from pyspark.sql.types import StructType, StructField, LongType

# 3. 타입을 잘못 지정해서 오류 발생하도록 스키마 정의 
# (문자열 컬럼을 StringType → LongType으로 설정하여 오류 유도)
myManualSchema = StructType([
    StructField("DEST_COUNTRY_NAME", LongType(), True),
    StructField("ORIGIN_COUNTRY_NAME", LongType(), True),
    StructField("count", LongType(), False)
])

# FAILFAST 모드이므로 스키마 불일치 시 즉시 오류 발생
df = (spark.read.format("csv")
    .option("header", "true")
    .option("mode", "FAILFAST")
    .schema(myManualSchema)
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
)

df.take(5)

# 예상대로 오류가 발생하고 종료된다.
```

<br>

<h2>2-3. CSV 파일 쓰기</h2>
<ul>
  <li>
    maXColumns와 inferSchema 등 몇 가지를 제외하면 읽기와 동일한 옵션을 제공한다.
  </li>
  <li>
    쓰기는 <strong>명령 실행 시점의 DataFrame의 파이션 수</strong>를 반영한다. 따라서 <strong>결과도 파티션 수 만큼</strong> 내보내게 된다.
  </li>
</ul>

```python
# 1. CSV 파일 작성.
csvFile = spark.read.format("csv")\
    .option("header", "true")\
    .option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("/opt/spark-data/data/flight-data/csv/2010-summary.csv")
```

```python
# 2. CSV 파일을 읽고 TSV 파일로 내보내기.
csvFile.write.format("csv").mode("overwrite").option("sep", "\t")\
    .save("/tmp/my-tsv-file.tsv")
```

<br><br>

<h1>3. JSON 파일</h1>
<ul>
  <li>
    자바스크립트에서 나온 파일 형식들은 <strong>자바스크립트 객체 표기법</strong>, 즉 JSON(JavaScript Object Notation)으로 더 친숙하게 알려져 있다.
  </li>
  <li>
    Spark는 JSON 파일을 사용할 때 <strong>줄로 구분된 JSON</strong>을 기본적으로 사용한다.
  </li>
  <li>
    <strong>multiLine 옵션</strong>을 통해 <strong>줄</strong>로 구분된 방식과 <strong>여러 줄</strong>로 구성된 방식을 선택적으로 사용할 수 있다. (<strong>true</strong>로 지정할 경우 전체 파일을 <strong>하나의 JSON 객체</strong>로 읽을 수 있다).
  </li>
  <li>
    Spark는 JSON 파일을 <strong>파싱</strong>하여 <strong>DataFrame을 생성</strong>한다.
  </li>
  <li>
    줄로 구분된 JSON은 전체 파일을 읽어 들인 다음 저장하는 방식이 아니기에 <strong>새로운 record를 추가</strong>할 수 있다.
  </li>
  <li>
    다른 포맷에 비해 훨씬 더 <strong>안정적인 포맷</strong>이다.
  </li>
  <li>
    <strong>구조화</strong>가 되어있고, <strong>최소한의 기본 데이터 타입이 존재</strong>한다는 장점이 있으며 <strong>Spark는 적합한 데이터 타입을 추정</strong>할 수 있어 원활하게 처리가 가능하다.
  </li>
  <li>
    JSON은 객체이기에 CSV보다 <strong>옵션 수가 적다</strong>.
  </li>
</ul>

<br>

<h2>3-1. JSON 옵션 (p255)</h2>
<ul>
  <li>
    JSON 객체를 다룰 때 사용할 수 있는 옵션들의 모음으로 필요할 때 참고하면 된다.
  </li>
  <li>
    줄로 구분된 JSON 파일을 읽는 방법은 <strong>데이터 포맷 설정</strong>과 <strong>옵션 지정 방식</strong>만 다르다.
  </li>
</ul>

```python
spark.read.format("json")
```

<br>

<h2>3-2. JSON 파일 읽기</h2>

```python
# 1. JSON 파일을 읽는 방법과 옵션 비교 예제
spark.read.format("json").option("mode", "FAILFAST")\
    .option("inferSchema", "true")\
    .load("/opt/spark-data/data/flight-data/json/2010-summary.json").show(5)
```

<br>

<h2>3-3. JSON 파일 쓰기</h2>
<ul>
  <li>
    <strong>데이터소스와 관계없이</strong> JSON 파일에 저장할 수 있다.
  </li>
  <li>
    이전에 만들어 둔 CSV DataFrame을 <strong>JSON 파일의 소스로 재사용</strong>할 수 있다.
  </li>
  <li>
    <strong>파티션당 하나의 파일</strong>을 만들며 DataFrame을 <strong>단일 폴더</strong>에 저장한다.
  </li>
  <li>
    JSON 객체는 <strong>한 줄에 하나씩</strong> 기록된다.
  </li>
</ul>

```python
# 1. JSON 파일 쓰기
csvFile.write.format("json").mode("overwrite").save("/tmp/my-json-file.json")
```

<br><br>

<h1>4. 파케이 파일</h1>
<ul>
  <li>
    파케이는 <strong>다양한 스토리지 최적화 기술</strong>을 제공하는 <strong>오픈소스 컬럼</strong> 기반의 데이터 저장 방식이며 <strong>분석 워크로드</strong>에 최적화 되어 있다.
  </li>
  <li>
    전체 파일을 읽는 대신 <strong>개별 컬럼</strong>을 읽을 수 있으며, <strong>컬럼 기반의 압축 기능</strong>을 제공한다.
  </li>
  <li>
    Apache Spark와 잘 호환되며 Spark의 <strong>기본 파일 포맷</strong>이다.
  </li>
  <li>
    파케이 파일은 읽기 연산 시 JSON이나 CSV보다 효울적이기에 <strong>저장용 데이터</strong>는 파케이 포맷으로 저장하는 것이 좋다.
  </li>
  <li>
    <strong>복합 데이터 타입</strong>을 지원하며 컬럼은 배열, 맵, 구조체 데이터 타입도 문제 없기 가능하다.
  </li>
  <li>
    단, CSV에서는 <strong>배열</strong>을 사용할 수 없다.
  </li>
</ul>

```python
# 1. 파케이 포맷 지정 방법.
spark.read.format("parquet")
```

<br>

<h2>4-1. 파케이 파일 읽기</h2>
<ul>
  <li>
    파케이는 데이터를 저장할 때 <strong>자체 스키마</strong>를 사용해 데이터를 저장하기 때문에 <strong>옵션이 거의 없으며</strong> 포맷을 지정하는 것만으로도 충분하다.
  </li>
  <li>
    Schema는 <strong>DataFrame을 표현</strong>하기 위해 정확한 schema가 필요한 경우에만 설정한다.
  </li>
    <ul>
      <li>
        단, <strong>읽는 시점</strong>에 스키마를 알 수 있기에 사실상 <strong>잘 사용하지 않는다</strong>.
      </li>
      <li>
        파케이 파일은 스키마가 <strong>파일 자체에 내장</strong>되어 있어 <strong>추정</strong> 또한 필요없다.
      </li>
    </ul>
</ul>

```python
# 1. 파케이 파일 읽기.
spark.read.format("parquet")

spark.read.format("parquet")\
    .load("/opt/spark-data/data/flight-data/parquet/2010-summary.parquet").show(5)
```

<br>

<h2>3-3. JSON 파일 쓰기</h2>