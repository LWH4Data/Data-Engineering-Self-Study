<ul>
  <li>
    <strong>표현식</strong>을 만드는 방법과 다양한 <strong>데이터 타입</strong>을 배운다.
  </li>
  <ul>
    <li>
      불리언 타입
    </li>
    <li>
      수치 타입
    </li>
    <li>
      문자열 타입
    </li>
    <li>
      date와 timestamp 타입
    </li>
    <li>
      null 값 다루기
    </li>
    <li>
      복합 데이터 타입
    </li>
    <li>
      사용자 정의 함수
    </li>
  </ul>
</ul>

<br>

<h1>1. API는 어디서 찾을까</h1>
<ul>
  <li>
    Spark의 버전은 계속 업데이트 되기에 최신 함수를 찾아볼 필요가 있다. (대부분은 공식문서가 존재한다).
  </li>
  <li>
    DataFrame(Dataset) 메서드
  </li>
    <ul>
      <li>
        DataFrame은 Row 타입을 갖는 Dataset이기에 Dataset 메서드를 만난다.
      </li>
      <li>
        DataFrameStatFunctions
      </li>
        <ul>
          <li>
            다양한 통계적 함수를 제공한다.
          </li>
          <li>
            http://bit.ly/2DPYhJC
          </li>
        </ul>
      <li>
        DataFrameNameFunctions
      </li>
        <ul>
          <li>
            null 데이터를 다루는 데 필요한 함수를 제공한다.
          </li>
          <li>
            http://bit.ly/2DPAqd3
          </li>
        </ul>
    </ul>
  <li>
    Column 메서드
  </li>
    <ul>
      <li>
        Column API와 org.apache.spark.sql.functions는 Spark 문서를 참고한다.
      </li>
        <ul>
          <li>
            Column API: http://bit.ly/2FloFbr
          </li>
          <li>
            org.apache.spark.sql.functions: http://bit.ly/2DPAycx
          </li>
        </ul>
    </ul>
</ul>

```python
# 1. 실습에 사용할 데이터프레임 생성.
df = spark.read.format("csv")\
.option("header", "true")\
.option("inferSchema", "true")\
.load("/opt/spark-data/Spark-The-Definitive-Guide/data/retail-data/by-day")
df.printSchema()
df.createOrReplaceTempView("dfTable")
```

<br><br>

<h1>2. 스파크 데이터 타입으로 변환하기</h1>
<ul>
  <li>
    Spark는 <strong>lit 함수</strong>를 사용하여 프로그래밍 언어의 고유 데이터를 <strong>Spark 데이터 타입으로 변환</strong>한다.
  </li>
</ul>

```python
# 1. lit을 통해 Spark의 데이터 타입으로 변환
from pyspark.sql.functions import lit

df.select(lit(5), lit("five"), lit(5.0))
```

<br><br>

<h1>3. 불리언 데이터 타입 다루기</h1>
<ul>
  <li>
    여러 예시들은 하단의 실습을 따라가며 확인할 수 있다. 소개된 함수 외에도 많은 함수들이 있으며 API 문서(http://bit.ly/2ptAiY2)를 참고하여 활용할 수 있다.
  </li>
  <li>
    불리언은 모든 <strong>필터링 작업의 기반</strong>으로 <strong>데이터 분석에 필수</strong>적이다.
  </li>
  <li>
    불리언 구문은 <strong>and, or, true, false</strong>로 구성된다.
  </li>
  <li>
    불리언 구문을 통해 <strong>true 혹은 false로 평가</strong>되는 논리 문법을 생성하고, 생성된 결과를 기준으로 <strong>row를 필터링</strong>한다.
  </li>
  <li>
    Spark에서 일치 여부를 확인할 때에는 <strong>===</strong> 혹은 <strong>=!=</strong>을 활용한다.
  </li>
    <ul>
      <li>
        <strong>not 함수</strong>나 <strong>equalTo 메서드</strong>를 활용할 수도 있다.
      </li>
    </ul>
  <li>
    <strong>and 메서드</strong> 혹은 <strong>or 메서드</strong>를 사용하여 불리언 표현식을 <strong>여러 부분</strong>에 지정할 수 있다.
  </li>
  <li>
    하단의 특징에도 불구하고 불리언 표현식을 사용하는 경우 항상 모든 표현식을 <strong>and로 묶어 차례대로 필터를 적용</strong>해야 한다. 이편이 <strong>이해하기가 쉬우며 가독성</strong>이 좋다.
  </li>
    <ul>
      <li>
        불리언 문을 차례대로 표현하더라도 Spark는 내부적으로 and 구문을 필터 사이에 추가하여 모든 필터를 하나의 문장으로 변환한다. 그리고 동시에 모든 필터를 처리한다.
      </li>
      <li>
        and 구문을 조건문으로 만들 수도 있다.
      </li>
    </ul>
  <li>
    <strong>or 구문</strong>을 사용하는 경우 반드시 <strong>동일한 구문</strong>에 조건을 정의해야 한다.
  </li>
  <li>
    필터를 반드시 표현식으로 정의할 필요는 없으며 별도의 작업 없이 <strong>컬렴명</strong>을 사용하여 <strong>필터를 정의</strong>할 수도 있다.
  </li>
  <li>
    <strong>null 문제</strong>: null & null인 경우 false가 반환되는 우려할 수 있다.
  </li>
    <ul>
      <li>
        <strong>eqNullSafe()</strong> 등과 같은 방법으로 null 값을 처리하여 연산을 수행할 수 있다.
      </li>
    </ul>
</ul>

```python
# 1. 불리언 식을 활용하여 데이터 필터링
from pyspark.sql.functions import col

df.where(col("InvoiceNo") == 536365) \
.select("InvoiceNo", "Description") \
.show(5, False)
```

```python
# 2. 동등 여부를 포함하여 데이터 필터링
from pyspark.sql.functions import col

df.where(col("InvoiceNo") != 536365)\
.select("InvoiceNo", "Description")\
# False를 통해 같지 않은 데이터만 필터링
.show(5, False)

# 일치하지 않음을 표현하는 예
df.where("InvoiceNo = 536365")
.show(5, false)
df.where("InvoiceNo <> 536365")
.show(5, false)
```

```python
# 3. 불리언 표현식을 활용한 데이터 필터링
from pyspark.sql.functions import instr

priceFilter = col("UnitPrice") > 600
descripFilter = instr(df.Description, "POSTAGE") >= 1
df.where(df.StockCode.isin("DOT")).where(priceFilter | descripFilter).show()
```

```python
# 4. 불리언 컬럼을 사용해 DataFrame 필터링
from pyspark.sql.functions import instr

DOTCodeFilter = col("StockCode") == "DOT"
priceFilter = col("UnitPrice") > 600
descripFilter = instr(col("Description"), "POSTAGE") >= 1
df.withColumn("isExpensive", DOTCodeFilter & (priceFilter | descripFilter))\
.where("isExpensive")\
.select("UnitPrice", "isExpensive").show(5)
```

```python
# 5. 컬럼명을 활용한 필터링
from pyspark.sql.functions import expr

df.withColumn("isExpensive", expr("NOT UnitPrice <= 250"))\
.where("isExpensive")\
.select("Description", "UnitPrice").show(5)
```

```python
# 6. null 값에 안전한 동치 테스트 수행
df.where(col("Description").eqNullSafe("hello")).show()
```

<br><br>

<h1>4. 수치형 데이터 타입 다루기</h1>

- count는 빅데이터 처리에서 필터링 다음으로 많이 수행하는 작업이다.
- 대부분은 수치형 데이터 타입을 사용해 연산 방식을 정의하기만 하면 된다.

```python
# 1. (현제 수량 * 단위 가격)^2 + 5 연산을 적용.
#   - pow 함수: 표시된 지수만큼 컬럼의 값을 거듭제곱한다.
from pyspark.sql.functions import expr, pow

fabricatedQuantity = pow(col("Quantity") * col("UnitPrice"), 2) + 5
df.select(expr("CustomerId"), fabricatedQuantity.alias("realQuantity")).show(2)

# SQL을 사용한 처리
df.selectExpr(
  "CustomerId",
  "(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity").show(2)
```

```python
# 2. 소수점 첫째 자리 반올림 실습
#   - Spark는 정확한 계산이 가능한 함수를 제공한다. 따라서 소수점 자리를 없애거나 할 필요가 
#   없다.
from pyspark.sql.functions import lit, round, bround

# round: 반올림
# bround: 반내림
df.select(round(lit("2.5")), bround(lit("2.5"))).show(2)
```

```python
# 3. 피어슨 상관계수 계산 실습
from pyspark.sql.functions import corr

# 직업 연산이라 결과가 바로 나온다.
df.stat.corr("Quantity", "UnitPrice")

# df.select(corr("Quantity", "UnitPrice")): transformation이기에 show()가 필요하
# 다.
df.select(corr("Quantity", "UnitPrice")).show()
```

```python
# 4. describe 메서드를 통해 요약 통계 반환.
#   - 집계 (count), 평균 (mean), 표준편차 (stddev), 최솟값 (min), 최댓값 (max)
#   - 통계 스키마는 변경될 수 있기에 describe는 콘설 확인용으로만 사용해야 한다.
df.describe().show()
```

```python
# 5. describe 대신 개별적으로 컬럼에 함수를 적용할 수도 있다.
from pyspark.sql.functions import count, mean, stddev_pop, min, max
```

```python
# 6. StatFunctions 패키지의 통계 함수를 통해 다양한 통계 계산 수행
#   - stat 속성을 사용해 접근할 수 있으며 다양한 통곗값을 계산할 때 사용하는 DataFrame이
#   다.

# approxQuantile 메서드를 활용해 데이터의 백분위수 계산
olName = "UnitPrice"
quantileProbs = [0.5]
relError = 0.05

df.stat.approxQuantile("UnitPrice", quantileProbs, relError)
```

```python
# 7. StatFunctions 패키지를 활용하여 교차표(cross-tabulation) 확인.
#   - 너무 큰 경우 화면에 전체 출력이 안될 수도 있다.
df.stat.crosstab("StockCode", "Quantity").show()
df.stat.freqItems(["StockCode", "Quantity"]).show()
```

```python
# 8. StatFunctions 패키지의 monotonically_increasing_id를 통해 모든 로우에 고유 ID 
#    값을 추가.
from pyspark.sql.functions import monotonically_increasing_id

df.select(monotonically_increasing_id()).show(2)
```

<br><br>

<h1>5. 문자열 데이터 타입 다루기</h1>

- 로그 파일에 정규 표현식을 사용하여 데이터 추출, 데이터 치환, 문자열 존재 여부, 대/소문자 변환 등의 작업을 할 수 있다.

```python
# 1. initcap
#   - 주어진 문자열을 공백으로 나누고 모든 단어의 첫 글자를 대문자로 변경한다.
from pyspark.sql.functions import initcap

df.select(initcap(col("Description"))).show()
```

```python
# 2. lower와 upper 함수를 통해 대소문자 변환
from pyspark.sql.functions import lower, upper

df.select(col("Description"),
lower(col("Description")),
upper(lower(col("Description")))).show(2)
```

```python
# 3. 공백 추가 제거 작업
#   - ltrim: 왼쪽 공백 제거
#   - rtrim: 오른쪽 공백 제거
#   - trim: 양쪽 공백 제거
#   - lpad(col, length, pad)
#     - col: 대상 데이터
#     - length: 지정한 길이로 맞춤
#     - pad: 부족한 부분을 채울 문자
#   - rpad(col, length, pad)
from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim

df.select(
  ltrim(lit("    HELLO    ")).alias("ltrim"),
  rtrim(lit("    HELLO    ")).alias("rtrim"),
  trim(lit("    HELLO    ")).alias("trim"),
  lpad(lit("HELLO"), 3, " ").alias("lp"),
  rpad(lit("HELLO"), 10, "*").alias("rp")).show(2)
```

<br><br>

<h2>5-1. 정규 표현식</h2>
<ul>
  <li>
    문자열의 <strong>존재 여부를 확인</strong>하거나 <strong>일치하는 모든 문자열을 치환</strong>할 때에는 보통 <strong>정규 표현식</strong>을 사용한다.
  </li>
  <li>
    <strong>정규 표현식</strong>을 사용해 문자열에서 값을 추출하거나 치환하는 데 필요한 <strong>규칙 모음</strong>을 정의할 수 있다. 
  </li>
  <li>
    Spark는 자바 정규 표현식이 갖는 강력한 능력을 활용한다.
  </li>
  <li>
    함수들의 사례는 하단의 실습 코드들에서 확인할 수 있다.
  </li>
</ul>


- Spark는 자바 정규 표현식이 가진 강력한 능력을 활용한다.
  - regexp_extract함수와 regexp_replace 함수를 사용하여 값을 추출하고 치환한다.

```python
# 1. regexp_replace 함수를 사용해 'desctiption' 컬럼의 값을 'COLOR'로 치환.
from pyspark.sql.functions import regexp_replace

regex_string = "BLACK|WHITE|RED|GREEN|BLUE"
df.select(
  regexp_replace(col("Description"), regex_string, "COLOR").alias("color_clean"),
  col("Description")).show(5)
```

```python
# 2. translate
#   - 정규 표현식을 사용하지 않고 문자열 치환
from pyspark.sql.functions import translate

# L → 1
# E → 3
# T → 7
df.select(translate(col("Description"), "LEET", "1337"), col("Description"))\
.show(5)
```

```python
# 3. regexp_extract를 사용해 처음 나타난 색상 이름 추출.
from pyspark.sql.functions import regexp_extract

extract_str = "(BLACK|WHITE|RED|GREEN|BLUE)"
df.select(
  regexp_extract(col("Description"), extract_str, 1).alias("color_clean"),
  col("Description")).show(5)
```

```python
# 4. contains를 컬럼의 문자열에 값이 포함되어 있는지 확인. (불리언 타입 반환).
from pyspark.sql.functions import instr

containsBlack = instr(col("Description"), "BLACK") >= 1
containsWhite = instr(col("Description"), "WHITE") >= 1
df.withColumn("hasSimpleColor", containsBlack | containsWhite)\
.where("hasSimpleColor")\
# 포함되지 않는 경우를 반환
.select("Description").show(3, False)
```

```python
# 5. locate를 사용해 인수의 개수가 동적으로 변하는 경우를 해결.
#   - 쉽게 확장이 가능하며 컬럼이나 불리언 필터를 프로그래밍 방식으로 생성할 수 있다.
#   - locate 함수를 확장해 입력값의 최소공배수나 소수 여부를 파악할 수 있다.

from pyspark.sql.functions import expr, locate

# 색상 리스트 초기화
simpleColors = ["black", "white", "red", "green", "blue"]


def color_locator(column, color_string):
  # locate는 column에 color_string.upper가 포함되면 위치를 반환한다. (없으면 0).
  return locate(color_string.upper(), column)\
    # 위치가 있는 경우를 1로, 없는 경우를 0으로 반환한다.
    .cast("boolean")\
    # 컬럼 이름을 지정한다.
    .alias("is_" + color_string)

# 리스트 컴프리헨션을 이용하여 색상 리스트에 맞게 boolean 컬럼을 생성한다.
selectedColumns = [color_locator(df.Description, c) for c in simpleColors]
'''
< 실제 예 >
selectedColumns = [
    color_locator(df.Description, "black"),
    color_locator(df.Description, "white"),
    color_locator(df.Description, "red"),
    color_locator(df.Description, "green"),
    color_locator(df.Description, "blue")
]
'''

# 전체 컬럼을 추가한다.
selectedColumns.append(expr("*"))

# 모든 컬럼을 선택
df.select(*selectedColumns)\
# white이거나 red인 경우만 필터링
.where(expr("is_white OR is_red"))\
# Description 컬럼만 출력.
.select("Description").show(3, False)
```

<br><br>

<h1>6. 날짜와 타임스탬프 데이터 타입 다루기</h1>
<ul>
  <li>
    Spark는 계속해서 <strong>시간대(timezone)</strong>, 즉 <strong>format</strong>이 올바르고 유요한지 확인해야 한다.
  </li>
  <li>
    Spark는 복잡함을 피하기 위해 다음 두 가지 정보만 집중적으로 관리한다.
  </li>
    <ul>
      <li>
        달력 형태의 <strong>date</strong>
      </li>
      <li>
        날짜와 시간 정보를 모두 갖는 <strong>timestamp</strong>
      </li>
    </ul>
  <li>
    Spark는 특정 날짜 포맷을 명시하지 않아도 자체적으로 식별하여 데이터를 읽을 수 있다. (권장 X).
  </li>
  <li>
    date와 timestamp를 다루는 작업은 문자열을 다루는 작업과 관련있다.
  </li>
    <ul>
      <li>
        날짜나 시간을 문자열로 저장하고, 런타임에 날짜 타입으로 변환하는 경우가 많다.
      </li>
      <li>
        주로 텍스트나 CSV 형식의 데이터를 다룰 때 많이 발생한다.
      </li>
    </ul>
  <li>
    시간대 설정이 필요한 경우 Spark SQL 설정의 spark.conf.sessionLocalTimeZone 속성을 로컬 시간대로 지정해 사용할 수 있다.
  </li>
  <li>
    시간대의 format은 반드시 자바 TimeZone format을 따라야 한다.
  </li>
  <li>
    특이한 format의 날짜와 시간 데이터를 다루어야 한다면 각 단계별로 어떤 데이터 타입과 format을 유지하는지 정확히알고 테스트 한다.
  </li>
    <ul>
      <li>
        TimestampType: 초 단위 정밀도 까지만 지원한다.
      </li>
      <li>
        Long: 밀리세컨드나 마이크로세컨드 단위를 지원한다. (이 이상은 TimestampType으로 전환될 때 제거된다).
      </li>
    </ul>
  <li>
    Spark는 특정 시점 데이터 format이 약간 특이하게 변할 수 있는데 이 문제를 피하기 위해서는 파싱이나 변환작업을 해야한다.
  </li>
</ul>

```python
# 1. today 컬럼에 현재 날짜를, now 컬럼에 현재 timestamp를 지정하여 실습 준비를 한다.
from pyspark.sql.functions import current_date, current_timestamp

# 0 ~ 9 까지의 수를 갖는 DataFrame을 생성.
dateDF = spark.range(10)\
# 현재 날짜를 나타내는 컬럼 추가.
.withColumn("today", current_date())\
# 현재 시각을 포함한 날짜를 나타내는 컬럼 추가.
.withColumn("now", current_timestamp())

# DataFrame을 SQL에서 사용할 수 있도록 임시 뷰 등록
dateDF.createOrReplaceTempView("dateTable")

# 스키마 출력.
dateDF.printSchema()
```

```python
# 2. date_sub 함수와 date_add 함수를 통해 오늘을 기준으로 5일 전후의 날짜 구하기.
#   - date_sub: 날짜의 뺄셈
#   - date_add: 날짜의 덧셈
from pyspark.sql.functions import date_add, date_sub

dateDF.select(date_sub(col("today"), 5), date_add(col("today"), 5)).show(1)
```

```python
# 3. datediff 함수를 사용하여 두 날짜 사이의 일수(차이)를 반환
from pyspark.sql.functions import datediff, months_between, to_date

# week_ago 컬럼을 생성하고 today - 7을 값으로 한다.
# datediff를 통해 생성한 week_ago 컬럼과 today의 컬럼의 차이가 7인지 확인.
dateDF.withColumn("week_ago", date_sub(col("today"), 7))\
.select(datediff(col("week_ago"), col("today"))).show(1)
```

```python
# 4. months_between 함수를 통해 개월 수의 차이를 반환한다.
# 첫 번째 인수에서 두 번째 인수를 차감하기에 소수점 음수가 반환된다.
dateDF.select(
  to_date(lit("2016-01-01")).alias("start"),
  to_date(lit("2017-05-22")).alias("end"))\
  .select(months_between(col("start"), col("end"))).show(1)
```

```python
# 5. to_date 함수를 통해 `문자열 → 날짜` 변환
#   - SimpleDateFormat 클래스가 지원하는 포맷을 사용해야 한다.
from pyspark.sql.functions import to_date, lit

spark.range(5).withColumn("date", lit("2017-01-01"))\
# format을 지정하지 않고 기본 "yyyy-MM-dd"로 받는다.
.select(to_date(col("date"))).show(1)
```

```python
# 6. Spark는 날짜를 파싱할 수 없다면 error 대신 null을 반환한다.
#   - null을 반환하기에 디버깅이 어렵고, 파이프라인이 까다롭다.

# 2016-20-12는 년-일-월 형태로 null을 반환한다.
dateDF.select(to_date(lit("2016-20-12")), to_date(lit("2017-12-11"))).show(1)
```

```python
# 7. 날짜 포맷으로 인한 null 값 문제 방지 방법들

# SimpleDateFormat 표준에 마주처 날짜 포맷 지정.
from pyspark.sql.functions import to_date

# 날짜 형식 지정.
dateFormat = "yyyy-dd-MM"
cleanDateDF = spark.range(1).select(
  to_date(lit("2017-12-11"), dateFormat).alias("date"),
  to_date(lit("2017-20-12"), dateFormat).alias("date2"))
cleanDateDF.createOrReplaceTempView("dateTable2")

# 결과 확인.
cleanDateDF.show(1)
```

```python
# 8. to_timestamp를 활용한 날짜 지정.
#   - to_timestamp는 항상 날짜 포맷을 지정해야 한다.
from pyspark.sql.functions import to_timestamp

cleanDateDF.select(to_timestamp(col("date"), dateFormat)).show()
```

```python
# 9. 날짜 비교

# 날짜 혹은 타임스탬프 타입을 활용.
cleanDateDF.filter(col("date2") > lit("2017-12-12")).show()

# Spark가 리터럴로 인식하는 문자열을 지정해 날짜 비교.
cleanDateDF.filter(col("date2") > "2017-12-12").show()
```

<br><br>

<h1>7. null 값 다루기</h1>
<ul>
  <li>
    Spark는 빈 문자열이나 대체 값 대신 <storng>null</storng>을 사용하여 <storng>최적화</storng>를 수행할 수 있다.
  </li>
  <li>
    DataFrame의 하위 패키지인 <storng>.na</storng>를 사용하는 것이 DataFrame에서 <storng>null을 다루는 기본</storng>이다.
  </li>
  <li>
    연산을 수행할 때 Spark가 <storng>null 값을 제어</storng>하는 방법을 <storng>명시적으로 지정</storng>하는 몇 가지 함수도 존재한다.
  </li>
  <li>
    Spark는 null 값을 허용하지 않는 컬럼을 선언하여도 <storng>null 값이 존재</storng>할 수 있는 함정이 있다. (강제성이 없다).
  </li>
</ul>

<br>

<h2>7-1. coalesce</h2>
<ul>
  <li>
    coalesce 함수는 지정한 여러 컬럼 중 <strong>null이 아닌 첫 번째 값(row)</strong>을 반환한다.
  </li>
  <li>
    만약 모든 컬럼에 null이 없다면 <strong>첫 번째 컬럼의 값</strong>을 반환한다.
  </li>
</ul>

```python
# 1. coalesce를 통해 주어진 컬럼들의 null 여부에 따른 결과를 반환한다.
from pyspark.sql.functions import coalesce

df. select(coalesce(col("Description"), col("CustomerId"))).show()
```

<br>

<h2>7-2. ifnull, nullif, nvl, nvl2</h2>
<ul>
  <li>
    SQL 함수인데 이번에는 python을 위주로 다룰 것이기에 생략한다.
  </li>
</ul>

<br>

<h2>7-3. drop</h2>
<ul>
  <li>
    <strong>drop</strong> 메서드는 <strong>null</strong> 값을 갖는 <strong>모든 row를 제거</strong>한다.
  </li>
</ul>

```python
# 1. drop 메서드를 사용하여 null이 포함된 모든 row를 제거한다.
df.na.drop()

# any: 하나의 컬럼의 값이라도 null 이라면 해당 row를 제거한다.
df.na.drop("any")

# all: 해당 row의 모든 컬럼의 값이 null 혹은 NaN인 경우에만 해당 row를 제거한다.
df.na.drop("all")
```

```python
# 2. subset을 통해 특정 컬럼들에만 적용할 수도 있다.
df.na.drop("all", subset=["StockCode", "InvoiceNo"])
```

<br>

<h2>7-4. fill</h2>
<ul>
  <li>
    <strong>fill 함수</strong>는 하나 이상의 컬럼에 존재하는 null 값을 <strong>특정 값</strong>으로 채울 수 있다.
  </li>
</ul>

```python
# 1. String 데이터 타입의 null 값을 치환.
df.na.fill("All Null values become this string")

# Integer
df.na.fill(5:Integer)

# Double
df.na.fill(5:Double)
```

```python
# 2. 컬럼명을 배열로 전달하여 다수의 컬럼에 fill 적용.
df.na.fill("all", subset=["StockCode", "InvoiceNo"])
```

```python
# 3. Map 타입을 사용해 다수의 컬럼에 fill 메서드를 적용할 수 있다.
#   - key: 컬럼명
#   - value: 값을 채우는 용도
fill_cols_vals = {"StockCode":5, "Description": "No Value"|}
df.na.fill(fill_cols_vals)
```

<br>

<h2>7-5. replace</h2>
<ul>
  <li>
    <strong>replace 메서드</strong>를 활용하여 <strong>조건</strong>에따라 <strong>null 값을 대체</strong>할 수 있다.
  </li>
  <li>
    대체하는 값은 원래 값과 <strong>데이터 타입</strong>이 같아야 한다.
  </li>
</ul>

```python
# 1. replace를 통한 null 대체
df.na.replace([""], ["UNKNOWN"], "Description")
```

<br><br>

<h1>8. 정렬하기</h1>
<ul>
  <li>
    5장에서 다룬 것과 같이 <strong>asc_nulls_first, desc_nulls_first, asc_nulls_last, desc_nulls_last</strong> 함수를 사용해 DataFrame을 정렬할 때 <strong>null 값이 표시되는 기준</strong>을 지정할 수 있다.
  </li>
</ul>

<br><br>

<h1>9. 복합 데이터 타입 다루기</h1>
<ul>
  <li>
    복합 데이터 타입에는 구조체(struct), 배열(array), 맵(map)이 있다.
  </li>
</ul>

<br>

<h2>9-1. 구조체</h2>
<ul>
  <li>
    구조체는 DataFrame 내부의 DataFrame으로 생각할 수 있다. 
  </li>
  <li>
    쿼리문에서 다수의 컬럼을 괄호로 묶어 구조체를 만들 수 있다.
  </li>
</ul>

```python
# 1. 컬럼들을 괄호로 묶어 구조체로 만들기.
from pyspark.sql.functions import struct

complexDF = df.select(struct("Description", "InvoiceNo").alias("complex"))
complexDF.createOrReplaceTempView("complexDF")
```

```python
# 2. 구조체는 DataFrame을 조회하는 것과 동일하게 사용할 수 있다. 단, 문법에 점(.)을 사
#    하거나 getField 메서드를 사용한다는 차이가 있다.

# 점(.)을 활용한 방법.
complexDF.select("complex.Description")

# getField 메서드를 활용한 방법.
complexDF.select(col("complex").getField("Description"))
```

```python
# 3. "*" 문자를 통해 모든 값을 조회하고, 모든 컬럼을 DataFrame의 최상위 수준으로 올릴 수 있
#    다.
complexDF.select("complex.*")
```

<br>

<h2>9-2. 배열</h2>
<ul>
  <li>
    데이터에서 Description 컬럼의 모든 단어를 하나의 row로 변환한다.
  </li>
</ul>

<h3>9-2-1. split</h3>
<ul>
  <li>
    우선 Description 컬럼을 복합 데이터 타입인 배열로 변환한다.
  </li>
  <li>
    배열로 변환핧 때에는 <strong>split 함수</strong>를 사용한다.
  </li>
  <li>
    split 함수는 <strong>구분자(delimiter)</strong>를 인수로 받고, 해당 구분자를 기준으로 <strong>배열로 변환</strong>한다.
  </li>
  <li>
    split 함수는 Spark에서 복합 데이터 타입을 마치 <strong>또 다른 컬럼</strong>처럼 다룰 수 있는 매우 강력한 기능이다.
  </li>
</ul>

```python
# 1. split을 통한 배열 변환
from pyspark.sql.functions import split

# 공백을 기준으로 값을 나누어 배열로 변환한다.
df.select(split(col("Description"), " ")).show(2)
```

```python
# 2. 파이썬과 유사한 문법을 통해 배열값 조회
df.select(split(col("Description"), " ").alias("array_col"))\
.selectExpr("array_col[0]").show(2)
```

<h3>9-2-2. 배열의 길이</h3>
<ul>
  <li>
    <strong>배열의 크기(size)</strong>를 조회해 <strong>배열의 길이</strong>를 알 수 있다.
  </li>
</ul>

```python
# 1. 배열의 크기 조회를 통한 길이 확인.
from pyspark.sql.functions import size

df.select(size(split(col("Description"), " "))).show(2)
```

<h3>9-2-3. array_contains</h3>
<ul>
  <li>
    <strong>array_contains 함수</strong>를 사용해 <strong>배열에 특정 값</strong>이 존재하는지 확인할 수 있다.
  </li>
</ul>

```python
# 1. array_contains를 통해 배열에 특정 값 포함 여부 확인.
from pyspark.sql.functions import array_contains

df.select(array_contains(split(col("Description"), " "), "WHITE")).show(2)
```

<h3>9-2-4. explode</h3>
<ul>
  <li>
    <strong>explode 함수</strong>는 <strong>배열 타입의 컬럼</strong>을 입력받고, 입력 받은 컬럼의 <strong>배열값에 포함된 모든 값</strong>을 <strong>row로 변환</strong>한다. (나머지 컬럼은 그대로 중복되어 표기된다).
  </li>
</ul>

```python
# 1. explode를 통한 `배열값 → row` 변환.
from pyspark.sql.functions import split, explode

df.withColumn("splitted", split(col("Description"), " "))\
.withColumn("exploded", explode(col("splitted")))\
.select("Description", "InvoiceNo", "exploded").show(8)
```

<br>

<h2>9-3. 맵</h2>
<ul>
  <li>
    맵은 <strong>map 함수</strong>와 컬럼의 <strong>key-value 쌍</strong>을 이용해 생성한다.
  </li>
  <li>
    <strong>배열과 동일한 방법</strong>으로 <strong>값을 선택</strong>할 수 있다.
  </li>
</ul>

```python
# 1. map 함수를 통해 Map 객체 생성.
from pyspark.sql.functions import create_map

df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map"))\
.show(2)
```

```python
# 2. 적합한 key를 통해 데이터를 조회. (해당 key가 없는 경우 null을 반환).
df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map"))\
.selectExpr("complex_map['WHITE METAL LANTERN']").show(2)
```

```python
# 3. map 타입을 분해하여 컬럼으로 변환.
df.select(create_map(col("Description"), col("InvoiceNo")).alias("complex_map"))\
.selectExpr("explode(complex_map)").show(2)
```

<br><br>

<h1>10. JSON 다루기</h1>
<ul>
  <li>
    Spark는 문자열 형태의 <strong>JSON을 직접 조작</strong>, <strong>파싱</strong> 그리고 <strong>JSON 객체를 생성</strong>할 수 있다.
  <li>
</ul>

```python
# 1. JSON 객체 생성.
jsonDF = spark.range(1).selectExpr("""
'{"myJSONKey": {"myJSONValue" : [1, 2, 3]}}' as jsonString""")
```

```python
# 2. JSON 객체 조회
from pyspark.sql.functions import get_json_object, json_tuple

jsonDF.select(

  # get_json_object: JSON객체(딕셔너리나 배열)를 인라인 쿼리로 조회
  get_json_object(col("jsonString"), "$.myJSONKey.myJSONValue[1]").alias("column"),

  # json_tuple: 중첩이 없는 단일 수준의 JSON 조회
  json_tuple(col("jsonString"), "myJSONKey")).show(2)
```

```python
# 3. to_json 함수를 통해 'StructType → JSON 문자열' 변환
from pyspark.sql.functions import to_json

df.selectExpr("(InvoiceNo, Description) as myStruct")\
.select(to_json(col("myStruct")))
```

```python
# 4. to_json으로 변환 후 다시 from_json으로 형 되돌리기.
from pyspark.sql.functions import from_json
from pyspark.sql.types import *

# 구조체 생성.
parseSchema = StructType((
  StructField("InvoiceNo", StringType(), True),
  StructField("Description", StringType(), True)))

# 구조체를 to_json으로 변환 후 다시 from_json으로 되돌린다.
df.selectExpr("(InvoiceNo, Description) as myStruct")\
.select(to_json(col("myStruct")).alias("newJSON"))\
.select(from_json(col("newJSON"), parseSchema), col("newJSON")).show(2)
```

<br><br>

<h1>11. 사용자 정의 함수</h1>

<ul>
  <li>
    Spark는 <strong>UDF(user defined function)</strong>을 사용할 수 있기에 매우 강력하다.
  </li>
  <li>
    Scala, python, Java 등으로 함수를 작성할 수 있는데 <strong>언어마다 성능</strong>이 다르기에 주의가 필요하다.
  </li>
  <li>
    사용자 정의 함수를 생성한 뒤 워커 노드에서 사용할 수 있도록 <strong>Spark에 등록</strong>한다.
  </li>
    <ul>
      <li>
        Spark는 <strong>드라이버</strong>에서 함수를 <strong>직렬화</strong>하고, <strong>네트워크</strong>를 통해 <strong>모든 익스큐터 프로세스에 전달</strong>한다.
      </li>
    </ul>
  <li>
    가급적 사용자 정의 함수는 <strong>Scala</strong>혹은 <strong>Java</strong>로 작성하는 것이 좋다.
  </li>
    <ul>
      <li>
        python의 경우 Spark 워커 노드에 python process를 실행하고 python이 이해할 수 있는 format으로 모든 데이터를 <strong>직렬화</strong>한다.<br>→ 데이터 전달을 위해 <strong>직렬화</strong>를하는 과정에서 <strong>큰 부하</strong> 발생.
      </li>
      <li>
        데이터가 python으로 전달되면 Spark에서 <strong>워커 메모리를 관리할 수 없다</strong>.
      </li>
        <ul>
          <li>
            JVM과 python이 <strong>동일한 머신에서 메모리 경합</strong>을 하기 때문에 자원에 제약이 생긴다.
          </li>
        </ul>
    </ul>
  <li>
    <strong>SQL 함수로 등록</strong>하면 <strong>모든 프로그래밍 언어</strong>와 <strong>SQL</strong>에서 사용자 정의 함수를 사용할 수 있다.
  </li>
  <li>
    Spark는 자체 데이터 타입을 사용하기 때문에 함수를 정의할 때 <strong>반환 타입</strong>을 지정하는 것이 좋다.
  </li>
  <li>
    함수에서 반환될 실제 데이터 타입과 <strong>일치하지 않는 데이터 타입</strong>을 지정하면 Spark는 오류가 아닌 <strong>null을 반환</strong>한다. (디버깅이 어려움).
  </li>
  <li>
    사용자 정의 함수에서 값을 <strong>선택적으로 반환</strong>하려면 <strong>python은 None</strong>를, <strong>Scala는 Option 타입</strong>을 반환해야 한다.
  </li>
</ul>

```python
# 1. 사용자 정의 함수 생성 및 적용

# DataFrame 생성.
udfExampleDF = spark.range(5).toDF("num")

# 함수 정의
def power3(double_value):
  return double_value ** 3

# 함수 실행.
power3(2.0)
```

```python
# 2. 사용자 정의 함수 등록
from pyspark.sql.functions import udf

# 함수 등록
power3udf = udf(power3)
```

```python
# 3. 등록한 사용자 정의 함수 사용.
from pyspark.sql.functions import col

udfExampleDF.select(power3udf(col("num"))).show(2)
```

```python
# 4. 스칼라로 등록된 사용자 정의 함수를 SQL 함수로 등록 후 python에서 사용.
# 현재 pyspark를 사용 중이라 실습은 불가(Scala로 등록이 안됨).
udfExampleDF.selectExpr("power3(num)").show(2)
```

```python
# 5. 데이터 타입이 달라 null이 출력되는 실습
from pyspark.sql.types import IntergerType, DoubleType

# 함수의 반환값이 DoubleType로 등록.
spark.udf.resiter("power3py", power3, DoubleType())

# 실제 함수 실행.
udfExampleDF.selectExpr("power3py(num)").show(2)

# 함수의 결과는 Integer인데 등록한 결과는 DoubleType이다.
# null값이 반환되지 않도록 하려면 함수의 결과가 Float으로 반환되게 해야한다.
```

<br><br>

<h1>12. Hive UDF</h1>
<ul>
  <li>
    하이브로 작성하는 법을 다루기에 정리하지 않는다.
  </li>
</ul>