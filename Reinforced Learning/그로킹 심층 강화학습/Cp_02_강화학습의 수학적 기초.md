<ul>
  <li>
    <strong>복잡성</strong>: 복잡한 상황을 정확하게 일반화하는 것을 학습하는 것은 결국 <strong>샘플링된 피드백 정보</strong>를 바탕으로 학습하기 때문에 어려움을 겪는다.
  </li>
  <li>
    <strong>순차성</strong>: 행동을 취한 결과가 나중에 나올 수 있다. 즉, 연속된 피드백으로 학습하는 상황에서 <strong>과거에 취했던 행동</strong>에 대해 가치를 정의하는 것은 어렵다.
  </li>
  <li>
    <strong>불확실성</strong>: <strong>세상이 돌아가는 원리</strong>를 모르는 상태에서 우리가 취한 행동이 어떻게 영향을 주는지 알 수 없다.
  </li>
  <li>
    이번 장부터 <strong>마프코프 결정 과정(Markov decision process, MDP)</strong>라는 수학적인 틀을 통해 위의 문제들을 나타내는 것에 대해 학습한다.
  </li>
    <ul>
      <li>
        강화학습의 <strong>핵심 요소</strong>에 대해 학습한다.
      </li>
      <li>
        <strong>마르코프 결정 과정(MDP)</strong>로 알려진 수학 프레임워크를 통해 강화학습 환경을 <strong>연속적인 의사결정 문제</strong>로 표현하는 방법을 학습한다.
      </li>
      <li>
        남은 장에서 강화학습 에이전트가 <strong>해결할 간단한 환경</strong>을 만든다.
      </li>
    </ul>
</ul>

<br>

<h1>1. 강화학습의 구성 요소</h1>
<ul>
  <li>
    강화학습을 구성하는 두 개의 주요 구성 요소는 <strong>에이전트</strong>와 <strong>환경</strong>이다.
  </li>
    <ul>
      <li>
        <strong>에이전트</strong>: <strong>결정</strong>을 내리는 주체이며, 문제에 대한 <strong>답을 제공</strong>한다.
      </li>
      <li>
        <strong>환경</strong>: 문제를 대표하는 <strong>객체</strong>이다.
      </li>
    </ul>
  <li>
    강화학습이 다른 기계학습과 다른 점은 에이전트와 환경 사이에서 <strong>반응을 주고 받는다는 것</strong>이다.
  </li>
    <ul>
      <li>
        에이전트는 행동을 통해 <strong>환경에 영향</strong>을 주려하고, 반대로 환경은 에이전트의 <strong>행동에 대해 반응</strong>한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-1. 문제, 에이전트, 환경에 대한 예시</h2>
<ul>
  <li>
    네 가지 상황을 에이전트, 환경, 행동 그리고 관찰로 정의하여 예를 보여준다.
  </li>
</ul>

<br>

<h2>1-2. 에이전트: 의사 결정권자</h2>
<ul>
  <li>
    에이전트는 강화학습의 큰 관점에서 <strong>결정을 내리는 요소</strong>이다.
  </li>
    <ul>
      <li>
        에이전트는 내부적으로 <strong>구성요소</strong>와 <strong>과정</strong>을 갖는데 이들은 <strong>특정 문제를 해결하는데 최적화</strong>되도록 만들어져 있다.
      </li>
    </ul>
  <li>
    대부분의 에이전트들은 다음 세 단계의 과정을 거친다.
  </li>
    <ul>
      <li>
        <strong>상호작용 요소</strong>를 통해 학습에 필요한 <strong>데이터를 수집</strong>한다.
        <br>→ 모든 에이전트들은 현재 취하고 있는 <strong>행동을 평가</strong>한다.
        <br>→ 전체적인 성능을 개선하기 위해 만들어진 무언가를 <strong>개선</strong>한다. 
      </li>
    </ul>
</ul>

<br>

<h2>1-3. 환경: 그 외의 모든 것</h2>
<ul>
  <li>
    실세상에서의 대부분의 의사-결정 문제들은 강화학습 환경으로 표현할 수 있으며 대표적으로 <strong>마르코프 결정 과정(Markov decision process, MDP)</strong>로 알려진 수학적 프레임워크를 통해 모델링을 한다.
  </li>
    <ul>
      <li>
        강화학습은 기본적으로 모든 환경이 이면에서 동작하는 MDP를 가지고 있다고 가정한다.
      </li>
    </ul>
  <li>
    환경은 문제와 관련된 <strong>변수들의 집합</strong>으로 표현된다.
  </li>
    <ul>
      <li>
        이렇게 표현할 수 있는 변수들의 모든 값에 대한 <strong>조합</strong>을 <strong>상태 영역(state space)</strong>이라 표현한다.
      </li>
    </ul>
  <li>
    에이전트는 어떤 방식으로든 환경으로 부터 무언가를 관찰할 수 있으며 이렇게 에이전트가 <strong>어떤 특정 시간에 얻을 수 있는 변수들의 집합</strong>을 <strong>관찰(observation)</strong>이라 한다.
  </li>
    <ul>
      <li>
        관찰의 변수들이 가질 수 있는 모든 조합을 <strong>관찰 영역(observation space)</strong>이라고 한다.
      </li>
    </ul>
  <li>
    강화학습에서는 환경이 완전히 관측 가능하면 상태와 관찰을 같은 의미로 쓰기도 하지만, 실제로는 <strong>관찰은 상태의 일부</strong>이거나 <strong>변형</strong>일 수 있다.
  </li>
    <ul>
      <li>
        즉, 항상 에이전트가 환경 내부 상태에 대해서 인지하는 것은 아니기 때문이다.
      </li>
    </ul>
  <li>
    매 상태에서 환경은 에이전트가 선택할 수 있는 여러 <strong>행동들의 집합</strong>을 만들어낸다. 모든 행동에 대한 집합은 <strong>행동 영역(action space)</strong>이라고 한다.
  </li>
  <li>
    환경은 에이전트의 행동에 반응해 <strong>상태를 변화</strong>시킨다. 현재 상태와 행동이 주어졌을 때 다음 상태로의 변화를 정의하는 함수를 <strong>전이함수(transition function)</strong>라 한다.
  </li>
    <ul>
      <li>
        전이가 발생한 후, 환경은 <strong>새로운 관찰을 방출</strong>하거나 반응으로 <strong>보상신호</strong>를 제공할 수 있다.
      </li>
        <ul>
          <li>
            행동과 관련된 보상에 대한 함수를 <strong>보상 함수(reward function)</strong>이라 한다.
          </li>
          <li>
            <strong>전이와 보상에 대한 집합</strong>을 환경에 대한 <strong>모델(model)</strong>이라 표현한다.
          </li>
        </ul>
    </ul>
  <li>
    환경이 에이전트의 행동에 대해서 어떤 방법으로든(에이전트의 행동을 무시하는 방법으로라도) <strong>반응</strong>한다는 사실을 인지해야 한다.
  </li>
  <li>
    MDP에 놓여진 환경에서 상호작용하는 능력을 표현하기 위해서는 <strong>상태</strong>, <strong>관찰</strong>, <strong>행동</strong>, <strong>전이 함수</strong> 그리고 <strong>보상 함수</strong>가 필요하다.
  </li>
  <li>
    <strong>요약</strong>
  </li>
    <ul>
      <li>
        MDP(Markov Decision Process)는 에이전트가 환경과 상호작용하며 장기적인 보상을 최대화하기 위해 <strong>의사결정을 내리는 과정</strong>을 <strong>수학적으로 모델링</strong>한 것이다.
      </li>
      <li>
        MDP의 구송 요소는 네 가지로 <strong>상태, 행동, 전이, 보상</strong>으로 정의된다.
      </li>
      <li>
        MDP는 에이전트가 환경의 상태를 <strong>완전히 관찰</strong>할 수 있다고 가정한다. 그러나 실제 환경에서는 상태를 완전히 알 수 없는 경우가 많으며, 이때 에이전트는 상태로부터 생성된 <strong>관찰만을 이용해 의사결정</strong>을 수행한다. 이러한 상황을 모델링한 것이 <strong>POMDP</strong>이다.
      </li>
        <ul>
          <li>
            실제 환경은 센서 노이즈, 숨겨진 변수, 부분 관측 등으로 인해 에이전트가 환경의 상태를 완전히 알 수 없는 경우가 많다. 따라서 이러한 제약을 반영하기 위해 <strong>상태와 관찰을 분리</strong>하는 것이다.
          </li>
        </ul>
    </ul>
</ul>