<ul>
  <li>
    <strong>복잡성</strong>: 복잡한 상황을 정확하게 일반화하는 것을 학습하는 것은 결국 <strong>샘플링된 피드백 정보</strong>를 바탕으로 학습하기 때문에 어려움을 겪는다.
  </li>
  <li>
    <strong>순차성</strong>: 행동을 취한 결과가 나중에 나올 수 있다. 즉, 연속된 피드백으로 학습하는 상황에서 <strong>과거에 취했던 행동</strong>에 대해 가치를 정의하는 것은 어렵다.
  </li>
  <li>
    <strong>불확실성</strong>: <strong>세상이 돌아가는 원리</strong>를 모르는 상태에서 우리가 취한 행동이 어떻게 영향을 주는지 알 수 없다.
  </li>
  <li>
    이번 장부터 <strong>마프코프 결정 과정(Markov decision process, MDP)</strong>라는 수학적인 틀을 통해 위의 문제들을 나타내는 것에 대해 학습한다.
  </li>
    <ul>
      <li>
        강화학습의 <strong>핵심 요소</strong>에 대해 학습한다.
      </li>
      <li>
        <strong>마르코프 결정 과정(MDP)</strong>로 알려진 수학 프레임워크를 통해 강화학습 환경을 <strong>연속적인 의사결정 문제</strong>로 표현하는 방법을 학습한다.
      </li>
      <li>
        남은 장에서 강화학습 에이전트가 <strong>해결할 간단한 환경</strong>을 만든다.
      </li>
    </ul>
</ul>

<br>

<h1>1. 강화학습의 구성 요소</h1>