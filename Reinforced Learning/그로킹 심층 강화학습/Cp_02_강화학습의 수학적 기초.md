<ul>
  <li>
    <strong>복잡성</strong>: 복잡한 상황을 정확하게 일반화하는 것을 학습하는 것은 결국 <strong>샘플링된 피드백 정보</strong>를 바탕으로 학습하기 때문에 어려움을 겪는다.
  </li>
  <li>
    <strong>순차성</strong>: 행동을 취한 결과가 나중에 나올 수 있다. 즉, 연속된 피드백으로 학습하는 상황에서 <strong>과거에 취했던 행동</strong>에 대해 가치를 정의하는 것은 어렵다.
  </li>
  <li>
    <strong>불확실성</strong>: <strong>세상이 돌아가는 원리</strong>를 모르는 상태에서 우리가 취한 행동이 어떻게 영향을 주는지 알 수 없다.
  </li>
  <li>
    이번 장부터 <strong>마프코프 결정 과정(Markov decision process, MDP)</strong>라는 수학적인 틀을 통해 위의 문제들을 나타내는 것에 대해 학습한다.
  </li>
    <ul>
      <li>
        강화학습의 <strong>핵심 요소</strong>에 대해 학습한다.
      </li>
      <li>
        <strong>마르코프 결정 과정(MDP)</strong>로 알려진 수학 프레임워크를 통해 강화학습 환경을 <strong>연속적인 의사결정 문제</strong>로 표현하는 방법을 학습한다.
      </li>
      <li>
        남은 장에서 강화학습 에이전트가 <strong>해결할 간단한 환경</strong>을 만든다.
      </li>
    </ul>
</ul>

<br>

<h1>1. 강화학습의 구성 요소</h1>
<ul>
  <li>
    강화학습을 구성하는 두 개의 주요 구성 요소는 <strong>에이전트</strong>와 <strong>환경</strong>이다.
  </li>
    <ul>
      <li>
        <strong>에이전트</strong>: <strong>결정</strong>을 내리는 주체이며, 문제에 대한 <strong>답을 제공</strong>한다.
      </li>
      <li>
        <strong>환경</strong>: 문제를 대표하는 <strong>객체</strong>이다.
      </li>
    </ul>
  <li>
    강화학습이 다른 기계학습과 다른 점은 에이전트와 환경 사이에서 <strong>반응을 주고 받는다는 것</strong>이다.
  </li>
    <ul>
      <li>
        에이전트는 행동을 통해 <strong>환경에 영향</strong>을 주려하고, 반대로 환경은 에이전트의 <strong>행동에 대해 반응</strong>한다.
      </li>
    </ul>
</ul>

<br>

<h2>1-1. 문제, 에이전트, 환경에 대한 예시</h2>
<ul>
  <li>
    네 가지 상황을 에이전트, 환경, 행동 그리고 관찰로 정의하여 예를 보여준다.
  </li>
</ul>

<br>

<h2>1-2. 에이전트: 의사 결정권자</h2>
<ul>
  <li>
    에이전트는 강화학습의 큰 관점에서 <strong>결정을 내리는 요소</strong>이다.
  </li>
    <ul>
      <li>
        에이전트는 내부적으로 <strong>구성요소</strong>와 <strong>과정</strong>을 갖는데 이들은 <strong>특정 문제를 해결하는데 최적화</strong>되도록 만들어져 있다.
      </li>
    </ul>
  <li>
    대부분의 에이전트들은 다음 세 단계의 과정을 거친다.
  </li>
    <ul>
      <li>
        <strong>상호작용 요소</strong>를 통해 학습에 필요한 <strong>데이터를 수집</strong>한다.
        <br>→ 모든 에이전트들은 현재 취하고 있는 <strong>행동을 평가</strong>한다.
        <br>→ 전체적인 성능을 개선하기 위해 만들어진 무언가를 <strong>개선</strong>한다. 
      </li>
    </ul>
</ul>

<br>

<h2>1-3. 환경: 그 외의 모든 것</h2>
<ul>
  <li>
    실세상에서의 대부분의 의사-결정 문제들은 강화학습 환경으로 표현할 수 있으며 대표적으로 <strong>마르코프 결정 과정(Markov decision process, MDP)</strong>로 알려진 수학적 프레임워크를 통해 모델링을 한다.
  </li>
    <ul>
      <li>
        강화학습은 기본적으로 모든 환경이 이면에서 동작하는 MDP를 가지고 있다고 가정한다.
      </li>
    </ul>
  <li>
    환경은 문제와 관련된 <strong>변수들의 집합</strong>으로 표현된다.
  </li>
    <ul>
      <li>
        이렇게 표현할 수 있는 변수들의 모든 값에 대한 <strong>조합</strong>을 <strong>상태 영역(state space)</strong>이라 표현한다.
      </li>
    </ul>
  <li>
    에이전트는 어떤 방식으로든 환경으로 부터 무언가를 관찰할 수 있으며 이렇게 에이전트가 <strong>어떤 특정 시간에 얻을 수 있는 변수들의 집합</strong>을 <strong>관찰(observation)</strong>이라 한다.
  </li>
    <ul>
      <li>
        관찰의 변수들이 가질 수 있는 모든 조합을 <strong>관찰 영역(observation space)</strong>이라고 한다.
      </li>
    </ul>
  <li>
    강화학습에서는 환경이 완전히 관측 가능하면 상태와 관찰을 같은 의미로 쓰기도 하지만, 실제로는 <strong>관찰은 상태의 일부</strong>이거나 <strong>변형</strong>일 수 있다.
  </li>
    <ul>
      <li>
        즉, 항상 에이전트가 환경 내부 상태에 대해서 인지하는 것은 아니기 때문이다.
      </li>
    </ul>
  <li>
    매 상태에서 환경은 에이전트가 선택할 수 있는 여러 <strong>행동들의 집합</strong>을 만들어낸다. 모든 행동에 대한 집합은 <strong>행동 영역(action space)</strong>이라고 한다.
  </li>
  <li>
    환경은 에이전트의 행동에 반응해 <strong>상태를 변화</strong>시킨다. 현재 상태와 행동이 주어졌을 때 다음 상태로의 변화를 정의하는 함수를 <strong>전이함수(transition function)</strong>라 한다.
  </li>
    <ul>
      <li>
        전이가 발생한 후, 환경은 <strong>새로운 관찰을 방출</strong>하거나 반응으로 <strong>보상신호</strong>를 제공할 수 있다.
      </li>
        <ul>
          <li>
            행동과 관련된 보상에 대한 함수를 <strong>보상 함수(reward function)</strong>이라 한다.
          </li>
          <li>
            <strong>전이와 보상에 대한 집합</strong>을 환경에 대한 <strong>모델(model)</strong>이라 표현한다.
          </li>
        </ul>
    </ul>
  <li>
    환경이 에이전트의 행동에 대해서 어떤 방법으로든(에이전트의 행동을 무시하는 방법으로라도) <strong>반응</strong>한다는 사실을 인지해야 한다.
  </li>
  <li>
    MDP에 놓여진 환경에서 상호작용하는 능력을 표현하기 위해서는 <strong>상태</strong>, <strong>관찰</strong>, <strong>행동</strong>, <strong>전이 함수</strong> 그리고 <strong>보상 함수</strong>가 필요하다.
  </li>
  <li>
    <strong>요약</strong>
  </li>
    <ul>
      <li>
        MDP(Markov Decision Process)는 에이전트가 환경과 상호작용하며 장기적인 보상을 최대화하기 위해 <strong>의사결정을 내리는 과정</strong>을 <strong>수학적으로 모델링</strong>한 것이다.
      </li>
      <li>
        MDP의 구송 요소는 네 가지로 <strong>상태, 행동, 전이, 보상</strong>으로 정의된다.
      </li>
      <li>
        MDP는 에이전트가 환경의 상태를 <strong>완전히 관찰</strong>할 수 있다고 가정한다. 그러나 실제 환경에서는 상태를 완전히 알 수 없는 경우가 많으며, 이때 에이전트는 상태로부터 생성된 <strong>관찰만을 이용해 의사결정</strong>을 수행한다. 이러한 상황을 모델링한 것이 <strong>POMDP</strong>이다.
      </li>
        <ul>
          <li>
            실제 환경은 센서 노이즈, 숨겨진 변수, 부분 관측 등으로 인해 에이전트가 환경의 상태를 완전히 알 수 없는 경우가 많다. 따라서 이러한 제약을 반영하기 위해 <strong>상태와 관찰을 분리</strong>하는 것이다.
          </li>
        </ul>
    </ul>
</ul>

<br>

<h2>1-4. 에이전트와 환경 간의 상호작용 사이클</h2>
<ul>
  <li>
    보통 환경은 잘 정의된 <strong>업무(task)</strong>를 가지며 업무의 목표는 <strong>보상 신호</strong>를 통해서 정의된다.
  </li>
    <ul>
      <li>
        보상 신호는 <strong>밀집(dense)</strong>되어 있을 수도 있고, <strong>부족(sparse)</strong>할 수도 있고, 어쩌면 <strong>중간</strong>일 수도 있다.
      </li>
      <li>
        보상 신호가 에이전트를 원하는대로 <strong>학습시키는 수단</strong>이 된다.
      </li>
        <ul>
          <li>
            <strong>밀집된 보상 신호</strong>: 에이전트의 직관성과 학습속도가 높아지지만 편건(bias)를 주입하게 되어 예상하지 못할 행동을 할 가능성을 낮춘다. <strong>(학습 속도 ↑, 새로운 행동 가능성 ↓)<strong>
          </li>
          <li>
            <strong>부족한 보상 신호</strong>: 직관성이 낮아져 에이전트가 새로운 행동을 취할 확률이 높아지지만, 학습시키는데 시간이 더 필요하다. <strong>(학습 속도 ↓, 새로운 행동 가능성 ↑)</strong>
          </li>
        </ul>
    </ul>
  <li>
    에이전트와 환경 간의 상호작용은 수 사이클 진행된다. 이때 각 사이클을 <strong>타임 스탬(time step)</strong>이라 부른다.
  </li>
    <ul>
      <li>
        타임 스텝은 <strong>시간 단위</strong>이다.
      </li>
    </ul>
  <li>
    <strong>음의 값</strong>을 보상으로 받는다하여도 강화학습에서는 <strong>보상을 받는다</strong>고 표현한다.
  </li>
  <li>
    관찰(또는 상태), 행동, 보상 그리고 새로운 관찰(또는 새로운 상태)를 합쳐 <strong>경험 튜플(experience tuple)</strong>이라 부른다.
  </li>
  <li>
    에이전트의 업무는 자연적으로 끝날수도, 그렇지 않을 수도 있다. 보통 반환값을 최대화하게끔 설계된다.
  </li>
    <ul>
      <li>
        <strong>에피소드형 업무(episodic task)</strong>
      </li>
        <ul>
          <li>
            자연적으로 끝나는 업무
          </li>
          <li>
            처음 시작부터 끝까지의 타임 스텝을 <strong>에피소드(episode)</strong>라 한다.
          </li>
          <li>
            단일 에피소드 동안 수집된 <strong>보상의 총합</strong>을 <strong>반환값(return)</strong>이라 한다.
          </li>
        </ul>
      <li>
        <strong>연속형 업무(continuing task)</strong>
      </li>
        <ul>
          <li>
            자연적으로 끝나지 않는 업무
          </li>
          <li>
            연속형 업무는 <strong>종료 없이</strong> 매 타임 스텝마다 보상이 주어지며, 누적된 타임 스텝의 성과를 반환값으로 정의한다.
          </li>
        </ul>
    </ul>
  <li>
    모든 경험 튜플은 학습하고 성능을 향상시킬 기회를 가지며 에이전트는 학습을 보조하기 위한 한 개 이상의 요소들을 갖는다. 요소들에는 다음과 같은 것들이 있으며 각 요소를 학습하도록 설계한다.
  </li>
    <ul>
      <li>
        <strong>정책(policy)</strong>: 관찰과 행동의 관계
      </li>
      <li>
        <strong>모델(model)</strong>: 기존의 관찰과 새로운 관찰/보상의 관계
      </li>
      <li>
        <strong>가치 함수(value function)</strong>: 관찰(행동이 덧붙여지는 경우 포함)과 예정 보상(reward-to-go, 반환값의 일부)간의 관계
      </li>
    </ul>
</ul>

<br><br>

<h1>2. MDP: 환경의 엔진</h1>
<ul>
  <li>
    다른 사람이 만든 MDP를 사용할 수도 있지만 스스로 환경을 만들 수 있는 것은 중요한 기술이다.
  </li>
  <li>
    학습을 위해 정확한 MDP를 알 필요는 없지만, 일반적으로 에이전트들은 MDP 안에서 동작한다고 가정하고 설계되기에 사실상 아는 것이 중요하다.
  </li>
  <li>
    <strong>MDP 구현 실습 예제 설명 (프로즌 레이크, frozen lake, FL)</strong>
  </li>
    <ul>
      <li>
        격자판에 구멍과 목표지점이 있다.
      </li>
      <li>
        에이전트는 매 에피소드마다 동일한 시작 지점에서 출발한다.
      </li>
      <li>
        구멍에 빠지면 에피소드를 즉시 종료하고 새로운 에피소드를 시작한다.
      </li>
      <li>
        목표 지점에 도달하면 +1 보상을 받는다.
      </li>
      <li>
        상, 하, 좌, 우로 이동하는 행동을 취할 수 있으나 1/3 확률로 의도대로 움직이고, 나머지 각각 1/3 확률로 의도한 방향의 수직으로 이동한다.
      </li>
    </ul>
</ul>

<br>

<h2>2-1. 상태: 환경의 특정한 설정들</h2>
<ul>
  <li>
    <strong>상태(state)</strong>는 문제에 포함되어 있는 독특하고, <strong>자기 자신만의 설정이 담긴 요소</strong>이다. 
  </li>
    <ul>
      <li>
        모든 가능한 상태들을 <strong>상태 영역(state space)</strong>이라하며 <strong>집합 S</strong>로 표기한다.
      </li>
      <li>
        상태 영역은 무한할 수도, 유한할 수도 있다.
      </li>
      <li>
        상태 영역과 단일 상태를 나타내는 변수들의 집합은 다르다.
      </li>
        <ul>
          <li>
            단일 상태를 나타내는 변수들의 집합은 항상 <strong>유한</strong>하고, <strong>상태와 상태 간의 크기</strong>가 정해져 있어야 한다.
          </li>
          <li>
            위와 같은 단일 상태의 특징을 바탕으로 할 때 상태 영역은 <strong>합에 대한 집합</strong>이다.
          </li>
            <ul>
              <li>
                <strong>내부 집합(단일 상태)</strong>은 변수들의 개수를 포함하기에 변수들의 개수는 <strong>항상 같으며 유한</strong>해야 한다.
              </li>
              <li>
                반면 <strong>외부 집합</strong>은 내부 집합에 포함된 요소들의 형태에 따라 <strong>무한할 수도 있다</strong>.
              </li>
            </ul>
        </ul>
    </ul>
  <li>
    상태는 <strong>이산 변수</strong> 혹은 <strong>연속 변수</strong> 있을 수 있으며 <strong>여러 개의 변수</strong>로 하나의 상태를 정의할 수도 있다.
  </li>
  <li>
    예제의 밴딧은 <strong>(의도한 행동 대비 전이 경우의 수, 실제 전이 결과, 상태(위치))</strong>로 <strong>(3, 3, 16)</strong>으로 표현된다.
  </li>
    <ul>
      <li>
        <strong>3</strong>: 의도한 방향을 기준으로 발생 가능한 <strong>전이 결과 유형의 수</strong>를 의미한다.
      </li>
      <li>
        <strong>3</strong>: 실제로 발생한 전이
      </li>
      <li>
        <strong>16</strong>: 현재 위치
      </li>
    </ul>
  <li>
    <strong>MDP</strong>는 상태를 <strong>완전히 관찰(fully observable)</strong>할 수 있어야 한다. 즉, 매 타임 스템마다 <strong>내부의 상태</strong>를 볼 수 있음을 의미하며 <strong>관찰과 상태가 동일</strong>하다.
  </li>
    <ul>
      <li>
        현재 밴딧 통로 환경은 MDP를 만들기에 에이전트는 환경 내부의 상태를 관찰할 수 있다.
      </li>
    </ul>
  <li>
    <strong>부분적으로만 관찰할 수 있는 마르코프 결정 과정(partial observable Markov decision processes, POMDPs)</strong>는 딱 에이전트가 <strong>관찰할 수 있는 상태</strong>만 다룬다. 즉, 관찰과 상태가 동일하지 않으며 내부 상태를 관찰할 수 없다.
  </li>
  <li>
    상태는 다른 상태와 구분될 수 있는 <strong>독립적인 형태의 모든 변수</strong>를 포함해야 한다. 예를 들어 다음 타임 스텝에 <strong>필요한 현재의 상태</strong>만 알고 있으면 된다.
  </li>
    <ul>
      <li>
        MDP에서 <strong>무기억성(memoryless) 특성</strong>은 <strong>마르코프 특성(Markov property)</strong>이라고 알려져있으며 행동 a를 했을 때 어떤 상태 s에서 다른 상태 s로 움직일 확률은 <strong>이전의 상태나 행동이 어떻게 취해졌는지와 상관없이 항상 같음</strong>을 의미한다.
      </li>
    </ul>
  <li>
    대부분의 강화학습 에이전트들은 마르코프 가정하에서 잘 동작하도록 설계 되었기 때문에, 마르코프 특성이 최대한 잘 유지될 수 있도록 필요한 변수들을 에이전트에게 알려줘야 한다.
  </li>
  <li>
    MDP에 속하는 모든 상태들의 집합을 S+라 하자. 보통 <strong>시작 상태(starting state)</strong> 혹은 <strong>초기 상태(initial state)</strong>라고 부르는 <strong>S+ 부분집합</strong>이 있다.
  </li>
    <ul>
      <li>
        우리는 어떤 S+에서의 특정 상태에서 어떤 확률 분포 간의 관계를 그릴 수 있다.
      </li>
      <li>
        이때 확률 분포는 무엇이든 될 수 있지만 학습이 이루어지는 동안에는 <strong>고정</strong>되어 있어야 한다. 즉, 해당 확률 분포에서 샘플링된 확률은 학습과 에이전트 검정의 <strong>처음 에피소드부터 마지막 에피소드까지는 항상 동일</strong>해야 한다.
      </li>
    </ul>
  <li>
    시작 상태 외에도 <strong>흡수(absorbing)</strong> 또는 <strong>종료 상태(terminal state)</strong>라는 특별한 상태가 있다. 
  </li>
    <ul>
      <li>
        종료 상태 이외의 모든 상태들은 <strong>S</strong>라고 표현한다.
      </li>
      <li>
        전이를 통해 바로 종료 상태로 가는 <strong>단일 종료 상태</strong>를 만드는 것이 일반적이지만 항상 그런 것은 아니다.
      </li>
      <li>
        종료 상태는 여럿일 수 있으며 종료 상태만 생각대로 구현되면 동작하는 원리는 상관없다.
      </li>
        <ul>
          <li>
            종료 상태는 특별한 상태이다.
          </li>
            <ul>
              <li>
                종료 상태에서 취할 수 있는 모든 행동들은 100% 확률로 <strong>현재 상태</strong>에 머무른다.
              </li>
              <li>
                종료 상태에서 전이되는 행동은 <strong>보상을 받지 않아야 한다</strong>.
              </li>
            </ul>
        </ul>
    </ul>
  <li>
    에피소드의 마지막에는 0이 아닌 보상을 주는 경우가 흔한데 이는 학습을 마치며 0이 아닌 결과(1: 승리, -1: 패배 등)를 얻기 위함이다. 만약 0이라면 알고리즘은 결과를 알 수 없기 때문이다.
  </li>
    <ul>
      <li>
        즉, 에이전트에게 <strong>결과 피드백</strong>을 주어야 하기에 0이 아닌 보상이 많은 것이다.
      </li>
    </ul>
</ul>

<br>

<p>

☆ 마르코프 특성을 수식으로 보면 다음과 같다. 즉, 현태 상태($S_t, A_t$) 기준의 다음 상태($S_{t+1}$)의 확률은 이전 모든 상태를 조건부로 확률로 하여도 동일하다. (이전의 상태와 행동의 영향을 받지 않는다).

$$ P(S_{t+1}|S_t,A_t)=P(S_{t+1}|S_t,A_t,S_{t-1},A_{t-1},...) $$

</p>

<br>

<h2>2-2. 행동: 환경에 영향을 끼치는 메커니즘</h2>
<ul>
  <li>
    MDP에서는 <strong>상태</strong>에 따라 결정되는 <strong>행동의 집합 A</strong>가 정의된다.
  </li>
    <ul>
      <li>
        즉, 특정 상태에서는 허용되지 않는 행동도 있다.
      </li>
      <li>
        행동의 집합 A는 <strong>상태를 인자로 받는 함수</strong>이며 다시 표현하면 A(s)로 표현할 수 있다. 즉, 상태 s에서 취할수 있는 행동들의 집합을 반환한다.
      </li>
      <li>
        필요하다면 상태 영역에 대한 행동들의 집합을 <strong>상수</strong>로 정의할 수 있다. 
      </li>
        <ul>
          <li>
            주어진 상태에서 행동을 수행할 수 없게 만들기 위해서는 상태-행동 짝에서의 <strong>모든 전이를 0</strong>으로 고정할 수 있다.
          </li>
          <li>
            어떤 상태 s와 어떤 행동 a에 대해서 <strong>같은 상태 s</strong>로 전이될 때 행동 a를 <strong>무간섭(no-intervene)</strong> 혹은 <strong>무동작(no-op)</strong> 행동으로 고정시키도록 모든 전이를 정할 수도 있다.
          </li>
        </ul>
    </ul>
  <li>
    <strong>행동 영역</strong> 또한 다음과 같은 특징을 갖는다.
  </li>
    <ul>
      <li>
        상태와 마찬가지로 <strong>유한하거나 무한</strong>할 수 있다.
      </li>
      <li>
        단일 행동에 대한 변수들의 집합은 <strong>한 개 이상의 요소</strong>를 가질 수도 있으며 이 수는 <strong>유한</strong>해야 한다.
      </li>
      <li>
        단, 상태 변수의 수와 다르게 행동을 나타내는 변수의 개수는 상수가 아닐 수 있다. 어떤 상태에서 취할 수 있는 행동은 <strong>상태</strong>에 따라 달라진다. 대부분의 환경은 모든 상태에서 취할 수 있는 행동의 수는 <strong>같도록</strong> 설계 되었다.
      </li>
    </ul>
  <li>
    환경은 미리 취할 수 있는 모든 행동을 <strong>알 수 있도록</strong> 만들어져 있다.
  </li>
    <ul>
      <li>
        에이전트는 <strong>결정적</strong>으로 혹은 <strong>확률적</strong>으로 <strong>행동을 선택</strong>한다.
      </li>
        <ul>
          <li>
            행동을 선택한다는 것은 에이전트의 행동에 따라 환경이 결정적 혹은 확률적으로 <strong>상호작용하는 것과는 다르다</strong>.
          </li>
          <li>
            즉, 에이전트는 <strong>조회표(lookup table)</strong>에서 행동을 결정할 수도, <strong>상태에 따른 확률 분포</strong>에서 행동을 결정할 수도 있다.
          </li>
        </ul>
    </ul>
  <li>
    다시 밴딧 예제
  </li>
    <ul>
      <li>
        현재 밴딧 통로, 미끄러지는 밴딧 통로 그리고 프로즌 레이크 환경에서의 핻옹은 에이전트가 움직이고자하는 방향을 나타내는 <strong>단일 개체(singleton)</strong>이다. 
      </li>
        <ul>
          <li>
            단일 개체란 행동 집합에서 <strong>하나의 행동만을 선택</strong>하는 것을 의미한다.
          </li>
        </ul>
      <li>
        프로즌 레이크에서 모든 상태에서 취할 수 있는 행동은 상, 하, 좌, 우 네 가지로 행동은 <strong>하나의 변수</strong>이고, 변수가 가질 수 있는 <strong>값의 개수 즉 행동 영역의 크기는 4(상, 하, 좌, 우)</strong> 이다.
      </li>
    </ul>
</ul>

<br>

## 2-3. 전이 함수: 에이전트의 행동에 대한 결과
- 행동에 대한 환경의 변화를 정의한 것을 **상태-전이 확률(state-transition probability)** 혹은 간단히 **전이 함수**라 한다.
- 전이 함수는 **$ T(s, a, s') $**로 표현한다. 혹은 **$ T(s, a) $**로 표현하기 도한다.
  - 어떤 상태 $ s $에서 행동 $ a $를 취하고 다음 상태 $ s' $이 되었을 때, **해당 확률을 반환**해준다는 의미이다.
  - 다음 상태와 상태에 대한 확률을 **딕셔너리 형태**로 정의하여 반환하기도 한다.
- $ T $ 함수에서 알아둘 것은 상태 $ s $에서 행동 $ a $를 취했을 때, 시스템이 어떻게 상호작용을 통해 변화하는지를 확률 분포 $ p(\cdot|s,a) $로 나타낼 수 있다는 점이다.
  - 어떤 확률 분포를 띄더라도 상태 $ s' $에 대한 모든 확률을 더하면 **1**이 된다.
- 전이 함수를 수식으로 표현하면 다음과 같다. 이전 타임 스템 t-1에서 상태 s, 행동 a일 때 타임 스텝 t의 상태가 s'로 전이될 확률을 의미한다.
$$ p(s'|s,a) = P(S_t = s'|S_{t-1}=s,A_{t-1}=a) $$
- 모든 확률이 1임은 다음의 수식을 통해 확인할 수 있다. 상태 S의 집합에 속하는 모든 상태 s와 행동 a의 전이 상태 s'의 확률의 합은 1이다.
  - $ \forall $: 모든, 임의의의 의미를 갖는다.
$$ \sum_{s'\in S}p(s'|s,a)=1, \forall_s\in S, \forall_a\in A(s)$$
- 대부분의 강화학습 알고리즘이 가정하는 핵심 요소 중 하나는 위 **분포**가 변화가 없는 **고정상태(stationary)**라는 것이다.
  - 확률적으로 전이가 발생할 확률이 높다 하더라도 이에 대한 확률 분포는 학습 및 검증 동안 변화하지 않는다.
  - 즉, 에이전트들은 적어도 **변화가 없다는 전제가 깔려있는 환경**과 상호 작용을 한다는 중요한 사실이다.

<br>

## 2-4. 보상 신호: 당근과 채찍
- 보상 함수 $ R $은 전이 튜플 $ s, a, s' $을 특정한 **스칼라 값**으로 매핑한다. 즉, 전이에 대한 좋고 나쁨의 정도를 숫자로 나타낸다.
  - 신호가 **양수**라면 보상을 **수익 또는 이득**이라 생각할 수 있다.
    - 대부분의 문제는 **적어도 한 개 이상의 양수 신호**를 갖는다.
  - 신호는 **음수**일 수 있으며 음수 보상을 **비용, 벌칙, 페널티 등**으로 볼 수 있다.
- 양수 및 음수와 관계없이 보상 함수의 출력으로 나오는 스칼라 값은 항상 **보상(reward)**으로 간주된다.
- 보상 함수는 다음과 같이 여러 표현 방법이 존재한다. 단, 가장 명확한 방법은 $ R(s,a,s') $ 이다.
  - $ R(s) $: 상태에 기반해 에이전트에게 보상을 주는 것을 희망.
  - $ R(s, a) $: 행동과 상태를 활용.
  - $ R(s, a, s') $: 상태와 행동 그리고 다음 상태를 모두 기재하는 방법으로 가장 명확하다.
    - 예를 들어 $ R(s, a) $를 구하기 위해 $ R(s, a, s') $ 상에서 갈 수 있는 **다음 상태에 대한 여백(marginalization)**을 구할 수 있다.
    - 혹은 $ R(s) $를 구하기 위해 $ R(s,a) $에서 취할 수 있는 **행동들에 대한 여백**도 계산할 수 있다.
    - 단, 한 번 $ R(s) $ 혹은 $ R(s, a) $를 구하게 되면 이전의 $ R(s, a, s') $를 **복구할 수 없다**.
- 보상 함수를 수식으로 표현하면 다음과 같다. 이전 타입 스텝의 상태 s와 해동 a에 따른 현재 타임 스텝 t의 보상의 기대값이다.
$$ r(s,a)=\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a] $$
- 전체 전이 튜플을 인자로 받는 함수로 정의하면 다음과 같다. 타임스텝 t에서의 보상은 모든 보상에 대한 **집합 R의 부분집합**니다. 또, 집합 R은 **실수의 부분집합**니다. 
  - 즉, 전체 보상 집합 $ \mathbb R $에서 후보 보상들 $ R $ 중 t 시점의 보상이 $ R_t $ 이다. 
$$ r(s, a, s') =\mathbb{E}[R_t|S_{t-1}=s,A_{t-1}=a,S_t=s') \quad R_t\in R \subset \mathbb R$$
- 전이와 보상 함수에 대해 표로 표현하는 것 또한 도움이 된다.
  표에는 상태($ s $), 행동($ a $), 다음 상태($ s' $), 전이 확률($ p $), 보상 신호($ r $)가 포함된다.

<br>

## 2-5. 호라이즌: 이상적인 방향으로의 시간의 변화
- MDP는 **시간**을 표현할 수 있다.
  - 시간은 타임 스텝, 에포크(epoch), 사이클(cycle), 반복 횟수(iteration)으로 표현할 수 있다.
  - 각 **집합**과 **이산적인 시간 사이를 동기화** 시켜주는 전역적인 시계를 의미한다.
- 시간적 관점의 두 업무
  - **에프소드형(episodic) 업무**는 시계가 멈추거나 에이전트가 종료 상태에 도달하는 등 **유한한 타임 스탭**을 갖고 있는 업무를 의미한다. 
  - 반면 **연속형 업무**는 업무가 지속되며 종료 상태 또한 존재하지 않아 **타임 스텝이 무한**하며 에이전트를 임의로 멈춰줘야 한다.
- **에이전트 관점**에서의 정의는 다음과 같으며 이를 **계획 호라이즌(planning horizon)**이라고 한다.
  - **유한 호라이즌(finite horizon)**: 에이전트가 유한한 타임 스텝내에 **업무가 종료된다는 것을 알고 있는** 계획 호라이즌.
  - **탐욕 호라이즌(greedy horizon)**: 계획 호라이즌이 **1**인 경우를 의미한다. 즉, 현재 상태의 **한 단계 뒤의 결과만 고려**하며 이후는 고려하지 않는 탐욕적 방식이다.
  - **무한 호라이즌(infinite horizon)**: 에이전트에게 미리 정의된 타임 스텝에 대한 제한이 없어 **무한하게 계획**할 수 있다. 에피소드가 종료가 된다 하더라도 에이전트의 관점에서 계획 호라이즌은 무한할 수 있다.
    - 무한한 계획 호라이즌을 갖는 업무를 무기한 호라이즌 업무(indefinite horizon task)라 한다.
    - 에이전트는 무한정으로 계획할 수 있지만, 상호작용은 환경에 의해서 어느 시점에서든지 멈출 수 있다.
- 에이전트가 루프에 빠질 가능성이 높기에 절대 종료되지 않는 업무에서는 타임 스텝을 기반으로 **임의의 종료 상태를 추가**하는 방법을 많이 사용한다.
- **에피소드형 업무**에서 시작부터 끝까지 연속적인 타임 스텝을 **에피소드(episode), 시도(tiral), 주기(period), 단계(stage)**라 표현한다.
- **무한 계획 호라이즌** 상에서의 에피소드는 **초기 상태와 종료 상태 사이에 발생한 모든 상호작용**을 모아둔 **집합**이다.

<br>

## 2-6. 감가: 불확실한 미래에 대한 가치 절감
- 무한 호라이즌 업무에서의 연속된 **타임 스템이 무한**할 수 있어 시간에 따라 **보상의 가치를 줄이는 방법**이 필요하다.
  - 구체적으로는 먼 미래 시점보다 **가까운 미래 시점의 보상**에 더 가치를 주어야 한다.
  - 보통 미래에 얻을 수 있는 보상의 가치를 기하급수적으로 줄이기 위해 **1보다 작은 양의 실수**를 사용한다.
- 가치를 더 낮게 평가하는 수치를 **감가율(discount factor)** 혹은 감마(gamma)라 부른다.
  - 감가율은 **시간**에 따라 보상의 중요도를 조절한다. 보상을 늦게 받을수록 현재의 가치를 계산할 때 영향력이 적어지게 된다.
- 감가율을 자주 사용하는 또 다른 이유는 예측된 반환값에 대한 **분산을 줄이기 위해서**이다.
  - 미래가 불확실할 때 더 먼 미래를 보는 것은 불안도를 높이기에 이를 방지한다.