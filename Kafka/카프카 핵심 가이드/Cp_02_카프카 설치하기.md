<ul>
  <li>
    이번 장은 Kafka 환경 설정에 대해서 다루기에 대부분의 내용은 생략하고 <strong>설정과 관련</strong>된 내용만 기록한다.
  </li>
</ul>

<h1>1. 환경 설정</h1>
<h2>1-1. 운영체제 선택하기</h2>
<ul>
  <li>
    OS에 적합한 환경을 선택하여 진행한다.
  </li>
</ul>

<br>

<h2>1-2. 자바 설치하기</h2>
<ul>
  <li>
    Ubuntu를 활용하여 linux 환경에서 경로를 생성하고 도서에서 지정된 버전의 JDK를 다운 받는다.
  </li>
  <li>
    linux 환경을 사용하기에 권한 설정이 필요하다.
  </li>
  <li>
    도서의 경우 /usr/java/jdk-11.0.10에 다운 받고, JAVA_HOME 경로를 변경하는 방법을 활용하는데 <strong>기본적으로 세팅되어 있는 경로(/usr/lib/java)</strong>에 JDK를 다운 받아서 활용해도 무관하다.
  </li>
</ul>

<br>

<h2>1-3. 주키퍼 설치하기</h2>
<h3>1-3-1. 독립 실행 서버</h3>

```bash
# 다운 받은 Kafka의 압축을 해제한다.
tar -zxf apache-zookeeper-3.5.9-bin.tar.gz

# 압축 해제한 폴더명을 'apache-zookeeper-3.5.9-bin → zookeeper'로 수정한다.
mv apache-zookeeper-3.5.9-bin /usr/local/zookeeper

# 필요한 폴더 생성
mkdir -p /var/lib/zookeeper

# zookeeper 설정 설정. (도서는 cat을 사용하는데 권한 문제로 tee를 사용해야할 수도 있다).
cat > /usr/local/zookeeper/conf/zoo.cfg << EOF
tickTime=2000
dataDir=/var/lib/zookeeper
clientPort=2181
EOF

# 기본 자파 경로를 설정하는 것인데 원래 환경을 사용할 것이라면 진행하지 않도록 한다.
export JAVA_HOME=/usr/java/jdk-11.0.19

# 주키퍼를 실행한다.
/usr/local/zookeeper/bin/zkServer.sh start

# 주키퍼의 정보를 조회한다. (telnet이 없다면 다운받기).
# 주키퍼의 포트인 2181로 확인한다.
telnet localhost 2181
```

<h3>1-3-2. 주키퍼 앙상블</h3>
<ul>
  <li>
    zookeeper는 고가용성을 보장하기 위해 <strong>앙상블(ensemble)</strong>이라 불리는 <strong>클러스터 단위</strong>로 작동하도록 설계되었다.
  </li>
    <ul>
      <li>
        zookeeper가 사용하는 부하 분산 알고리즘 때문에 앙상블은 <strong>홀수 개의 서버</strong>를 가지는 것이 권장된다.
      </li>
        <ul>
          <li>
            zookeeper가 요청에 응답하려면 앙상블 멤버(quorum)의 <strong>과반 이상이 작동</strong>하고 있어야하기 때문이다.
          </li>
        </ul>
      <li>
        zookeeper 앙상블을 구성할 때에는 <strong>5개</strong>의 노드 크기를 고려한다.
      </li>
        <ul>
          <li>
            <strong>정비 작업</strong>을 수행할 시에 2대 이상의 노드 정지를 받아낼 수 있어야 한다.
          </li>
          <li>
            9대 이상의 노드는 <strong>합의(consensus) 프로토콜</strong> 특성상 성능이 나빠진다.
          </li>
          <li>
            클라이언트가 너무 많아 5대 혹은 7대의 노드가 부하를 감당하기 힘들다면 <strong>옵저버 노드</strong>를 추가하여 읽기 전용 트래픽을 분산 시킬 수 있다.
          </li>
        </ul>
    </ul>
  <li>
    주키퍼 서버를 앙상블로 구성하기 위해서는 다음 두 가지가 필요하다.
  </li>
    <ul>
      <li>
        각 서버는 <strong>공통된 설정 파일</strong>을 사용해야 한다.
      </li>
        <ul>
          <li>
            설정 파일에는 앙상블에 포함된 모든 서버의 목록이 포함되어야 한다.
          </li>
          <li>
            각 서버는 데이터 디렉토리에 자신의 ID 번호를 지정하는 myid 파일을 가지고 있어야 한다.
          </li>
          <li>
            설정에서 initLimit 값은 팔로워가 리더와의 연결할 수 있는 초기화 제한 시간을, syncLimit은 팔로워와 리더가 연결할 수 있는 동기화 제한 시간을 의미한다.
          </li>
            <ul>
              <li>
                위 두 값은 모두 tickTime 단위로 정의된다.
              </li>
            </ul>
          <li>
            설정은 앙상블 안의 모든 서버 내역 <strong>{X}={hostname}:{peerPort}:{leaderPort}</strong> 또한 정의한다. 
          </li>
            <ul>
              <li>
                <strong>X</strong>: 서버의 ID. 정숫값이어야 하지만 0부터 시작할 필요가 없고, 순차적일 필요도 없다.
              </li>
              <li>
                <strong>hostname</strong>: 서버의 호스트명 또는 IP 주소
              </li>
              <li>
                <strong>peerPort</strong>: 앙상블 안의 <strong>서버들이 서로 통신</strong>할 때 사용하는 TCP 포트 번호
              </li>
              <li>
                <strong>leaderPort</strong>: <strong>리더를 선출</strong>하는 데 사용되는 TCP 포트 번호
              </li>
            </ul>
          <li>
            클라이언트는 clientPort에 지정된 포트 번호로 앙상블에 연결할 수 있으면 되지만, 앙상블의 멤버들은 <strong>세 포트를 모두</strong> 사용해서 서로 통신할 수 있어야 한다.
          </li>
        </ul>
      <li>
        공통 설정 파일 외에도 각 서버는 dataDir 디렉토리에 <strong>myid</strong>라는 이름의 파일을 가지고 있어야 한다.
      </li>
        <ul>
          <li>
            파일은 설정 파일에 지정된 것과 일치하는 서버 ID 번호를 포함해야 한다.
          </li>
        </ul>
    </ul>
</ul>

<h4>한 대의 서버에서 주키퍼 앙상블 테스트하기</h4>
<ul>
  <li>
    설정 파일의 모든 호스트명을 <strong>localhost</strong>로 지정하고 모든 peerPort, leaderPort에 <strong>서로 다른 포트</strong>를 할당하여 하나의 서버에서 주키퍼 앙상블을 실행하고 테스트할 수 있다.
  </li>
  <li>
    각각의 주키퍼 인스턴스에 서로 다른 dataDir와 clientPort를 할당하기 위해서 <strong>추가적인 zoo.cfg</strong>를 생성해야 할 수도 있다.
  </li>
  <li>
    테스트 방법은 테스트 환경에서는 유용하겠으나, 프로덕션 시스템에는 권장하지 않는다.
  </li>
</ul>

<br><br>

<h1>2. 카프카 프로커 설치하기</h1>
<ul>
  <li>
    자바와 주키퍼가 설정되어 있다면 이제 Kafka를 설치할 준비가 된 것이다. (https://kafka.apache.org/downloads.html, 교재는 2.7.0 사용).
  </li>
  <li>
    구버전에서 사용하던 <strong>--zookeeper</strong> 연결 문자열은 거의 모든 경우 지원이 중단 되었으며, 대체재는 <strong>--bootstrap-server</strong>를 사용해서 kafka 브로커에 직접 연결하는 방법이 있다.
  </li>
</ul>

```bash
# 생성한 /usr/local/로 이동후 
sudo wget https://archive.apache.org/dist/kafka/2.7.0/kafka_2.12-2.7.0.tgz

# 받은 Kafka 파일 압축 해제
sudo tar -zxf kafka_2.13-2.7.0.tgz

# 해제한 파일명을 'kafka_2.12-2.7.0.tgz → /usr/local/kafka'로 변경
sudo mv kafka_2.12-2.7.0 /usr/local/kafka

# kafka로그를 저장할 파일 생성.
sudo mkdir /tmp/kafka-logs

# 자바 기본 경로 변경. (기본 환경을 사용한다면 설정 X).
sudo export JAVA_HOME=/usr/java/jdk-11.0.10

# kafka 실행.
sudo /usr/local/kafka/bin/kafka-server-start.sh -daemon \
    /usr/local/kafka/config/server.properties

# topic을 생성하여 kafka가 실행 중인지 테스트
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \
    --create --replication-factor 1 --partitions 1 --topic test

# 생성한 topic에 대한 정보 확인
/usr/local/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 \
    --describe --topic test

# topic에 메시지 쓰기
/usr/local/kafka/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test
Test Message 1
Test Message 2
# Ctrl + C를 눌러 종료

# 쓴 메시지 읽기
/usr/local/kafka/bin/kafka-console-consuer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
# Ctrl + C를 눌러 종료
```

<br><br>

<h1>3. 브로커 설정하기</h1>
<ul>
  <li>
    Kafka의 배포판 설정은 개념 증명(Proof of Concept, PoC)를 목적으로 잘 구성되어 있다.
  </li>
  <li>
    규모가 커질 경우 튜닝을 해야할 수도 있지만, 일반적인 경우는 바꿀 일이 없기에 기본값을 사용해도 된다.
  </li>
</ul>

<br>

<h2>3-1. 브로커 설정하기</h2>
<ul>
  <li>
    단일 서버에서 단독으로 실행되는 브로커가 아니라면 다음의 매개변수들을 설정해 주어야 한다.
  </li>
</ul>

<h3>3-1-1. broker.id</h3>
<ul>
  <li>
    모든 Kafka 브로커는 <strong>정숫값 식별자</strong>를 가지며 <strong>broker.id</strong>로 설정할 수 있다.
  </li>
  <li>
    기본값은 0이며 클러스터 내의 다른 브로커들과는 달라야 한다.
  </li>
  <li>
    Broker id는 <strong>임의로 선택</strong>이 가능하고 필요하다면 Broker간 데이터와 역할 이동도 가능하지만, Broker id는 <strong>호스트별로 고정된 값</strong>을 사용하는 것이 강력하게 권장된다.
  </li>
    <ul>
      <li>
        host1.example.com → broker.id = 1, host2.example.com → broker.id = 2와 같이 호스트 도메인의 숫자와 borker.id의 숫자를 맞추어 준다.
      </li>
    </ul>
</ul>

<h3>3-1-2. listeners</h3>
<ul>
  <li>
    기존에 port를 사용하는 방법은 중단되었다. 대신 <strong>9092 TCP 포트</strong>에서 돌아가는 <strong>listener</strong>와 함께 Kafka를 실행시킨다.
  </li>
  <li>
    listeners 설정은 쉼표로 구분된 <strong>리스터 이름</strong>과 <strong>URI 목록</strong>이다.
  </li>
    <ul>
      <li>
        일반적인 보안 프로토콜이 아닌 경우 반드시 listeners.security.protocol.map 설정을 잡아주어야 한다.
      </li>
      <li>
        Listener는 <strong>{프로토콜}://{호스트 이름}:{포트}</strong>의 형태로 정의된다.
      </li>
        <ul>
          <li>
            PLAINTEXT://localhost:9092
          </li>
            <ul>
              <li>
                localhost의 9092로 들어오는 데이터(메시지, 메타데이터 등)를 PLAINTEXT로 듣고 있음.
              </li>
            </ul>
          <li>
            SSL://:9091
          </li>
            <ul>
              <li>
                모든 인터페이스(0.0.0.0 → :로 생략됨)에서 들어오는 정보를 SSL로 암호화하여 듣고 있음.
              </li>
            </ul>
        </ul>
      <li>
        호스트 이름을 <strong>0.0.0.0(생략되기도 함)</strong>으로 잡을 경우 <strong>모든 네트워크 인터페이스</strong>로부터 연결을 받게 된다.
      </li>
    </ul>
  <li>
    listeners 설정을 하지 않는 경우 <strong>기본 인터페이스</strong>에 대해서만 연결을 받는다.
  </li>
  <li>
    <strong>1024 미만의 포트</strong>를 사용하는 것은 권장하지 않으며, 만약 사용한다 하더라도 루트 권한으로 Kafka를 실행시켜야 한다.
  </li>
</ul>

<h3>3-1-3. zookeeper.connect</h3>
<ul>
  <li>
    zookeeper.connect는 <strong>브로커의 메타데이터</strong>가 저장되는 <strong>주키퍼의 위치</strong>를 가리킨다.
  </li>
  <li>
    세미콜론으로 연결된 <strong>{호스트 이름}:{포트}/{경로}</strong>의 목록 형식으로 지정할 수 있다.
  </li>
    <ul>
      <li>
        <strong>호스트 이름</strong>: 주키퍼 서버의 호스트 이름이나 IP 주소
      </li>
      <li>
        <strong>포트</strong>: 주키퍼의 클라이언트 포트 번호
      </li>
      <li>
        <strong>/{경로}</strong>: 선택 사항으로 Kafka 클러스터의 chroot 환경으로 사용될 주키퍼의 경로이다. 지정하지 않으면 루트 디렉터리가 사용된다.
      </li>
        <ul>
          <li>
            <strong>chroot</strong>
          </li>
            <ul>
              <li>
                Zookeeper의 내부 디렉터리 구조 중 <strong>어디부터 루트처럼</strong> 사용할지 지정하는 옵션이다.
              </li>
              <li>
                대체로 chroot를 사용하면 다른 애플리케이션과 충돌할 일이 없이 주키퍼 앙상블을 공유해서 사용할 수 있다.
              </li>
              <li>
                같은 앙상블에서 속하는 다수의 주키퍼 서버를 지정하면 특정 서버에 장애가 발생하여도 Kafka 브로커가 같은 주키퍼 앙상블을 <strong>다른 주키퍼 서버</strong>에 연결할 수 있어 대응이 가능하다.
              </li>
            </ul>
        </ul>
    </ul>
</ul>

<h3>3-1-4. log.dirs</h3>
<ul>
  <li>
    Kafka는 모든 메시지를 <strong>로그 세그먼트(log segment)</strong> 단위로 묶어 <strong>log.dir</strong> 설정에 지정된 <strong>디스크 디렉토리</strong>에 저장한다.
  </li>
  <li>
    <strong>다수의 디렉토리</strong>를 지정하고자 할 경우 <strong>log.dirs</strong>를 사용하는 것이 좋다.
  </li>
    <ul>
      <li>
        <strong>log.dirs</strong>는 쉼표로 구분된 <strong>로컬 시스템 경로의 목록</strong>이다.
      </li>
    </ul>
  <li>
    한 개 이상의 경로가 지정될 경우 Broker는 <strong>가장 적은 수</strong>의 파티션이 지정된 디렉토리에 새 파티션을 저장한다.
  </li>
    <ul>
      <li>
        사용된 디스크의 용량이 아닌 <strong>저장된 파티션의 수</strong>를 기준으로 파티션의 저장 위치를 배정하기에 <strong>균등한 양의 데이터</strong>가 저장되지 않는다.
      </li>
    </ul>
</ul>

<h3>3-1-5. num.recovery.threads.per.data.dir</h3>
<ul>
  <li>
    Kafka는 설정 가능한 <strong>스레드 풀</strong>을 사용해서 로그 세그먼트를 관리한다. 스레드 풀은 아래와 같은 작업을 수행한다.
  </li>
    <ul>
      <li>
        브로커가 <strong>정상적으로 시작</strong> 되었을 때, 각 파티션의 로그 세그먼트 <strong>파일을 연다</strong>.
      </li>
      <li>
        브로커가 장애 발생 후 <strong>다시 시작</strong>되었을 때, 각 파티션의 로그 세그먼트를 검사하고 잘못된 부분은 <strong>삭제</strong>한다.
      </li>
      <li>
        브로커가 <strong>종료</strong>될 때, 로그 세그먼트를 정상적으로 <strong>닫는다</strong>.
      </li>
    </ul>
  <li>
    기본적으로 <strong>하나의 로그 디렉터리</strong>에 대해 <strong>하나의 스레드</strong>만이 사용된다.
  </li>
    <ul>
      <li>
        하나의 로그 디렉터리에서 사용되는 하나의 스레드는 브로커의 시작과 종료에만 사용되기에 <strong>병렬화</strong>를 위해서는 <strong>더 많은 쓰레드</strong>를 할당해야 한다.
      </li>
    </ul>
  <li>
    num.recovery.threads.per.data.dir의 설정은 <strong>log.dirs별</strong>로 지정된다.
  </li>
    <ul>
      <li>
        e.g. num.recovery.threads.per.data.dir = 8 이고 log.dirs는 3이라면 총 스레드는 8 * 3 = 24가 된다.
      </li>
    </ul>
</ul>

<h3>3-1-6. auto.create.topics.enable</h3>
<ul>
  <li>
    Kafka의 기본 설정에는 다음과 같은 상황에서 Broker들이 <strong>자동으로 Topic을 생성</strong>한다.
  </li>
    <ul>
      <li>
        프로듀서가 토픽에 <strong>메시지를 쓰기 시작</strong>할 때 
      </li>
      <li>
        컨슈머가 토픽으로부터 <strong>메시지를 읽기 시작</strong>할 때
      </li>
      <li>
        클라이언트가 토픽에 대한 <strong>메타데이터를 요청</strong>할 때 
      </li>
    </ul>
  <li>
    특정 작업을 진행할 때 <strong>의도치 않게 토픽을 생성</strong>하는 문제가 있을 수 있으며 이 경우 <strong>auto.create.topics.enable = false</strong>로 설정하여 문제를 해결할 수 있다.
  </li>
</ul>

<h3>3-1-7. auto.leader.rebalance.enable</h3>
<ul>
  <li>
    모든 Topic의 리더 역할이 하나의 Broker에 집중되어 Kafka 클러스터의 균형이 깨질 수 있다.
  </li>
  <li>
    <strong>auto.leader.rebalance.enable 옵션</strong>을 활성화하면 가능한 한 리더 역할이 균등하게 분산되도록하여 <strong>리더가 편향되는 것을 방지</strong>할 수 있다.
  </li>
    <ul>
      <li>
        옵션을 활성화하면 <strong>파티션의 분포 상태를 주기적으로 확인</strong>하는 백그라운드 스레드가 시작된다.
      </li>
        <ul>
          <li>
            확인 주기는 <strong>leader.imbalance.check.interval.seconds</strong> 값으로 설정이 가능하다.
          </li>
        </ul>
      <li>
        특정 브로커에 리더 역할이 할당된 파티션의 비율이 <strong>leader.imbalance.per.broker.percentage</strong>의 값을 넘어가면 파티션 선호 리더(preferred leader) 리밸런싱(rebalancing)이 발생한다.
      </li>
    </ul>
</ul>

<h3>3-1-8. delete.topic.enable</h3>
<ul>
  <li>
    <strong>delete.topic.enable 설정을 false</strong>로 하면 <strong>토픽 삭제 기능이 막힌다</strong>.
  </li>
</ul>

<br>

<h2>3-2. 토픽별 기본값</h2>
<ul>
  <li>
    카프카 브로커는 새로 생성되는 topic에 적용되는 수많은 설정 또한 존재하며 관리용 툴을 사용해 토픽 단위로 설정이 가능하다.
  </li>
</ul>

<h3>3-2-1. num.partitions</h3>
<ul>
  <li>
    새로운 토픽이 생성될 때 <strong>몇 개의 파티션</strong>을 갖게 되는지를 결정한다.
  </li>
    <ul>
      <li>
        주로 자동 토픽 생성 기능이 활성화되어 있을 때 사용되며 기본값은 1이다.
      </li>
    </ul>
  <li>
    Kafka에서는 파티션의 개수를 늘릴 수는 있지만 <strong>줄일 수는 없다</strong>는 점을 주의해야 한다.
  </li>
    <ul>
      <li>
        파티션에는 순서와 offset이 강하게 결합되어 있기 때문이다.
      </li>
      <li>
        따라서 num.partitions보다 더 적은 수의 파티션을 사용하려면 직접 토픽을 생성해야 한다.
      </li>
    </ul>
  <li>
    파티션은 Kafka 클러스터 안에서 <strong>토픽의 크기가 확장</strong>되는 방법이기도 하며 따라서 브로커가 추가될 때 클러스터 전체에 걸쳐 <strong>메시지 부하가 고르게 분산</strong>되도록 파티션 개수를 잡아주는 것이 중요하다.
  </li>
  <li>
    일반적으로는 토픽당 파티션 개수를 클러스터 내 <strong>브로커의 수</strong>와 맞추거나 혹은 <strong>배수</strong>로 설정한다. 이러면 파티션에 브로커별로 고르게 분포하도록 할 수 있다.
  </li>
  <li>
    만약 토픽의 목표 처리량과 컨슈머의 예상 처리량에 대해 어느 정도 추정값이 있다면 <strong>'토픽 목표 처리량 / 컨슈머의 예상 처리량'</strong>을 통해 파티션 수를 계산해 볼 수도 있다.
  </li>
  <li>
    기준이 없다면 경험상 매일 디스크 안에 저장되어 있는 파티션의 용량을 6GB 미만으로 유지하고 천천히 늘리는 것이 좋았다.
  </li>
</ul>

<h4>a. 파티션 수는 어떻게 결정해야 하는가?</h4>
<ul>
  <li>
    토픽에 대해 달성하고자 하는 <strong>처리량</strong>은 대략 어느 정도인지 고려한다.
  </li>
  <li>
    단일 파티션에서 달성하고자 하는 <strong>최대 읽기 처리량</strong>은 어느 정도인지 고려한다.
  </li>
  <li>
    각 프로듀서가 단일 파티션에 쓸 수 있는 최대 속도에 대해서도 비슷한 추정을 할 수 있다. 그러나 프로듀서가 컨슈머에 비해 일반적으로 더 빠르기에 이 과정은 생략할 수 있다.
  </li>
  <li>
    만약 <strong>키값</strong>을 기준으로 선택된 파티션에 메시지를 전송하는 경우 나중에 파티션을 추가하기 힘들다. 따라서 <strong>미래의 사용량 예측값</strong>을 기준으로 처리량을 계산한다.
  </li>
  <li>
    각 브로커에 배치할 파티션의 수뿐만 아니라 <strong>브로커별</strong>로 사용 가능한 <strong>디스크 공간, 네트워크 대역폭</strong> 또한 고려해야 한다.
  </li>
  <li>
    <strong>과대 추산</strong>은 피해야 한다. 각 파티션은 브로커의 메모리와 다른 자원들을 사용할 뿐만 아니라 메타데이터 업데이트나 리더 역할 변경에 걸리는 시간 역시 증가한다.
  </li>
  <li>
    데이터를 <strong>미러링</strong>할 예정인지 고려한다. 이 경우 미러링 설정 처리량을 고려해야 한다. 미러링 구성에서 큰 파티션은 병목이 되는 경우가 많다.
  </li>
  <li>
    <strong>클라우스 서비스</strong>를 활용한다면 가성 머신이나 디스크에 초당 입출력(input/output operations per second, IOPS) 제한이 있는지등을 고려해야 한다.
  </li>
</ul>

<h3>3-2-2. default.replication.factor</h3>
<ul>
  <li>
    자동 토픽 생성 기능이 활성화 되어 있는 경우 새로 새성되는 토픽의 <strong>복제 팩터</strong>를 결정한다.
  </li>
    <ul>
      <li>
        어떻게 복제를 해야하는가는 클러스터에 필요한 <strong>지속성</strong>과 <strong>가용성</strong>에 영향을 받기에 나중에 상세히 다룬다.
      </li>
    </ul>
  <li>
    보통 <strong>min.insync.replicas</strong> 설정값보다 <strong>1 이상</strong> 크게 잡을 것을 권장하며, <strong>내고장성(fault tolerance)</strong>를 좀 더 보장하고 싶은 경우 <strong>2 큰 값</strong>으로 복제 팩터를 설정하는 것이 좋다.
  </li>
    <ul>
      <li>
        보통은 RF++로 줄여서 표기한다.
      </li>
      <li>
        이렇게하면 replica가 적어도 3 개이기에 replica set 안에 일부러 정지시킨 replica와 예상치 않게 저징된 replica가 동시에 하나씩 발생해도 장애가 발생하지 않는다.
      </li>
    </ul>
</ul>

<h3>3-2-3. log.retentions.ms</h3>
<ul>
  <li>
    Kafka가 <strong>얼마나 오랫동안 메시지를 보존</strong>해야 하는지를 지정할 때 가장 많이 사용하는 설정이 <strong>시간 기준 보존 주기</strong> 설정이다.
  </li>
    <ul>
      <li>
        설정 파일에 정의된 기본값은 <strong>log.retention.hours</strong> 설정을 사용하며 <strong>168(=1주일)</strong>이다.
      </li>
      <li>
        또 다른 단위로 <strong>log.retention.minutes</strong> 혹은 <strong>log.retention.ms</strong>를 사용할 수도 있다.
      </li>
    </ul>
  <li>
    세 개의 설정은 모두 동일한 역할을 하지만, 1개 이상의 설정이 정의될 경우 <strong>더 작은 단위 설정값</strong>이 우선되기에 <strong>log.retention.ms</strong>를 사용할 것을 권장한다.
  </li>
  <li>
    <strong>시간 기준 보존과 마지막 수정 시각</strong>
  </li>
    <ul>
      <li>
        시간 기준 보존은 디스크에 저장된 각 로그 세그먼트 파일의 <strong>마지막 수정 시각(mtime)</strong>을 기준으로 작동한다.
      </li>
    </ul>
</ul>

<h3>3-2-4. log.retention.bytes</h3>
<ul>
  <li>
    메시지 만료의 또 다른 기준으로 보존되는 <strong>메시지의 용량</strong>을 사용한다.
  </li>
  <li>
    매개변수로 설정이 가능하며 <strong>파티션 단위</strong>로 적용된다.
  </li>
    <ul>
      <li>
        8개의 파티션을 갖는 토픽에 log.retention.bytes 설정값을 1GB로 잡으면 토픽의 최대 저장 용량은 8GB가 된다.
      </li>
    </ul>
  <li>
    파티션 단위로 적용되기에 파티션의 수를 증가하면 마찬가지로 용량 또한 증가한다.
  </li>
  <li>
    <strong>-1</strong>을 할당하면 데이터가 <strong>영구히 보존</strong>된다.
  </li>
  <li>
    <strong>크기와 시간을 기준으로 보존 설정하기</strong>
  </li>
    <ul>
      <li>
        <strong>log.retention.bytes</strong>와 <strong>log.retention.ms</strong> 둘 모두 설정이 되어 있다면 둘 중 <strong>먼저</strong> 기준이 충족된쪽으로 결정되기에 <strong>둘 중 하나만</strong>을 사용하는 것을 권장한다.
      </li>
    </ul>
</ul>

<h3>3-2-5. log.segment.bytes</h3>
<ul>
  <li>
    앞의 설정들은 <strong>로그 세그먼트</strong>에 적용되는 것이지 메시지 각각에 적용되지 않는다.
  </li>
  <li>
    브로커에 쓰여진 메시지는 해당 파티션의 <strong>현재 로그 세그먼트의 끝</strong>에 추가된다.
  </li>
  <li>
    로그 세그먼트의 크기가 log.segment.bytes에 지정된 크기에 다다르면 브로커는 기존 세그먼트를 닫고 <strong>새로운 세그먼트</strong>를 연다.
  </li>
  <li>
    로그 세그먼트는 닫히기 전까지는 <strong>만료와 삭제 대상이 되지 않는다</strong>.
  </li>
  <li>
    더 작은 로그 세그먼트 크기는 <strong>파일을 더 자주 닫고 새로 할당함</strong>을 의미한다.
  </li>
    <ul>
      <li>
        I/O에 부수 비용(overhead)가 발생하기에 디스크 효율성이 낮아진다.
      </li>
    </ul>
  <li>
    토픽에 들어오는 메시지가 적다면 로그 세그먼트를 작게 설정하는 것이 좋다. 그렇지 않다면 새로운 로그 세그먼트가 생성되기 까지 많은 시간이 걸린다.
  </li>
  <li>
    <strong>타임스탬프 기준으로 오프셋 찾기</strong>
  </li>
    <ul>
      <li>
        로그 세그먼트의 크기는 <strong>타임스탬프를 기준으로 오프셋</strong>을 찾는 기능에도 영향을 미친다. 클라이언트가 특정 타임스탬프로 파티션의 오프셋을 요청하면 <strong>해당 시각에 쓰인 로그 세그먼트</strong>를 찾기 때문이다.
      </li>
      <li>
        이 때에는 로그 세그먼트의 생성 시각과 맞지막 수정 시각을 활용한다.
      </li>
    </ul>
</ul>

<h3>3-2-6. log.roll.ms</h3>
<ul>
  <li>
    <strong>log.roll.ms</strong>를 활용해 로그 세그먼트 파일이 <strong>닫힐 때까지 기다리는 시간</strong>을 지정할 수 있다.
  </li>
  <li>
    log.segment.bytes와 log.roll.ms 또한 둘 중 먼저 충족되는 것을 기준으로 세그먼트가 닫히기에 둘 중 하나만 사용하는 것이 좋다.
  </li>
  <li>
    <strong>시간 기준 세그먼트 사용 시의 디스크 성능</strong>
  </li>
    <ul>
      <li>
        시간 기준 로그 세그먼트를 제한할 경우 한 번에 로그 세그먼트가 <strong>동시에</strong> 닫힐 수 있어(동시에 시간 충족) <strong>디스크 성능</strong>을 고려해야 한다.
      </li>
    </ul>
</ul>

<h3>3-2-7. min.insync.replicas</h3>
<ul>
  <li>
    데이터 지속성 위주로 클러스터를 설정할 때 min.insync.replicas를 <strong>2</strong>로 설정하면 <strong>최소한 2개의 레플리카</strong>가 <strong>최신 상태로 프로듀서와 동기화</strong>되도록 할 수 있다.
  </li>
    <ul>
      <li>
        프로듀서의 ack 설정을 'all'로 잡아 두는 것과 함께 사용한다.
      </li>
      <li>
        프로듀서의 쓰기 작업이 성공하기 위해 최소한 두 개의 레플리카(리더 + 팔로워 중 하나)가 응답하도록 할 수 있다.
      </li>
    </ul>
  <li>
    아래와 같은 상황에서 데이터 유실을 방지할 수 있다.
  </li>
    <ul>
      <li>
        1. 리더가 쓰기 작업에 응답한다.
      </li>
      <li>
        2. 리더가 장애를 발생한다.
      </li>
      <li>
        3. 리더 역할이 최근의 (성공한) 쓰기 작업 내역을 복제하기 전의 다른 레플리카로 옮겨진다.
      </li>
    </ul>
  <li>
    단, min.insync.replicas를 높게 잡을 경우 <strong>오버헤드</strong>가 발생하여 성능이 떨어질 수 있다.
  </li>
    <ul>
      <li>
        따라서 몇 개의 메시지 유실은 무관할 경우 기본값인 1을 사용할 것을 권장한다.
      </li>
    </ul>
</ul>

<h3>3-2-8. message.max.bytes</h3>
<ul>
  <li>
    Kafka 브로커가 쓸 수 있는 <strong>메시지의 최대 크기를 제한</strong>한다.
  </li>
    <ul>
      <li>
        기본값은 1,000,000(=1MB)이며 이를 초과한 메시지의 경우 브로커가 거부하며 에러를 리턴한다.
      </li>
      <li>
        해당 설정은 <strong>압축된 메시지의 크기</strong>를 기준으로 한다.
      </li>
    </ul>
  <li>
    메시지의 크기에 대한 설정이기에 I/O 처리량, 스레드 처리량, 저장소 등 다양한 요소와 연관된다.
  </li>
  <li>
    메시지 크기 설정 조정하기
  </li>
    <ul>
      <li>
        Kafka 브로커의 메시지 크기 설정은 컨슈머 클라이언트의 fetch.message.bytes 설정과도 맞아야한다.
      </li>
      <li>
        브로커측의 message.max.bytes가 클라이언트의 fetch.message.bytes보다 클 경우 클라이언트가 메시지를 읽지 못하는 문제가 발생한다.
      </li>
    </ul>
</ul>

<br><br>

<h1>4. 하드웨어 선택하기</h1>
<ul>
  <li>
    Kafka는 특정한 하드웨어 구성을 요구하지 않아 하드웨어 선택이 자유롭다.
  </li>
  <li>
    하지만 전체적인 성능을 고려한다면 <strong>디스크 처리량과 용량, 메모리, 네트워크 그리고 CPU</strong>를 고려해야 한다.
  </li>
  <li>
    Kafka를 매우 크게 확장해야 할 경우 업데이트되어야 하는 <strong>메타데이터의 양</strong> 때문에 하나의 브로커가 처리할 수 있는 <strong>파티션의 수</strong>에도 제한이 생길 수 있다.
  </li>
</ul>

<br>

<h2>4-1. 디스크 처리량</h2>
<ul>
  <li>
    브로커의 <strong>디스크의 처리량</strong>은 프로듀서 클라이언트의 성능에 가장 큰 영향을 미친다.
  </li>
  <li>
    대부분의 프로듀서 클라이언트는 메시지 전송이 성공했다고 결론을 내리기 전 <strong>최소 1개 이상의 브로커</strong>가 메시지가 커밋되었다고 응답을 보낼 때까지 대기하게 된다.
  </li>
    <ul>
      <li>
        즉, 브로커가 <strong>디스크에 쓰는 속도</strong>가 빠를수록 지연이 줄어든다.
      </li>
    </ul>
  <li>
    디스크 처리량과 관련해 가장 명백한 문제는 <strong>HDD</strong>를 사용할 것인지 아니면 <strong>SSD</strong>를 사용할 것인지이다.
  </li>
    <ul>
      <li>
        <strong>SSD</strong>: 탐색과 접근에 들어가는 시간이 짧아 성능이 매우 좋다.
      </li>
      <li>
        <strong>HDD</strong>: SSD보다 싼 가격에 더 많은 용량을 제공한다.
      </li>
    </ul>
  <li>
    SAS(Serial Attached SCSI) 혹은 SATA(Serial ATA)와 같은 <strong>스토리지 장치</strong>를 서버/컴퓨터에 연결하기 위한 <strong>하드웨어 인터페이스</strong> 또한 성능에 영향을 준다.
  </li>
</ul>

<br>

<h2>4-2. 디스크 용량</h2>
<ul>
  <li>
    필요한 디스크 용량은 <strong>특정한 시점에 얼마나 많은 메시지</strong>들이 보존되어야 하는지에 따라 결정된다.
  </li>
  <li>
    저장 용량은 Kafka 클러스터의 크기를 변경하거나 확장 시점을 결정할 때 고려해야할 요소 중 하나이다.
  </li>
  <li>
    전체 트래픽은 <strong>토픽별로 다수의 파티션</strong>을 잡아 줌으로써 클러스터 전체에 균형 있게 분산이 가능하다. 따라서 단일 브로커의 용량이 충분하지 않은 경우 <strong>브로커를 추가</strong>하여 전체 용량을 증대시킬 수 있다.
  </li>
</ul>

<br>

<h2>4-3. 메모리</h2>
<ul>
  <li>
    Kafka 컨슈머는 프로듀서가 추가한 메시지를 <strong>맨 끝</strong>에서 읽어오는 식이 보통이다.
  </li>
    <ul>
      <li>
        따라서 최적의 작동은 <strong>시스템의 페이지 캐시</strong>에 저장되어 있는 메시지를 컨슈머가 읽어오는 것이다. (브로커가 디스크로부터 메시지를 다시 읽어 오는 것보다 빠르다).
      </li>
      <li>
        시스템에 페이지 캐시로 사용할 수 있는 메모리를 할당해 컨슈머 클라이언트의 성능을 향상시킬 수 있다.
      </li>
    </ul>
  <li>
    Kafka는 자바 가상 머신(Java Virtual Machine, JVM)에 많은 힙 메모리를 필요로 하지 않는다.
  </li>
    <ul>
      <li>
        따라서 힙 메모리 외의 시스템 메모리의 나머지 영역은 <strong>페이지 캐시</strong>로 사용되어 시스템이 사용 중인 <strong>로그 세그먼트를 캐시</strong>하도록 하여 성능 향상이 가능하다.
      </li>
      <li>
        OS의 페이지 캐시를 사용하기 때문에 Kafka는 <strong>독립적</strong>으로 운영할 것을 권장한다. (다른 앱이 페이지 캐시를 사용하지 않도록).
      </li>
    </ul>
</ul>

<br>

<h2>4-4. 네트워크</h2>
<ul>
  <li>
    사용 가능한 <strong>네트워크 대역폭</strong>은 Kafka가 처리할 수 있는 <strong>트래픽의 최대량</strong>을 결정한다.
  </li>
    <ul>
      <li>
        따라서 디스크 용량과 함께 클러스터의 크기를 결정하는 가장 결정적인 요소 중 하나이다.
      </li>
    </ul>
  <li>
    Kafka는 <strong>다수의 컨슈머</strong>를 동시에 지원하기 때문에 인입되는 네트워크 사용량과 유츌되는 네트워크 사용량이 달라 <strong>네트워크 불균형</strong> 현상이 발생한다.
  </li>
    <ul>
      <li>
        네트워크 인터페이스가 포화상태에 빠질 경우, 클러스터 내부의 복제 작업이 밀려 클러스터가 취약해 질 수 있다. 따라서 최소한 10GB 이상을 처리할 수 있는 네트워크 인퍼테이스 카드(Network Interface Card, NIC)를 사용할 것을 권장한다.
      </li>
    </ul>
</ul>

<br>

<h2>4-5. CPU</h2>
<ul>
  <li>
    Kafka 클러스터를 <strong>매우 크게</strong> 확장하지 않는 한, 처리 능력은 디스크나 메모리만큼 중요하지 않다.
  </li>
  <li>
    Kafka는 <strong>클라이언트가 메시지를 압축</strong>해서 보내야 하는데 브로커는 각 메시지의 체크섬을 확인하고 오프셋을 부여하기 위해 <strong>모든 메시지 배치의 압축을 해제</strong>해야 한다. 그리고 마지막으로 <strong>다시 디스크에 저장</strong>하기 위해 <strong>압축</strong>을 수행하며 이때 Kafka의 <strong>처리 능력</strong>이 중요하게 다뤄진다.
  </li>
</ul>

<br><br>

<h1>5. 클라우드에서 카프카 사용하기</h1>
<ul>
  <li>
    클라우드 환경에서 Kafka를 직접 설치하는 경우의 설정을 다룬다.
  </li>
  <li>
    대부분 클라우드 환경에서는 다양한 가상 머신 인스턴스를 선택할 수 있으며, 이들 각각에 서로 다른 조합의 CPU, 메모리, IOPS, 디스크 설정을 추가해서 사용할 수 있다.
  </li>
</ul>

<br>

<h2>5-1. 마이크로소프트 애저</h2>
<ul>
  <li>
    애저의 경우 <strong>디스크를 가상 머신(VM)과 분리</strong>해서 관리할 수 있으며 따라서 저장 장치를 선택할 때 사용할 VM을 고려할 필요가 없다.
  </li>
  <li>
    Kafka를 설정하기 위한 애저의 여러 서버 설정들에 대해 다룬다.
  </li>
</ul>

<br>

<h2>5-2. 아마존 웹 서비스</h2>
<ul>
  <li>
    애저와 동일하게 디스크 설정 등 Kafka에 적합한 환경 구축을 위한 AWS 서버 설정에 대해 다룬다.
  </li>
</ul>

<br><br>

<h1>6. 카프카 클러스터 설정하기</h1>
<ul>
  <li>
    개발용이나 PoC(Proof-of-cncept) 용도로는 단일 Kafka 브로커로도 충분하다.
  </li>
  <li>
    여러 대의 브로커를 사용할 경우 다음과 같은 이점이 있다.
  </li>
    <ul>
      <li>
        <strong>부하를 다수의 서버로 확장</strong>할 수 있다.
      </li>
      <li>
        단일 시스템 장애에서 발생할 수 있는 <strong>데이터 유실을 방지</strong>할 수 있다.
      </li>
      <li>
        클라이언트의 요청을 처리하면서 <strong>유지 관리 작업</strong>을 수행할 수 있다.
      </li>
    </ul>
</ul>

<br>

<h2>6-1. 브로커 개수</h2>
<ul>
  <li>
    Kafka 클러스터의 적절한 크기를 결정하는 요소에는 일반적으로 다음과 같은 것들이 있다.
  </li>
    <ul>
      <li>
        디스크 용량
      </li>
      <li>
        브로커당 레플리카 용량
      </li>
      <li>
        CPU 용량
      </li>
      <li>
        네트워크 용량
      </li>
    </ul>
  <li>
    가장 먼저 고려할 요소는 필요한 메시지를 저장하는 데 필요한 <strong>디스크 용량</strong>과 브로커가 사용할 수 있는 <strong>저장소 용량</strong>이다.
  </li>
    <ul>
      <li>
        레플리카를 증가하는 경우 저장 용량은 최소 100% 이상 증가하게 된다.
      </li>
      <li>
        파티션의 레플리카 수가 과할 경우 <strong>병목 현상</strong>을 유발할 수 있다. 최근에는 파티션 레플리카의 수를 브로커당 14,000개, 클러스터당 100만개 이하로 유지할 것을 권장한다.
      </li>
    </ul>
  <li>
    CPU의 경우 일반적으로 주요한 병목 지점이 되지 않지만, 감당할 수 없는 클라이언트 연결이나 요청이 쏟아지면 병목이 발생할 수 있다.
  </li>
  <li>
    네트워크 용량에 대해서는 다음 두 가지를 염두해 두어야 한다.
  </li>
    <ul>
      <li>
        네트워크 인터페이스의 전체 용량이 얼마인지?
      </li>
      <li>
         데이터를 읽는 컨슈머가 여럿이거나 데이터가 보존되는 동안 트래픽이 일정하지 않을 경우에도 클라이언트 트래픽을 받아낼 수 있는지?
      </li>
    </ul>
  <li>
    Kafka에 메시지가 들어오면 <strong>OS의 캐시 페이지</strong>에 적재되고 결국 파티션 레플리카도 캐시 페이지에서 <strong>데이터를 가져오는 요청 처리 작업</strong>을 하기에 파티션 레플리카가 많아지면 <strong>CPU 부하</strong>가 늘어난다.
  </li>
</ul>

# p39
